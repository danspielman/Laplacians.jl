<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Solving Linear Equations · Laplacians.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Laplacians.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">About</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../Installation/">Installation</a></li><li><a class="tocitem" href="../Examples/">Examples</a></li><li><a class="tocitem" href="../CSCgraph/">Sparse matrices as graphs</a></li><li class="is-active"><a class="tocitem" href>Solving Linear Equations</a><ul class="internal"><li><a class="tocitem" href="#Harmonic-Interpolation-1"><span>Harmonic Interpolation</span></a></li><li><a class="tocitem" href="#The-Solver-Interface-1"><span>The Solver Interface</span></a></li><li><a class="tocitem" href="#Cholesky-Factorization-1"><span>Cholesky Factorization</span></a></li><li><a class="tocitem" href="#CG-and-PCG-1"><span>CG and PCG</span></a></li><li><a class="tocitem" href="#Low-Stretch-Spanning-Trees-1"><span>Low-Stretch Spanning Trees</span></a></li><li><a class="tocitem" href="#Augmented-Spanning-Tree-Preconditioners-1"><span>Augmented Spanning Tree Preconditioners</span></a></li><li><a class="tocitem" href="#The-solvers-of-Koutis,-Miller-and-Peng.-1"><span>The solvers of Koutis, Miller and Peng.</span></a></li><li><a class="tocitem" href="#Sampling-Solvers-of-Kyng-and-Sachdeva-1"><span>Sampling Solvers of Kyng and Sachdeva</span></a></li><li><a class="tocitem" href="#Algebraic-Multigrid-1"><span>Algebraic Multigrid</span></a></li><li><a class="tocitem" href="#Solvers-from-Matlab-1"><span>Solvers from Matlab</span></a></li></ul></li><li><a class="tocitem" href="../LSST/">Low Stretch Spanning Trees</a></li></ul></li><li><a class="tocitem" href="../Developing/">Developing</a></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../graphGenerators/">generators</a></li><li><a class="tocitem" href="../operators/">operators</a></li><li><a class="tocitem" href="../graphUtils/">graphUtils</a></li><li><a class="tocitem" href="../graphAlgs/">graphAlgs</a></li><li><a class="tocitem" href="../IO/">IO</a></li><li><a class="tocitem" href="../solvers/">solvers</a></li><li><a class="tocitem" href="../sparsification/">sparsification</a></li><li><a class="tocitem" href="../akpw/">akpw</a></li><li><a class="tocitem" href="../treeAlgs/">treeAlgs</a></li><li><a class="tocitem" href="../randTrees/">randTrees</a></li><li><a class="tocitem" href="../localClustering/">localClustering</a></li><li><a class="tocitem" href="../privateFuncs/">Private Functions</a></li><li><a class="tocitem" href="../indexOfAll/">All of the above</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Solving Linear Equations</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Solving Linear Equations</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/danspielman/Laplacians.jl/blob/master/docs/src/usingSolvers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><p>[TOC]</p><h1 id="Solving-linear-equations-in-Laplacians-and-SDD-matrices-1"><a class="docs-heading-anchor" href="#Solving-linear-equations-in-Laplacians-and-SDD-matrices-1">Solving linear equations in Laplacians and SDD matrices</a><a class="docs-heading-anchor-permalink" href="#Solving-linear-equations-in-Laplacians-and-SDD-matrices-1" title="Permalink"></a></h1><p>The main purpose of this package is to experiment with the implementation of algorithms for solving systems of linear equations in Laplacian and symmetric, diagonally dominant, M-matrices (SDDM).</p><p>At present, the fastest solver in this package for Laplacians is <a href="../solvers/#Laplacians.approxchol_lap-Union{Tuple{SparseArrays.SparseMatrixCSC{Tv, Ti}}, Tuple{Ti}, Tuple{Tv}} where {Tv, Ti}"><code>approxchol_lap</code></a>. A second version <a href="../solvers/#Laplacians.approxchol_lap2-Union{Tuple{SparseArrays.SparseMatrixCSC{Tv, Ti}}, Tuple{Ti}, Tuple{Tv}} where {Tv, Ti}"><code>approxchol_lap2</code></a> is slower but more robust. For SDDM systems, one should use <a href="../solvers/#Laplacians.approxchol_sddm"><code>approxchol_sddm</code></a>.  Here is a quick demo.  Read more for other solvers and other options you can pass to the solvers.</p><pre><code class="language-julia">julia&gt; a = grid3(50); # an adjacency matrix
julia&gt; la = lap(a); # it&#39;s Laplacian
julia&gt; sol = approxchol_lap(a); # a solver for la
julia&gt; b = randn(size(la,1)); b = b .- mean(b); # a right-hand-side
julia&gt; x = sol(b); # the solution
julia&gt; norm(la*x-b) / norm(b)
5.911931368666469e-7
julia&gt; x = sol(b, tol=1e-12); # a higher accuracy solution
julia&gt; norm(la*x-b) / norm(b)
7.555529748070115e-11
julia&gt; x = sol(b, tol=1e-1, verbose=true); # faster, lower accuracy, with info
PCG stopped after: 0.022 seconds and 3 iterations with relative error 0.07929402690389374.

julia&gt; sddm = copy(la); # doing it with a SDDM matrix
julia&gt; sddm[1,1] += 1;
julia&gt; sol = approxchol_sddm(sddm, verbose=true); # solver, with output
Using greedy degree ordering. Factorization time: 0.7143130302429199
Ratio of operator edges to original edges: 2.1120548223350255
ratio of max to min diagonal of laplacian : 6.0
Solver build time: 0.747 seconds.

julia&gt; x = sol(b, verbose=false); # a solve, supressing output
julia&gt; norm(sddm*x - b) / norm(b)
8.739618692868002e-7</code></pre><p>We recall that a matrix $ L $ is a <em>Laplacian</em> matrix if:</p><ul><li>It is symmetric,</li><li>its off-diagonal entries are non-positive, and</li><li>all of its row sums are 0.</li></ul><p>These conditions imply that the diagonal entries are non-negative, and that the matrix is singular.  So, we only hope to solve equations of the form  $ Lx = b $ when <span>$b$</span> is in the span of the matrix.  When the graph of the nonzero entries of the matrix is connected, this is precisely when the sum of the entries in $ b $ is zero.  Laplacian matrices are always positive semidefinite.</p><p>A matrix $ M $ is a symmetric <em>M-matrix</em> if:</p><ul><li>It is symmetric,</li><li>its off-diagonal entries are non-positive, and</li><li>it is positive definite.</li></ul><p>A matrix symmetric $ M $ is <em>diagonally dominant</em> if each of its diagonals is at least the sum of the absolute values of the off-diagonal entries in its row.  A Laplacians matrix is diagonally dominant.  A diagonally dominant matrix is always positive semidefinite.</p><p>A <em>SDDM</em> matrix (symmetric, diagonally-dominant M-matrix) is a matrix that is both diagonally dominant and an M-matrix.  You may think of a SDDM matrix as a Laplacian plus a non-negative, non-zero, diagonal matrix.  However, this is only guaranteed to produce a SDDM matrix when the graph underlying the Laplacian is connected.</p><p>Laplacians.jl contains code for solving systems of linear equations in both Laplacian and SDDM matrices.  In fact, these problems are equivalent.  So, usually a solver for one type of system is implemented, and then wrapped to solve the other. The same ideas can be used to solve systems of equations in SDD matrices (the off-diagonals can be positive or negative), but a wrapper for these has not yet been written.</p><h2 id="Harmonic-Interpolation-1"><a class="docs-heading-anchor" href="#Harmonic-Interpolation-1">Harmonic Interpolation</a><a class="docs-heading-anchor-permalink" href="#Harmonic-Interpolation-1" title="Permalink"></a></h2><p>One of the main reasons to solve Laplacian and SDDM systems is to interpolate harmonic functions on graphs.  In an unweighted graph, these have the property that the value at every vertex is the average of the values of its neighbors.  To make this sensible, some values must be fixed.</p><p>For example, below we fit a harmonic function on the 4-by-4 grid. We fix the values of vertices <code>1</code>, <code>4</code>, and <code>16</code> to <code>0.0</code>, <code>0.5</code>, and <code>2.0</code>, respectively.  We then show the results by forcing them into a 4-by-4 grid.</p><pre><code class="language-julia">julia&gt; a = grid2(4);
julia&gt; S = [1; 4; 16];
julia&gt; vals = [0; 0.5; 2];
julia&gt; x = harmonic_interp(a, S, vals);
julia&gt; reshape(x,4,4)
4×4 Array{Float64,2}:
 0.0       0.460942  0.749255  0.903101
 0.398927  0.633572  0.883721  1.05695
 0.563208  0.790698  1.09511   1.38402
 0.5       0.8709    1.322     2.0     </code></pre><h2 id="The-Solver-Interface-1"><a class="docs-heading-anchor" href="#The-Solver-Interface-1">The Solver Interface</a><a class="docs-heading-anchor-permalink" href="#The-Solver-Interface-1" title="Permalink"></a></h2><p>All of the SDDM solvers take the SDDM matrix as input.</p><p><em>All of the Laplacian solvers take the adjacency matrix of the underlying graph as input.</em></p><p>To solve a system of linear equations, one first passes the matrix defining the system to a linear equation solving algorithm.  This will return a function that solves systems of linear equations in that matrix.  For example,</p><pre><code class="language-julia">julia&gt; n = 1000;
julia&gt; a = wted_chimera(n);  # produces a graph, as a sparse adjacency matrix
julia&gt; b = randn(n);
julia&gt; b = b - mean(b); # so there is a solution
julia&gt; f = chol_lap(a)
(::#79) (generic function with 1 method)
julia&gt; x = f(b);
julia&gt; la = lap(a);  # construct the Laplacian of a
julia&gt; norm(la*x-b)
2.9565023548855584e-13</code></pre><p>All of the solvers take the following keyword arguments. This means that they are optional, and will be set to their default values if not specified.</p><ul><li><code>tol</code> : the relative accuracy required: $ \| M x - b \| / \| b \| $.</li><li><code>maxits</code> : the maximum number of iterations, for iterative algorithms.</li><li><code>maxtime</code> : quit if it takes more than this many seconds.  Not all routines obey this, but they try.</li><li><code>verbose</code> : set to <code>true</code> to display diagnostic information.</li><li><code>pcgIts</code> : If the algorithm is iterative, this allows it to return the number of iterations it performed.  If <code>pcgIts</code> is an array of positive length, then its first entry is set to the number of iterations.  Where <code>verbose</code> prints this information, <code>pcgIts</code> allows it to be returned to other code.  To disable this set <code>pcgIts</code> to a zero length array, like <code>Int[]</code>.</li></ul><p>Most of the solvers are iterative, exploiting the preconditioned conjugate gradient. These are the solvers for which <code>maxits</code>, <code>maxtime</code> and <code>pcgIts</code> make the most sense.  Some solvers, like Cholesky factorization, just ignore these parameters.</p><p>All of these parameters may be set in the call that constructs <code>f</code>.  They may then be over-ridden by again setting them in the call to <code>f</code>. Let&#39;s see how this works when using the conjugate gradient.</p><pre><code class="language-julia">julia&gt; f = cgLapSolver(a, tol=1e-2, verbose=true)
(::f) (generic function with 1 method)
julia&gt; x = f(b);
CG BLAS stopped after: 78 iterations with relative error 0.009590493139133275.
julia&gt; norm(la*x-b)/norm(b)
0.00959049313913375

julia&gt; pcgIts = [0]
1-element Array{Int64,1}:
 0
julia&gt; x = f(b,verbose=false, pcgIts=pcgIts);
julia&gt; pcgIts
1-element Array{Int64,1}:
 78

julia&gt; x = f(b,verbose=true, maxits=50);
CG BLAS stopped after: 50 iterations with relative error 0.050483096216933886.

julia&gt; x = f(b, tol=1e-4);
CG BLAS stopped after: 131 iterations with relative error 8.886882933346416e-5.
julia&gt; norm(la*x-b)/norm(b)
8.886882933294668e-5</code></pre><p>For some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.</p><p>In the following, we document many of the solvers that have been implemented in this package.</p><h2 id="Cholesky-Factorization-1"><a class="docs-heading-anchor" href="#Cholesky-Factorization-1">Cholesky Factorization</a><a class="docs-heading-anchor-permalink" href="#Cholesky-Factorization-1" title="Permalink"></a></h2><p>Cholesky factorization, the version of Gaussian Elimination for symmetric matrices, should be the first solver you try.  It will be very fast for matrices of dimension less than 1000, and for larger matrices coming from two-dimensional problems.</p><p>You can compute a cholesky factor directly with <code>cholfact</code>.  It does  more than just compute the factor, and it saves its result in a data structure that implements <code>\</code>.  It uses SuiteSparse by Tim Davis.</p><p>Here is an example of how you would use it to solve a general non-singular linear system.</p><pre><code class="language-julia">a = grid2(5)
la = lap(a)
sddm = copy(la)
sddm[1,1] = sddm[1,1] + 1
F = cholfact(sddm)

n = size(a)[1]
b = randn(n)
x = F \ b
norm(sddm*x-b)

 	1.0598778281116327e-14</code></pre><p>As <code>cholfact</code> does not satisfy our interface, we wrap it in a routine <a href="../solvers/#Laplacians.chol_sddm"><code>chol_sddm</code></a> that does.</p><p>To solve systems in Laplacian matrices, use <a href="../solvers/#Laplacians.chol_lap"><code>chol_lap</code></a>.  Recall that this routine should be passed the adjacency matrix.</p><pre><code class="language-julia">f = chol_lap(a)
b = randn(n);
b = b - mean(b);
norm(la*f(b) - b)
	2.0971536951312585e-15</code></pre><h2 id="CG-and-PCG-1"><a class="docs-heading-anchor" href="#CG-and-PCG-1">CG and PCG</a><a class="docs-heading-anchor-permalink" href="#CG-and-PCG-1" title="Permalink"></a></h2><p>We have written implementations of Conjugate Gradient (CG) and Preconditioned Conjugate Gradient (PCG) that satisfy the interface. These routines use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  </p><pre><code class="language-julia">seed!(1)
n = 50
M = randn(n,n); M = M * M&#39;;
b = randn(n)
x = cg(M,b,maxits=100,verbose=true);
CG BLAS stopped after: 66 iterations with relative error 2.0166243927814765e-7.

bbig = convert(Array{BigFloat,1},b)
xbig = cg(M,bbig,maxits=100,tol=1e-30)
CG Slow stopped after: 50 iterations with relative error 2.18672511297479336887519117065525148757254642683072581090418060286711737398731e-38.

norm(M*xbig - bbig)
1.605742093628722039938504001423963138146137896744531914963345296279741402982296e-37</code></pre><p>To create a function <code>f</code> that uses cg to solve systems in M, use <a href="../solvers/#Laplacians.cgSolver"><code>cgSolver</code></a>.  For Laplacians, use <a href="../solvers/#Laplacians.cgLapSolver-Tuple{SparseArrays.SparseMatrixCSC}"><code>cgLapSolver</code></a>.</p><pre><code class="language-julia">julia&gt; n = 1000;
julia&gt; a = wted_chimera(n,1);
julia&gt; f = Laplacians.cgLapSolver(a,maxits=100);

julia&gt; b = randn(n);
julia&gt; b = b - mean(b);
julia&gt; x = f(b,verbose=true);
CG BLAS stopped after: 100 iterations with relative error 0.012102058751548373.


julia&gt; la = lap(a);
julia&gt; sddm = copy(la);
julia&gt; sddm = sddm + spdiagm(rand(n)/100);
julia&gt; g = cgSolver(sddm,verbose=true)
(::f) (generic function with 1 method)

julia&gt; x = g(b);
CG BLAS stopped after: 253 iterations with relative error 7.860172210007891e-7.</code></pre><p>PCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.</p><pre><code class="language-julia">seed!(1)
a = mapweight(grid2(200),x-&gt;1/(rand(1)[1]));
la = lap(a)
n = size(la)[1]
b = randn(n)
b = b - mean(b);

d = diag(la)
prec(x) = x ./ d
@time x = pcg(la,b,prec,maxtime=1,tol=1e-2,verbose=true);

PCG BLAS stopped at maxtime.
PCG BLAS stopped after: 530 iterations with relative error 0.07732478003311881.
  1.007756 seconds (10.32 k allocations: 648.525 MB, 9.69% gc time)

@time x = pcg(la,b,prec,maxtime=3,tol=1e-2,verbose=true);
PCG BLAS stopped after: 1019 iterations with relative error 0.009984013184429813.
  2.086828 seconds (19.57 k allocations: 1.216 GB, 9.92% gc time)</code></pre><p>Without the preconditioner, CG takes much longer on this example.</p><pre><code class="language-julia">@time x = cg(la,b,tol=1e-2,maxtime=10,verbose=true);

CG BLAS stopped at maxtime.
CG BLAS stopped after: 8879 iterations with relative error 0.054355534834831624.
 10.001998 seconds (97.91 k allocations: 2.649 GB, 4.48% gc time)</code></pre><p><a href="../solvers/#Laplacians.pcgSolver"><code>pcgSolver</code></a> creates a function that uses the preconditioner to solve systems in the matrix.</p><pre><code class="language-julia">f = pcgSolver(la,prec)
@time x = f(b,maxtime=3,tol=1e-2,verbose=true);
PCG BLAS stopped after: 1019 iterations with relative error 0.009984013184429813.
  1.892217 seconds (19.58 k allocations: 1.216 GB, 9.47% gc time)</code></pre><p><a href="../solvers/#Laplacians.pcgLapSolver-Tuple{AbstractMatrix, AbstractMatrix}"><code>pcgLapSolver</code></a> uses the Laplacian of one matrix as a preconditioner for the first.  It solves systems of linear equations in the preconditioner by Cholesky factorization.  It performs the Cholesky factorization when <a href="../solvers/#Laplacians.pcgLapSolver-Tuple{AbstractMatrix, AbstractMatrix}"><code>pcgLapSolver</code></a> is called.  This is why we do the work of creating <code>f</code> only once.  Here is an example using a Low-Stretch Spanning Tree preconditioner.</p><pre><code class="language-julia">@time t = akpw(a)
  0.210467 seconds (1.43 M allocations: 91.226 MB, 19.23% gc time)

@time f = pcgLapSolver(a,t)
  0.160210 seconds (288 allocations: 28.076 MB, 72.28% gc time)

@time x = f(b,maxtime=3,tol=1e-2,verbose=true);
PCG BLAS stopped after: 260 iterations with relative error 0.009864463201800925.
  1.014897 seconds (28.02 k allocations: 1.008 GB, 9.81% gc time)</code></pre><h2 id="Low-Stretch-Spanning-Trees-1"><a class="docs-heading-anchor" href="#Low-Stretch-Spanning-Trees-1">Low-Stretch Spanning Trees</a><a class="docs-heading-anchor-permalink" href="#Low-Stretch-Spanning-Trees-1" title="Permalink"></a></h2><p>In order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code that is guaranteed to produce these.  Instead, we supply three heuristics: <a href="../akpw/#Laplacians.akpw-Tuple{Any}"><code>akpw</code></a> which is inspired by the algorith of Alon, Karp, Peleg and West, and  randomized versions of Prim and Kruskal&#39;s algorithm. <a href="../randTrees/#Laplacians.randishKruskal-Tuple{SparseArrays.SparseMatrixCSC}"><code>randishKruskal</code></a> samples the remaining edges with probability proportional to their weight.  <a href="../randTrees/#Laplacians.randishPrim-Union{Tuple{SparseArrays.SparseMatrixCSC{Tval, Tind}}, Tuple{Tind}, Tuple{Tval}} where {Tval, Tind}"><code>randishPrim</code></a> samples edges on the boundary while using the same rule.  We recommend using <a href="../akpw/#Laplacians.akpw-Tuple{Any}"><code>akpw</code></a>.</p><p>See <a href="../LSST/">Low Stretch Spanning Trees</a> to learn more about these.</p><h2 id="Augmented-Spanning-Tree-Preconditioners-1"><a class="docs-heading-anchor" href="#Augmented-Spanning-Tree-Preconditioners-1">Augmented Spanning Tree Preconditioners</a><a class="docs-heading-anchor-permalink" href="#Augmented-Spanning-Tree-Preconditioners-1" title="Permalink"></a></h2><p>These are obtained by constructing a spanning tree of a graph, and then adding back some more edges from the graph.  The tree should have low stretch.  The edges to add back are chosen at random with probabilities proportional to their stretches.</p><p>These are implemented in the routines</p><ul><li><a href="../solvers/#Laplacians.augTreeSddm-Union{Tuple{SparseArrays.SparseMatrixCSC{Tv, Ti}}, Tuple{Ti}, Tuple{Tv}} where {Tv, Ti}"><code>augTreeSddm</code></a>, for SDDM matrices</li><li><a href="../solvers/#Laplacians.augTreeLap-Union{Tuple{SparseArrays.SparseMatrixCSC{Tv, Ti}}, Tuple{Ti}, Tuple{Tv}} where {Tv, Ti}"><code>augTreeLap</code></a></li><li><a href="../solvers/#Laplacians.augTreePrecon-Union{Tuple{SparseArrays.SparseMatrixCSC{Tv, Ti}}, Tuple{Ti}, Tuple{Tv}} where {Tv, Ti}"><code>augTreePrecon</code></a></li><li><a href="../solvers/#Laplacians.augmentTree-Union{Tuple{Ti}, Tuple{Tv}, Tuple{SparseArrays.SparseMatrixCSC{Tv, Ti}, SparseArrays.SparseMatrixCSC{Tv, Ti}, Ti}} where {Tv, Ti}"><code>augmentTree</code></a></li></ul><h2 id="The-solvers-of-Koutis,-Miller-and-Peng.-1"><a class="docs-heading-anchor" href="#The-solvers-of-Koutis,-Miller-and-Peng.-1">The solvers of Koutis, Miller and Peng.</a><a class="docs-heading-anchor-permalink" href="#The-solvers-of-Koutis,-Miller-and-Peng.-1" title="Permalink"></a></h2><p>Solvers inspired by the algorithm from &quot;Approaching optimality for solving SDD systems&quot; by Koutis, Miller, and Peng, &lt;i&gt;SIAM Journal on Computing&lt;/i&gt;, 2014.</p><ul><li><a href="../solvers/#Laplacians.KMPLapSolver-Tuple{Any}"><code>KMPLapSolver</code></a></li><li><a href="../solvers/#Laplacians.KMPSDDMSolver-Tuple{Any}"><code>KMPSDDMSolver</code></a></li></ul><h2 id="Sampling-Solvers-of-Kyng-and-Sachdeva-1"><a class="docs-heading-anchor" href="#Sampling-Solvers-of-Kyng-and-Sachdeva-1">Sampling Solvers of Kyng and Sachdeva</a><a class="docs-heading-anchor-permalink" href="#Sampling-Solvers-of-Kyng-and-Sachdeva-1" title="Permalink"></a></h2><p>These are inspired by the paper &quot;Approximate Gaussian Elimination for Laplacians: Fast, Sparse, and Simple&quot; by Rasmus Kyng and Sushant Sachdeva, FOCS 2016.</p><p>These first two follow that paper reasonably closely.</p><ul><li><a href="@ref"><code>samplingSDDMSolver</code></a></li><li><a href="@ref"><code>samplingLapSolver</code></a></li></ul><p>The following is a modification of the algorithm that eliminates edges one at a time. This solver is by Yuan Gao, Rasmus Kyng, and Daniel A. Spielman.</p><p>The algorithm has not yet been analyzed.  It is presently the fastest in this package, and our recommended choice.</p><ul><li><a href="../solvers/#Laplacians.approxchol_lap-Union{Tuple{SparseArrays.SparseMatrixCSC{Tv, Ti}}, Tuple{Ti}, Tuple{Tv}} where {Tv, Ti}"><code>approxchol_lap</code></a></li></ul><p>A second version is roughly a factor 2 slower, but more robust:</p><ul><li><a href="../solvers/#Laplacians.approxchol_lap2-Union{Tuple{SparseArrays.SparseMatrixCSC{Tv, Ti}}, Tuple{Ti}, Tuple{Tv}} where {Tv, Ti}"><code>approxchol_lap2</code></a></li></ul><p>A detailed description of <a href="../solvers/#Laplacians.approxchol_lap-Union{Tuple{SparseArrays.SparseMatrixCSC{Tv, Ti}}, Tuple{Ti}, Tuple{Tv}} where {Tv, Ti}"><code>approxchol_lap</code></a> and <a href="../solvers/#Laplacians.approxchol_lap2-Union{Tuple{SparseArrays.SparseMatrixCSC{Tv, Ti}}, Tuple{Ti}, Tuple{Tv}} where {Tv, Ti}"><code>approxchol_lap2</code></a> and experimental evaluation of them can be found in the paper &quot;Robust and Practical Solution of Laplacian Equations by Approximate Elimination&quot; by Yuan Gao, Rasmus Kyng, and Daniel A. Spielman. Paper link: https://arxiv.org/abs/2303.00709.</p><h2 id="Algebraic-Multigrid-1"><a class="docs-heading-anchor" href="#Algebraic-Multigrid-1">Algebraic Multigrid</a><a class="docs-heading-anchor-permalink" href="#Algebraic-Multigrid-1" title="Permalink"></a></h2><p>This is an interface to the algebraic multigrid solvers from the PyAMG package.</p><ul><li><a href="@ref"><code>AMGLapSolver</code></a></li><li><a href="@ref"><code>AMGSolver</code></a>, for SDDM systems.</li></ul><h2 id="Solvers-from-Matlab-1"><a class="docs-heading-anchor" href="#Solvers-from-Matlab-1">Solvers from Matlab</a><a class="docs-heading-anchor-permalink" href="#Solvers-from-Matlab-1" title="Permalink"></a></h2><p>The <a href="https://github.com/JuliaInterop/MATLAB.jl">MATLAB.jl</a> package allows Julia to call routines from Matlab, provided you have Matlab installed.  It does this in a very efficient fashion: it starts up the Matlab process when you type <code>using MATLAB</code>, and then communicates with it.  So, we have wrapped some solvers from Matlab so that they obey the same interface.</p><p>These are not part of the Laplacians module, but are included in the package under <code>src/matlabSolvers.jl</code>.  To include them, type</p><pre><code class="language-julia">include(string(Pkg.dir(&quot;Laplacians&quot;) , &quot;/src/matlabSolvers.jl&quot;))</code></pre><p>We provide the docstrings for these here.</p><h3 id="Incomplete-Cholesky-Factorizations-1"><a class="docs-heading-anchor" href="#Incomplete-Cholesky-Factorizations-1">Incomplete Cholesky Factorizations</a><a class="docs-heading-anchor-permalink" href="#Incomplete-Cholesky-Factorizations-1" title="Permalink"></a></h3><p>These use the no-fill incomplete Cholesky factorizations implemented in Matlab.  They first order the vertices by the <code>symrcm</code> ordering.</p><p>The solvers are:</p><ul><li><code>f = matlab_ichol_sddm(sddm; tol, maxtime, maxits, pctIts, verbose)</code></li><li><code>f = matlab_ichol_lap(A; tol, maxtime, maxits, pctIts, verbose)</code></li></ul><p>A routine that just wraps the function that solves equations in the preconditioner is provided as well:</p><ul><li><code>f = matlab_ichol(sddm)</code></li></ul><h3 id="Koutis&#39;s-Combinatorial-Multigrid-(CMG)-1"><a class="docs-heading-anchor" href="#Koutis&#39;s-Combinatorial-Multigrid-(CMG)-1">Koutis&#39;s Combinatorial Multigrid (CMG)</a><a class="docs-heading-anchor-permalink" href="#Koutis&#39;s-Combinatorial-Multigrid-(CMG)-1" title="Permalink"></a></h3><p>You must have installed Yiannis Koutis&#39;s <a href="http://www.cs.cmu.edu/~jkoutis/cmg.html">Combinatorial Multigrid Code</a>, and it must be on Matlab&#39;s default path.  As this code returns a function rather than a preconditioner, it would be inefficient to make it use our PCG code and satisfy our interface.  So, it does not.</p><ul><li><code>x = matlabCmgSolver(mat, b; tol::Real=1e-6, maxits=10000)</code></li></ul><p>The matrix <code>mat</code> can either be SDDM or a Laplacian.  This solves the system in <code>b</code>.</p><p>If you need to specify the solver separately from <code>b</code>, you can call</p><ul><li><code>x = matlabCmgSolver(mat; tol::Real=1e-6, maxits=10000)</code></li></ul><p>or, for the Laplacians of the adjacency matrix <code>A</code>,</p><ul><li><code>x = matlabCmgLap(A; tol::Real=1e-6, maxits=10000)</code></li></ul><p>However, this does not create the solver.  It merely returns a call to the previous routine.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../CSCgraph/">« Sparse matrices as graphs</a><a class="docs-footer-nextpage" href="../LSST/">Low Stretch Spanning Trees »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 3 April 2024 08:29">Wednesday 3 April 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
