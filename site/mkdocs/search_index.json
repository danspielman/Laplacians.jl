{
    "docs": [
        {
            "location": "/about/index.html", 
            "text": "A package for graph computations related to graph Laplacians\n\n\nGraphs are represented by sparse adjacency matrices, etc.", 
            "title": "About"
        }, 
        {
            "location": "/Julia/index.html", 
            "text": "Using Julia\n\n\nConverting to Julia 0.4\n\n\nSmall details\n\n\n\n\n\n\nJulia Notebooks\n\n\nWorkflows\n\n\nDan's current workflow:\n\n\nAdd your current workflow here:\n\n\n\n\n\n\nThings to be careful of (common bugs)\n\n\nUseful Julia functions\n\n\nOptimizing code in Julia\n\n\nVectorization is Bad.\n\n\n\n\n\n\nHow should Julia packages be organized?\n\n\nHow should notebooks play with Git?\n\n\n\n\n\n\n\n\n\n\nUsing Julia\n\n\nTo use the julia notebooks, you will need ipython and the IJulia package.  You should also get \njupyter\n, which you should be able to install from ipython.\nTo install IJulia, you type \nPkg.add(\"IJulia\")\n from Julia.\nThen, you just need to type \nusing IJulia\n once.  This will tell jupyter about the Julia kernel.  To run Julia notebooks, you now type \njupyter notebook\n.  You can select the kernel your new notebook is using.\n\n\nDan recommends installing the anaconda distribution of python.\nYou will then need to install some things from that, like\nconda install jupyter (the new notebooks package)\nconda install mathjax\nconda install matplotlib\n\n\nThis repository contains projects implemented in Julia by Dan Spielman's group.  While organizing by language is strange, we are trying it to help us learn the language.\n\n\nThere are two projects in here so far.  One, yinsGraph, is for doing graph theory and solving Laplacian systems.  It's documentation is at \nyinsGraph.md\n\n\nI would like to use the documentation pages in this root directory to discuss issues with how to get Julia to work well.  For now, I'll just ask some questions and write a little that I've figured out.  If you figure some out, please write it here.\n\n\nConverting to Julia 0.4\n\n\nA description of the changes made in Julia 0.4 appears to be here\n[https://github.com/JuliaLang/julia/blob/release-0.4/NEWS.md]\n(https://github.com/JuliaLang/julia/blob/release-0.4/NEWS.md)\n\n\nThe first problem you will encounter when using Julia 0.4 is that it is stingier about its load path.  It won't load something from the current directory unless it is in the load path.  You can add a directory to the load path like\n\n\npush!(LOAD_PATH,\n.\n)\n\n\n\n\nor\n\n\npush!(LOAD_PATH,\n/Users/[your_username]/git/julia/yinsGraph\n)\n\n\n\n\nTo overcome this issue, you can add the above line to the end of the julia.rc file, found in\n\n\n/Applications/Julia-0.4.0-rc4.app/Contents/Resources/julia/etc/julia\n\n\n\n\nJulia 0.4 lets you take advantage of docstrings.\nFor example, \n?ringGraph\n produces\n\n\nThe simple ring on n vertices\n\n\n\n\nWhen having a multiline comment, make sure that lines don't have starting and trailing spaces.\nThis will mess up the indentation when calling '?func_name'.\n\n\nSmall details\n\n\n\n\nJulia 0.4 is trying to wean you off Matlab-like notation.  You should no longer create vectors like \n[1:n]\n.  Instead, you should type \ncollect(1:n)\n\n\n\n\nJulia Notebooks\n\n\nTo get the Julia notebooks working, I presently type \njupyter notebook\n.\nI then select the kernel to be Julia-0.3.11.\nIt seems important to run this command from a directory that contains all the directories\nthat have notebooks that you will use.  In particular, I advise against \"uploading\" notebooks\nfrom other directories.  That has only given me trouble.\n\n\nThe calico extensions that seem to be hosted at Brynmawr seem interesting.\nI haven't yet figured out how to get them to work.\nHere are the relevent links:\n\n\n\n\nhttp://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Help.ipynb\n\n\nhttp://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Notebook%20Users%20Manual.ipynb\n\n\n\n\nTo turn a notebook into html, you type something like\n\n\nipython nbconvert Laplacians.ipynb\n\n\n\n\nWorkflows\n\n\nJulia has an IDE called Juno.  Both Dan and Serban have encountered some trouble with it: we have both found that it sometimes refuses to reload .jl code that we have written.  Please document workflows that you have found useful here:\n\n\nDan's current workflow:\n\n\n\n\nI use emacs (which has a mode for Julia) and the notebooks.\n\n\nI develop Julia code in a \"temporary\" file with a name like develX.jl.  While I am developing, this code is not included by the module to which it will eventually belong.\n\n\n\n\nAfter modifying code, I reload it with \ninclude(\"develX.jl\")\n.  This works fine for reloading methods.  It is not a good way to reload modules or types.  So, I usually put the types either in a separate file, or in my julia notebook.\n\n\n\n\n\n\nI am writing this documention in MacDown.\n\n\n\n\n\n\nAdd your current workflow here:\n\n\nThings to be careful of (common bugs)\n\n\n\n\n\n\nJulia passes vectors and matrices to routines by reference, rather than by copying them.  If you type \nx = y\n when x and y are arrays, then this will make x a pointer to y.  If you want x to be a copy of y, type \nx = copy(y)\n.  This can really mess up matlab programmers.  I wrote many functions that were modifying their arguments without realizing it.\n\n\n\n\n\n\nOn the other hand, if you type \nx = x + y\n, then x becomes a newly allocated vector and no longer refers to the original.  This is true even if you type \nx += y\n.  Here is an example that shows two of the possible behaviors, and the difference between what happens inside functions.\n\n\n\n\n\n\n\n\nAdds b in to a\n\nfunction addA2B(a,b)\n    for i in 1:length(a)\n        a[i] += b[i]\n    end\nend\n\n\nFails to add b in to a\n\nfunction addA2Bfail(a,b)\n    a += b\nend\n\na = [1 0]\nb = [2 2]\naddA2B(a,b)\na\n\n1x2 Array{Int64,2}:\n 3  2\n\na = [1 0]\nb = [2 2]\naddA2Bfail(a,b)\na\n\n1x2 Array{Int64,2}:\n 1  0\n\na += b\na\n\n1x2 Array{Int64,2}:\n 3  2\n\n\n\n\n\n\n\n\n\nIf you are used to programming in Matlab, you might be tempted to type a line like \nfor i in 1:10,\n.  \nDo not put extra commas in Julia!\n  It will cause bad things to happen.\n\n\n\n\n\n\nJulia sparse matrix entries dissapear if they are set to 0. In order to overcome this, use the \nsetValue\n function. \nsetValue(G, u, i, 0)\n will set \nweighti(G, u, i)\n to 0 while also leaving \n(u, nbri(G, u, i))\n in the matrix.\n\n\n\n\n\n\nUseful Julia functions\n\n\nI am going to make a short list of Julia functions/features that I find useful.  Please add those that you use often as well.\n\n\n\n\n\n\ndocstrings: in the above example, I used a docstring to document each function.  You can get these by typing \n?addA2B\n.  You can also  \nwrite longer docstrings and use markdown\n.  I suggest putting them in front of every function.\n\n\n\n\n\n\nmethods(foo)\n lists all methods with the name foo.\n\n\n\n\nfieldnames(footype)\n tells you all the fields of footype.  Note that this is 0.4.  In 0.3.11, you type \nnames(footype)\n\n\n\n\njulia\n a = sparse(rand(3,3));\njulia\n fieldnames(a)\n5-element Array{Symbol,1}:\n :m\n :n\n :colptr\n :rowval\n :nzval\n\n\n\n\nOptimizing code in Julia\n\n\nThe best way that I've found of figuring out what's slowing down my code has been to use \n@code_warntype\n.  It only exists in version 4 of Julia.  For this reason, I keep one of those around.\n\n\nNote that the first time you run a piece of code in Julia, it gets compiled.  So, you should run it on a small example before trying to time it.  Then, use \n@time\n to time your code.\n\n\nI recommend reading the Performance Tips in the Julia documentation, not that I've understood all of it yet.\n\n\nVectorization is Bad.\n\n\nJulia is the anti-matlab in that vectorization is slow.\nStill it is a good way to write your code the first time.\nHere are some examples of code that adds one vector into another.\nThe first is vectorized, the second turns that into a loop, and the fastest uses BLAS.  Note that this was done in Julia 0.3.11.  The vectorized code is much faster, but still not fast, in 0.4.\n\nAlso note that you have to run each routine once before it will be fast.  This is because it compiles it the first time your run it\n\n\nn = 10^7\na = rand(n)\nb = rand(n)\n@time a += b;\n\nelapsed time: 0.155296017 seconds (80000312 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time add2(a,b);\n\nelapsed time: 0.021190554 seconds (80 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time BLAS.axpy!(1.0,b,a);\n\nelapsed time: 0.015894922 seconds (80 bytes allocated)\n\n\n\n\n\nOne reason that \na += b\n was slow was that it seems to allocate a lot of memory.\n\n\nHow should Julia packages be organized?\n\n\nIn yinsGraph, I decided to just make one big module called yinsGraph.jl.  It then includes a bunch of individual files, most of which contain many functions and types.  I think this is much nicer than making one file per functions, as some functions are very short.\n\n\nI put the export statements in the main module.  The reason for this is that while developing code in a file, I don't include that in the module.  This way I can reload it as I change it without having to restart the kernel.  This does not seem to work as well for types.  I'm not sure why.\n\n\nHow should notebooks play with Git?\n\n\nThe great thing about the notebooks is that they contain live code, so that you can play with them.  But, sometimes you get a version that serves as great documentation, and you don't want to klobber it my mistake later (or evern worse, have someone else klobber it).  Presumably if someone accidently commits a messed up version we can unwind that.  But, is there a good way to keep track of this?", 
            "title": "Using Julia"
        }, 
        {
            "location": "/Laplacians/index.html", 
            "text": "Laplacians\n\n\nTo install Laplacians\n\n\nTo use Laplacians\n\n\nAbout this Documentation\n\n\nSome Documentation that hasn't yet made it elsewhere.\n\n\nFundamental Graph Algorithms:\n\n\nSolving Linear equations:\n\n\n\n\n\n\nTo develop Laplacians\n\n\nUsing sparse matrices as graphs\n\n\nParametric Types\n\n\nData structures:\n\n\nInterface issue:\n\n\nWriting tests:\n\n\n\n\n\n\nIntegration with other packages.\n\n\n\n\n\n\n\n\n\n\nLaplacians\n\n\nLaplacians.jl is a package for analyzing graphs in Julia.  Our emphsis is on things related to Laplacian linear sytems, such as solving equations, sparsifying, computing eigenvectors, partitioning, and other graph optimization tasks.\n\n\nThe graphs are represented as sparse matrices.  The particular class in Julia is called a SparseMatrixCSC.  The reasons for this are:\n\n\n\n\nThey are fast, and\n\n\nWe want to do linear algebra with them, so matrices help.\n\n\n\n\nYou can probably learn more about the CSC (Compressed Sparse Column) format by googling it.\n\n\nSo far, speed tests of code that we've written for connected components, shorest paths, and minimum spanning trees have been as fast or faster than the previous routines we could call from Matlab.\n\n\nTo install Laplacians\n\n\nyou will need a number of packages.\nYou install these like\n\n\nPkg.add(\nPyCall\n)\nPkg.add(\nPyPlot\n)\nPkg.add(\nDataStructures\n)\n\n\n\n\nI also recommend the Optim package.\n\n\nI think you need to install matplotlib in python before PyPlot.\nLook at this page for more information: https://github.com/stevengj/PyPlot.jl\n\n\nI'm not sure if there are any others.  If you find that there are, please list them above.\n\n\nTo use Laplacians\n\n\nExamples of how to do many things in yinsGraph may be found in the IJulia notebooks.  These have the extensions .ipynb.  When they look nice, I think it makes sense to convert them to .html.\n\n\nRight now, the notebooks worth looking at are:\n\n\n\n\nyinsGraph\n - usage, demo, and speed tests (Laplacians was previously called yinsGraph)\n\n\nSolvers\n - code for solving equations.  How to use direct methods, conjugate gradient, and a preconditioned augmented spanning tree solver.\n\n\n\n\n(I suggest that you open the html in your browser)\n\n\nAbout this Documentation\n\n\nThis documentation is still very rough.\nIt is generated by a combination of Markdown and semi-automatic generation.  The steps to generate and improve it are:\n\n\n\n\nEdit Markdown files in the \ndocs\n directory.  For example, you could use MacDown to do this.\n\n\nIf you want to add a new page to the documention, create one.  Edit the file mkdocs.yml so show where it should appear.\n\n\nRun \nmkdocs build\n in the root directory to regenerate the documenttion from the Markdown.\n\n\nAdd Julia 0.4 docstrings to every function that you want to appear in the API.  You do not need to give the function template--that appears automatically.\n\n\nTo update the API part of the Markdown files, from the root directory run\n\n\n\n\ninclude(\ndocs/build.jl\n)\n\n\n\n\n\n\nThe generation of the API index is a modification of code that I found in a package called LightGraph.jl.  I have very little idea of how it works, so it might be buggy.  We should find a better way later.  The Lexicon package looked like a good possibility, but it lacks a lot of features that I would like.  In particular, it can only organize APIs by module, not by file.\n\n\n\n\nSome Documentation that hasn't yet made it elsewhere.\n\n\nFundamental Graph Algorithms:\n\n\n\n\ncomponents\n computes connected components, returns as a vector\n\n\nvecToComps\n turns into an array with a list of vertices in each component\n\n\nshortestPaths(mat, start)\n  returns an array of distances,\n    and pointers to the node closest (parent array)\n\n\nkruskal(mat; kind=:min)\n  to get a max tree, use \nkind = :max\n\n    returns it as a sparse matrix.\n\n\n\n\nSolving Linear equations:\n\n\nWe have implemented Conjugate Gradient (cg) and the Preconditioned Conjugate Gradient (pcg).  These implementations use BLAS when they can, and a slower routine for data types like BigFloat.\n\n\nTo learn more, read \nsolvers.md\n.\n\n\nTo develop Laplacians\n\n\nJust go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.\nThe yinsGraph.ipynb notebook contains some examples of speed tests.\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"\n\n\nI think that each file should contain a manifest up top listing the functions and types that it provides.  They should be divided up into those that are for internal use only, and those that should be exported.  Old code that didn't work well, but which you want to keep for reference should go at the end.\n\n\nUsing sparse matrices as graphs\n\n\nThe routines \ndeg\n, \nnbri\n and \nweighti\n will let you treat a sparse matrix like a graph.\n\n\ndeg(graph, u)\n is the degree of node u.\n\nnbri(graph, u, i)\n is the ith neighbor of node u.\n\nweighti(graph, u, i)\n is the weight of the edge to the ith neighbor of node u.\n\n\nNote that we start indexing from 1.\n\n\nFor example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:\n\n\n  for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end\n\n\n\n\nBut, this turns out to be much slower than working with the structure directly, like\n\n\n  for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end\n\n\n\n\n\n\n[ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.\n\n\n\n\nParametric Types\n\n\nA sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\n\n\n\n\nTv\n, sometimes written \nTval\n denotes the types of the values, and \nTi\n or \nTind\n denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths, \nstart\n is of type \nTi\n.  Inside the code, whenever we write something like \npArray = zeros(Ti,n)\n, it creates an array of zeros of type Ti.  Using these parameteric types is \nmuch\n faster than leaving the types unfixed.\n\n\nData structures:\n\n\n\n\nIntHeap\n a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.\n\n\n\n\nInterface issue:\n\n\nThere are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.\n\n\nWriting tests:\n\n\nI haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.\n\n\nIntegration with other packages.\n\n\nThere are other graph packages that we might want to sometimes use.\n\n\n\n\nGraphs.jl\n : I found this one to be too slow and awkward to be useful.\n\n\nLightGraphs.jl\n : this looks more promising.  We will have to check it out.", 
            "title": "Overview"
        }, 
        {
            "location": "/yinsGraph/index.html", 
            "text": "yinsGraph\n\n\nTo install yinsGraph\n\n\nTo use yinsGraph\n\n\nGraph generators:\n\n\nOperations on Graphs:\n\n\nFundamental Graph Algorithms:\n\n\nSolving Linear equations:\n\n\n\n\n\n\nTo develop yinsGraph\n\n\nUsing sparse matrices as graphs\n\n\nParametric Types\n\n\nData structures:\n\n\nInterface issue:\n\n\nWriting tests:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyinsGraph\n\n\nyinsGraph is a package that I (Dan) am writing to explore and manipulate graphs in Julia.  The graphs are represented as sparse matrices.  The particular class in Julia is called a SparseMatrixCSC.  The reasons for this are:\n\n\n\n\nThey are fast, and\n\n\nWe want to do linear algebra with them, so matrices help.\n\n\n\n\nYou can probably learn more about the CSC (Compressed Sparse Column) format by googling it.\n\n\nSo far, speed tests of code that I've written for connected components, shorest paths, and minimum spanning trees have been as fast or faster than the previous routines I could call from Matlab.\n\n\nTo install yinsGraph\n\n\nyou will need a number of packages.\nYou install these like\n\n\nPkg.add(\nPyCall\n)\nPkg.add(\nPyPlot\n)\nPkg.add(\nDataStructures\n)\n\n\n\n\nI also recommend the Optim package.\n\n\nI think you need to install matplotlib in python before PyPlot.\nLook at this page for more information: https://github.com/stevengj/PyPlot.jl\n\n\nI'm not sure if there are any others.  If you find that there are, please list them above.\n\n\nTo use yinsGraph\n\n\nExamples of how to do many things in yinsGraph may be found in the IJulia notebooks.  These have the extensions .ipynb.  When they look nice, I think it makes sense to convert them to .html.\n\n\nRight now, the notebooks worth looking at are:\n\n\n\n\nyinsGraph\n - usage, demo, and speed tests\n\n\n\n\nSolvers\n - code for solving equations.  How to use direct methods, conjugate gradient, and a preconditioned augmented spanning tree solver.\n\n\n\n\n\n\nIterativeSolvers\n The implementation of CG in IterativeSolvers sort of sucks, as I now see in the tests.  It is allocating way to much memory.  It should be fixed either by devectorizing (see Julia Performance Tips), or by using BLAS routines.\n\n\n\n\n\n\n(I suggest that you open the html in your browser)\n\n\nGraph generators:\n\n\n readIJ(filename::String)\n readIJV(filename::String)\n writeIJV(filename::String, mat)\n ringGraph(n::Int64)\n generalizedRing(n::Int64, gens)\n randMatching(n::Int64)\n randRegular(n::Int64, k::Int64)\n grownGraph(n::Int64, k::Int64)\n grownGraphD(n::Int64, k::Int64)\n prefAttach(n::Int64, k::Int64, p::Float64)\n hyperCube(d::Int64)\n completeBinaryTree(n::Int64)\n grid2(n::Int64)\n grid2(n::Int64, m::Int64; isotropy=1)\n grid2coords(n::Int64, m::Int64)\n\n\n\n\n\n\n[ ] The types in the arguments of those should probably be more flexible/general.\n\n\n\n\nFor example, to generate a 4-by-5 grid, you type\n\n\ngraph = grid2(4,5)\n\n\n\n\nOperations on Graphs:\n\n\n\n\nshortIntGraph\n  for converting the index type of a graph to an Int32.\n\n\nlap\n  to produce the laplacian of a graph\n\n\n[ ] Maybe this should grab the upper triangular part, and symmetrize first.\n\n\nunweight\n - change all the weights to 1\n\n\nmapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind},f)\n  to apply the function f to the weight of every edge.\n\n\nuniformWeight\n  an example of mapweight.  It ignores the weight, and maps every weight to a random in [0,1]\n\n\nproductGraph(a0::SparseMatrixCSC, a1::SparseMatrixCSC)\n the cartesian product.  Given two paths it makes a grid.\n\n\nedgeVertexMat(mat::SparseMatrixCSC)\n  signed edge vertex matrix\n\n\nsubsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)\n\n  produce a new graph that keeps each edge with probability p.\n\n\ntwoLift(a, k)\n create a 2-lift of a with k flipped edges.  If k is unspecified, this generates a random 2-lift.\n\n\njoinGraphs(a, b, k)\n create a disjoint union of a and b, and add k random edges between them\n\n\nplotGraph(gr,x,y,color=[0,0,1];dots=true,setaxis=true,number=false)\n\n\nspectralDrawing(graph)\n\n\n\n\nFundamental Graph Algorithms:\n\n\n\n\ncomponents\n computes connected components, returns as a vector\n\n\nvecToComps\n turns into an array with a list of vertices in each component\n\n\nshortestPaths(mat, start)\n  returns an array of distances,\n    and pointers to the node closest (parent array)\n\n\nkruskal(mat; kind=:min)\n  to get a max tree, use \nkind = :max\n\n    returns it as a sparse matrix.\n\n\n\n\nSolving Linear equations:\n\n\nWe have implemented Conjugate Gradient (cg) and the Preconditioned Conjugate Gradient (pcg).  These implementations use BLAS when they can, and a slower routine for data types like BigFloat.\n\n\nTo learn more, read \nsolvers.md\n.\n\n\nTo develop yinsGraph\n\n\nJust go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.\nThe yinsGraph.ipynb notebook contains some examples of speed tests.\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"\n\n\nI think that each file should contain a manifest up top listing the functions and types that it provides.  They should be divided up into those that are for internal use only, and those that should be exported.  Old code that didn't work well, but which you want to keep for reference should go at the end.\n\n\nUsing sparse matrices as graphs\n\n\nThe routines \ndeg\n, \nnbri\n and \nweighti\n will let you treat a sparse matrix like a graph.\n\n\ndeg(graph, u)\n is the degree of node u.\n\nnbri(graph, u, i)\n is the ith neighbor of node u.\n\nweighti(graph, u, i)\n is the weight of the edge to the ith neighbor of node u.\n\n\nNote that we start indexing from 1.\n\n\nFor example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:\n\n\n  for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end\n\n\n\n\nBut, this turns out to be much slower than working with the structure directly, like\n\n\n  for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end\n\n\n\n\n\n\n[ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.\n\n\n\n\nParametric Types\n\n\nA sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\n\n\n\n\nTv\n, sometimes written \nTval\n denotes the types of the values, and \nTi\n or \nTind\n denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths, \nstart\n is of type \nTi\n.  Inside the code, whenever we write something like \npArray = zeros(Ti,n)\n, it creates an array of zeros of type Ti.  Using these parameteric types is \nmuch\n faster than leaving the types unfixed.\n\n\nData structures:\n\n\n\n\nIntHeap\n a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.\n\n\n\n\nInterface issue:\n\n\nThere are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.\n\n\nWriting tests:\n\n\nI haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.", 
            "title": "yinsGraph"
        }, 
        {
            "location": "/solvers/index.html", 
            "text": "Solving linear equations in Laplacians\n\n\nDirect Solvers\n\n\nIterative Solvers\n\n\nLow-Stretch Spanning Trees\n\n\nAugmented spanning tree preconditioners\n\n\n\n\n\n\n\n\n\n\nSolving linear equations in Laplacians\n\n\nRight now, our solver code is in \nsolvers.jl\n, but not included in yinsGraph.  So, you should include this directly.  Implementations of cg and pcg have been automatically included in yinsGraph.  They are in the file \npcg.jl\n\n\nFor some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.\n\n\nDirect Solvers\n\n\nYou can compute a cholesky factor directly with \ncholfact\n.  It does  more than just compute the factor, and it saves its result in a data structure that implements \n\\\n.  It uses SuiteSparse by Tim Davis.\n\n\nHere is an example of how you would use it to solve a general non-singular linear system.\n\n\na = grid2(5)\nla = lap(a)\nla[1,1] = la[1,1] + 1\nF = cholfact(la)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(la*x-b)\n\n    1.0598778281116327e-14\n\n\n\n\nLaplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.\n\n\nla = lap(a)\nf = lapWrapSolver(cholfact,la)\nb = randn(n); b = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15\n\n\n\n\nHere are two other ways of using the wrapper:\n\n\nlapChol = lapWrapSolver(cholfact)\nf = lapChol(la)\nb = randn(n);\nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.6924696662484416e-15\n\nx = lapWrapSolver(cholfact,la,b)\nnorm(la*x - b)\n    2.6924696662484416e-15\n\n\n\n\nIterative Solvers\n\n\nThe first, of course, is the Conjugate Gradient (cg).\n\n\nOur implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance \ntol\n and the maximum number of iterations, \nmaxits\n.  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  Here are examples.\n\n\nn = 50\na = randn(n,n); a = a * a';\nb = randn(n)\nx = cg(a,b,maxits=100)\nnorm(a*x - b)\n    1.2191649497921835e-6\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(a,bbig,maxits=100)\nnorm(a*xbig - bbig)\n    1.494919244242202629856363570306545126541716514824419323325986374186529786019681e-33\n\n\n\n\nAs a sanity check, we do two speed tests against Matlab.\n\n\nla = lap(grid2(200))\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n@time x = cg(la,b,maxits=1000)\n    0.813791 seconds (2.77 k allocations: 211.550 MB, 3.56% gc time)\n\nnorm(la*x-b)\n    0.0001900620047823064\n\n\n\n\nAnd, in Matlab:\n\n\n a = grid2(200);\n\n la = lap(a);\n\n b = randn(length(a),1); b = b - mean(b);\n\n tic; x = pcg(la,b,[],1000); toc\npcg converged at iteration 688 to a solution with relative residual 9.8e-07.\nElapsed time is 1.244917 seconds.\n\n norm(la*x-b)\n\nans =\n\n   1.9730e-04\n\n\n\n\nPCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.\n\n\na = mapweight(grid2(200),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\npre(x) = x ./ d\n@time x = pcg(la,b,pre,maxits=2000)\n    3.322035 seconds (42.21 k allocations: 1.194 GB, 5.11% gc time)\nnorm(la*x-b)\n    0.008508746034886803\n\n\n\n\nIf our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.\n\n\n@time x = pcg(la,b,pre,tol=1e-1,maxits=10^5)\n    0.747042 seconds (9.65 k allocations: 275.819 MB, 4.87% gc time)\nnorm(la*x-b)\n    19.840756251253442\n\n@time x = cg(la,b,tol=1e-1,maxits=10^5)\n    6.509665 seconds (22.55 k allocations: 1.680 GB, 3.68% gc time)\nnorm(la*x-b)\n    19.222483530605043\n\n\n\n\nLow-Stretch Spanning Trees\n\n\nIn order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code in Julia that is guaranteed to produce these.  Instead, for now, we have two routines that can be thought of as randomized versions of Prim and Kruskall's algorithm.\n\nrandishKruskall\n samples the remaining edges with probability proportional to their weight.  \nrandishPrim\n samples edges on the boundary while using the same rule.\n\n\nBoth use a data structure called \nSampler\n that allows you to store integers with real values, and to sample according to those real values.\n\n\nWe also have code for computing the stretches.\nHere are some examples.\n\n\na = grid2(1000)\nt = randishKruskal(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    43.410262262262265\n\nt = randishPrim(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    33.14477077077077\n\n\n\n\n\nAugmented spanning tree preconditioners\n\n\nHere is code that will invoke one.\nIt is designed for positive definite systems.  So, let's give it one.\nRight now, it is using a randomized version of a MST.  There is no real reason to think that this should work.\n\n\na = mapweight(grid2(1000),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nla[1,1] = la[1,1] + 1\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000)\n    6.529052 seconds (4.00 M allocations: 1.858 GB, 15.34% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.058915 seconds (9.74 k allocations: 23.209 GB, 6.84% gc time)\n\nnorm(la*x - b)\n    99.74452367765869\n\n# Now, let's contrast with using CG\n\n@time y = cg(la,b,tol=1e-1,maxits=1000)\n    28.719631 seconds (4.01 k allocations: 7.473 GB, 3.74% gc time)\n\nnorm(la*y-b)\n    3243.6014713600766\n\n\n\n\n\nThat was not too impressive.  We will have to investigate.  By default, it presently uses randishKruskal.  Let's try randishPrim.  You can pass the treeAlg as a parameter.\n\n\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000,treeAlg=randishPrim);\n    6.319489 seconds (4.00 M allocations: 2.030 GB, 18.81% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.503484 seconds (9.76 k allocations: 23.268 GB, 7.31% gc time)\n\nnorm(la*x - b)\n    99.29610874176991\n\n\n\n\nTo solve systems in a Laplacian, we could wrap it.\n\n\nn = 40000\nla = lap(randRegular(n,3))\nf = lapWrapSolver(augTreeSolver,la,tol=1e-6,maxits=1000)\nb = randn(n); b = b - mean(b)\nx = f(b)\nnorm(la*x-b)\n    0.00019304778073388\n\n\n\n\nAs you can see, lapWrapSolver can pass tol and maxits arguments to its solver, if they are given to it.", 
            "title": "Solvers"
        }, 
        {
            "location": "/generators/index.html", 
            "text": "The following is a list of the graph generators.\n\n\nDeterministic\n\n\ncompleteGraph\n\n\ncompleteGraph(n::Int64)\n\n\n\n\nThe complete graph \n\n\npathGraph\n\n\npathGraph(n::Int64)\n\n\n\n\nThe path graph on n vertices \n\n\nringGraph\n\n\nringGraph(n::Int64)\n\n\n\n\nThe simple ring on n vertices\n\n\ngeneralizedRing\n\n\ngeneralizedRing(n::Int64, gens)\n\n\n\n\nA generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example, \n\n\ngeneralizedRing(17, [1 5])\n\n\n\n\nhyperCube\n\n\nhyperCube(d::Int64)\n\n\n\n\nThe d dimensional hypercube.  Has 2^d vertices\n\n\ncompleteBinaryTree\n\n\ncompleteBinaryTree(n::Int64)\n\n\n\n\nThe complete binary tree on n vertices\n\n\ngrid2\n\n\ngrid2(n::Int64)\ngrid2(n::Int64, m::Int64)\n\n\n\n\nAn n-by-m grid graph.  iostropy is the weighting on edges in one direction.\n\n\ngrid2coords\n\n\ngrid2coords(n::Int64, m::Int64)\ngrid2coords(n)\n\n\n\n\nCoordinates for plotting the vertices of the n-by-m grid graph\n\n\nRandom\n\n\nThese are randomized graph generators.\n            ### randMatching\n\n\nrandMatching(n::Int64)\n\n\n\n\nA random matching on n vertices\n\n\nrandRegular\n\n\nrandRegular(n::Int64, k::Int64)\n\n\n\n\nA sum of k random matchings on n vertices\n\n\ngrownGraph\n\n\ngrownGraph(n::Int64, k::Int64)\n\n\n\n\nCreate a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.    \n\n\ngrownGraphD\n\n\ngrownGraphD(n::Int64, k::Int64)\n\n\n\n\nLike a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices\n\n\nprefAttach\n\n\nprefAttach(n::Int64, k::Int64, p::Float64)\n\n\n\n\nA preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.", 
            "title": "Generators"
        }, 
        {
            "location": "/generators/index.html#deterministic", 
            "text": "completeGraph  completeGraph(n::Int64)  The complete graph   pathGraph  pathGraph(n::Int64)  The path graph on n vertices   ringGraph  ringGraph(n::Int64)  The simple ring on n vertices  generalizedRing  generalizedRing(n::Int64, gens)  A generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example,   generalizedRing(17, [1 5])  hyperCube  hyperCube(d::Int64)  The d dimensional hypercube.  Has 2^d vertices  completeBinaryTree  completeBinaryTree(n::Int64)  The complete binary tree on n vertices  grid2  grid2(n::Int64)\ngrid2(n::Int64, m::Int64)  An n-by-m grid graph.  iostropy is the weighting on edges in one direction.  grid2coords  grid2coords(n::Int64, m::Int64)\ngrid2coords(n)  Coordinates for plotting the vertices of the n-by-m grid graph", 
            "title": "Deterministic"
        }, 
        {
            "location": "/generators/index.html#random", 
            "text": "These are randomized graph generators.\n            ### randMatching  randMatching(n::Int64)  A random matching on n vertices  randRegular  randRegular(n::Int64, k::Int64)  A sum of k random matchings on n vertices  grownGraph  grownGraph(n::Int64, k::Int64)  Create a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.      grownGraphD  grownGraphD(n::Int64, k::Int64)  Like a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices  prefAttach  prefAttach(n::Int64, k::Int64, p::Float64)  A preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.", 
            "title": "Random"
        }, 
        {
            "location": "/operations/index.html", 
            "text": "Operations on one graph\n\n\nshortIntGraph\n\n\nshortIntGraph(a::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nConvert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much\n\n\nlap\n\n\nlap(a)\n\n\n\n\nCreate a Laplacian matrix from an adjacency matrix.\n\n\nWe might want to do this differently, say by enforcing symmetry\n\n\nunweight\n\n\nunweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\nCreate a new graph in that is the same as the original, but with all edge weights 1\n\n\nmapweight\n\n\nmapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)\n\n\n\n\nCreate a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write\n\n\nb = mapweight(a, x-\nrand(1)[1])\n\n\n\n\nuniformWeight\n\n\nuniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\nPut a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.\n\n\nedgeVertexMat\n\n\nedgeVertexMat(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nThe signed edge-vertex adjacency matrix\n\n\nsubsampleEdges\n\n\nsubsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)\n\n\n\n\nCreate a new graph from the old, but keeping edge edge with probability \np\n\n\ntwoLift\n\n\ntwoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)\n\n\n\n\nCreats a 2-lift of a.  \nflip\n is a boolean indicating which edges cross\n\n\nplotGraph\n\n\nplotGraph(gr, x, y)\nplotGraph(gr, x, y, color)\n\n\n\n\nPlots graph gr with coordinates (x,y)\n\n\nspectralDrawing\n\n\nspectralDrawing(a)\n\n\n\n\nComputes spectral coordinates, and then uses plotGraph to draw\n\n\nspectralCoords\n\n\nspectralCoords(a)\n\n\n\n\nComputes the spectral coordinates of a graph\n\n\ndiagmat\n\n\ndiagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nreturns the diagonal matrix(as a sparse matrix) of a graph\n\n\nOperations on two graphs\n\n\njoinGraphs\n\n\njoinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)\n\n\n\n\ncreate a disjoint union of graphs a and b,  and then put k random edges between them\n\n\ngeneralizedNecklace\n\n\ngeneralizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti\n:Integer}, k::Int64)\n\n\n\n\nConstructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.", 
            "title": "Operations"
        }, 
        {
            "location": "/operations/index.html#operations-on-one-graph", 
            "text": "shortIntGraph  shortIntGraph(a::SparseMatrixCSC{Tv,Ti :Integer})  Convert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much  lap  lap(a)  Create a Laplacian matrix from an adjacency matrix.  We might want to do this differently, say by enforcing symmetry  unweight  unweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})  Create a new graph in that is the same as the original, but with all edge weights 1  mapweight  mapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)  Create a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write  b = mapweight(a, x- rand(1)[1])  uniformWeight  uniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})  Put a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.  edgeVertexMat  edgeVertexMat(mat::SparseMatrixCSC{Tv,Ti :Integer})  The signed edge-vertex adjacency matrix  subsampleEdges  subsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)  Create a new graph from the old, but keeping edge edge with probability  p  twoLift  twoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)  Creats a 2-lift of a.   flip  is a boolean indicating which edges cross  plotGraph  plotGraph(gr, x, y)\nplotGraph(gr, x, y, color)  Plots graph gr with coordinates (x,y)  spectralDrawing  spectralDrawing(a)  Computes spectral coordinates, and then uses plotGraph to draw  spectralCoords  spectralCoords(a)  Computes the spectral coordinates of a graph  diagmat  diagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})  returns the diagonal matrix(as a sparse matrix) of a graph", 
            "title": "Operations on one graph"
        }, 
        {
            "location": "/operations/index.html#operations-on-two-graphs", 
            "text": "joinGraphs  joinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)  create a disjoint union of graphs a and b,  and then put k random edges between them  generalizedNecklace  generalizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti :Integer}, k::Int64)  Constructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.", 
            "title": "Operations on two graphs"
        }
    ]
}