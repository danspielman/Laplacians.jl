{
    "docs": [
        {
            "location": "/index.html", 
            "text": "Laplacians.jl\n\n\n \n \n\n\nLaplacians is a package containing graph algorithms, with an emphasis on tasks related to spectral and algebraic graph theory. It contains (and will contain more) code for solving systems of linear equations in graph Laplacians, low stretch spanning trees, sparsifiation, clustering, local clustering, and optimization on graphs.\n\n\nAll graphs are represented by sparse adjacency matrices. This is both for speed, and because our main concerns are algebraic tasks. It does not handle dynamic graphs. It would be very slow to implement dynamic graphs this way.\n\n\nThe documentation may be found by clicking the \"docs\" link above.\n\n\nThis includes instructions for installing Julia, and some tips for how to start using it.  It also includes guidelines for Dan Spielman's collaborators.\n\n\nFor some examples of some of the things you can do with Laplacians, look at \n\n\n\n\nthis Julia notebook\n.\n\n\nLow Stretch Spanning Trees\n\n\nInformation about solving Laplacian equations\n\n\nAn example of sparsification\n\n\nAnd, try the chimera and wtedChimera graph generators.  They are designed to generate a wide variety of graphs so as to exercise code.\n\n\n\n\nIf you want to solve Laplacian equations, we recommend \napproxCholLap\n.\n\n\nThe algorithms provide by Laplacians.jl include:\n\n\n\n\nakpw\n, a heuristic for computing low stretch spanning trees written by Daniel Spielman, inspired by the algorithm from the paper \"A graph-theoretic game and its application to the k-server problem\" by Alon, Karp, Peleg, and West, \nSIAM Journal on Computing\n, 1995.\n\n\napproxCholLap\n: a fast heuristic for solving Laplacians equations written by Daniel Spielman, based on the paper \"Approximate Gaussian Elimination for Laplacians: Fast, Sparse, and Simple\" by Rasmus Kyng and Sushant Sachdeva, FOCS 2016.   For SDDM systems, use \napproxCholSddm\n.\n\n\nsparsify\n, an implementation of sparsification by effective resistance sampling, following Spielman and Srivastava.\n\n\nKMPLapSolver\n and \nKMPSDDSolver\n: linear equation solvers based on the paper \"Approaching optimality for solving SDD systems\" by Koutis, Miller, and Peng, \nSIAM Journal on Computing\n, 2014.\n\n\nsamplingSDDSolver\n and \nsamplingLapSolver\n, based on the paper \"Approximate Gaussian Elimination for Laplacians: Fast, Sparse, and Simple\" by Rasmus Kyng and Sushant Sachdeva, FOCS 2016.\n\n\nchimera\n and \nwtedChimera\n graph generators for testing graph algorithms, by Daniel Spielman.\n\n\nLocal Graph Clustering Heuristics, implemented by Serban Stan, including \nprn\n a version of PageRank Nibble based on \"Using PageRank to Locally Partition a Graph\", \nInternet Mathematics\n and \nLocalImprove\n based on \"Flow-Based Algorithms for Local Graph Clustering\" by Zeyuan Allen-Zhu and Lorenzo Orecchia, SODA 2014.\n\n\n\n\n\n\nCurrent Development Version\n\n\nTo get the current version of the master branch, run \nPkg.checkout(\"Laplacians\")\n\n\n\n\nVersion 0.2.1, September 18, 2017\n\n\nFixed a bug in \napproxCholSddm\n that caused it to be slow.\n\n\n\n\nVersion 0.2.0, July 17, 2017\n\n\nThis version is compatabile with Julia 0.6.  It will not work with Julia 0.5.X.\n\n\nChanges:\n\n\n\n\nAdded \napproxCholSddm\n, a wrapper of \napproxCholLap\n that solves SDDM systems.\n\n\n\n\n\n\nVersion 0.1.4, June 6, 2017\n\n\nThis is the current version.  It is what you retrieve when you run \nPkg.add(\"Laplacians\")\n. \n\n\nChanges:\n\n\n\n\nAdded \nsparsify\n, an implementation of sparsification by effective resistance sampling, following Spielman and Srivastava.\n\n\nAdded \napproxQual\n and \nconditionNumber\n for checking how well one graph approximates another.\n\n\nFixed a bug in the solution of Laplacian systems in disconnected graphs.\n\n\n\n\n\n\nVersion 0.1.3, June 2, 2017\n\n\nMajor Changes:\n\n\n\n\nChanged the name of the approximate Cholesky solver from \nedgeElimLap\n to \napproxCholLap\n.  Made improvements in this solver.\n\n\nImproved PCG so that it can now detect stagnation.  Made options to do this even better when using it with a good preconditioner, like \napproxCholLap\n.\n\n\nAdded in code for comparing the running times of solvers.  The difficulty here is that we need to stop them if they run too long.  Added code to do this with threads inside Julia, and with \ngtimeout\n when calling Matlab to use icc, CMG, or LAMG.\n\n\n\n\n\n\nVersion 0.1.2, April 2, 2017\n\n\nThis is the current version.  It is what you retrieve when you run \nPkg.add(\"Laplacians\")\n. \n\n\nMajor Changes:\n\n\n\n\nadded \nedgeElimLap\n - a fast Laplacian solver.\n\n\nfixed a bug in the unweighted version of \nakpw\n.\n\n\n\n\n\n\nVersion 0.1.1, December 26, 2016\n\n\nChangelist:\n\n\n\n\nAll of the linear equation solvers now have the same interface, and the Laplacian solvers work for disconnected graphs.\n\n\nSome support for calling solvers from Matlab has been added.\n\n\nDocumentation is now through Documenter.jl.\n\n\n\n\n\n\nVersion 0.0.3 / 0.1.0, November 20, 2016\n\n\nVersions 0.0.3 and 0.1.0 are the same. These versions works with Julia 0.5.\n\n\nWarning: the behavior of chimera and wtedChimera differs between Julia 0.4 and Julia 0.5 because randperm acts differently in these.\n\n\n\n\nVersion 0.0.2, November 19, 2016\n\n\nThis is the version that works with Julia 0.4. It was captured right before the upgrade to Julia 0.5", 
            "title": "About"
        }, 
        {
            "location": "/index.html#laplaciansjl", 
            "text": "Laplacians is a package containing graph algorithms, with an emphasis on tasks related to spectral and algebraic graph theory. It contains (and will contain more) code for solving systems of linear equations in graph Laplacians, low stretch spanning trees, sparsifiation, clustering, local clustering, and optimization on graphs.  All graphs are represented by sparse adjacency matrices. This is both for speed, and because our main concerns are algebraic tasks. It does not handle dynamic graphs. It would be very slow to implement dynamic graphs this way.  The documentation may be found by clicking the \"docs\" link above.  This includes instructions for installing Julia, and some tips for how to start using it.  It also includes guidelines for Dan Spielman's collaborators.  For some examples of some of the things you can do with Laplacians, look at    this Julia notebook .  Low Stretch Spanning Trees  Information about solving Laplacian equations  An example of sparsification  And, try the chimera and wtedChimera graph generators.  They are designed to generate a wide variety of graphs so as to exercise code.   If you want to solve Laplacian equations, we recommend  approxCholLap .  The algorithms provide by Laplacians.jl include:   akpw , a heuristic for computing low stretch spanning trees written by Daniel Spielman, inspired by the algorithm from the paper \"A graph-theoretic game and its application to the k-server problem\" by Alon, Karp, Peleg, and West,  SIAM Journal on Computing , 1995.  approxCholLap : a fast heuristic for solving Laplacians equations written by Daniel Spielman, based on the paper \"Approximate Gaussian Elimination for Laplacians: Fast, Sparse, and Simple\" by Rasmus Kyng and Sushant Sachdeva, FOCS 2016.   For SDDM systems, use  approxCholSddm .  sparsify , an implementation of sparsification by effective resistance sampling, following Spielman and Srivastava.  KMPLapSolver  and  KMPSDDSolver : linear equation solvers based on the paper \"Approaching optimality for solving SDD systems\" by Koutis, Miller, and Peng,  SIAM Journal on Computing , 2014.  samplingSDDSolver  and  samplingLapSolver , based on the paper \"Approximate Gaussian Elimination for Laplacians: Fast, Sparse, and Simple\" by Rasmus Kyng and Sushant Sachdeva, FOCS 2016.  chimera  and  wtedChimera  graph generators for testing graph algorithms, by Daniel Spielman.  Local Graph Clustering Heuristics, implemented by Serban Stan, including  prn  a version of PageRank Nibble based on \"Using PageRank to Locally Partition a Graph\",  Internet Mathematics  and  LocalImprove  based on \"Flow-Based Algorithms for Local Graph Clustering\" by Zeyuan Allen-Zhu and Lorenzo Orecchia, SODA 2014.", 
            "title": "Laplacians.jl"
        }, 
        {
            "location": "/index.html#current-development-version", 
            "text": "To get the current version of the master branch, run  Pkg.checkout(\"Laplacians\")", 
            "title": "Current Development Version"
        }, 
        {
            "location": "/index.html#version-021-september-18-2017", 
            "text": "Fixed a bug in  approxCholSddm  that caused it to be slow.", 
            "title": "Version 0.2.1, September 18, 2017"
        }, 
        {
            "location": "/index.html#version-020-july-17-2017", 
            "text": "This version is compatabile with Julia 0.6.  It will not work with Julia 0.5.X.  Changes:   Added  approxCholSddm , a wrapper of  approxCholLap  that solves SDDM systems.", 
            "title": "Version 0.2.0, July 17, 2017"
        }, 
        {
            "location": "/index.html#version-014-june-6-2017", 
            "text": "This is the current version.  It is what you retrieve when you run  Pkg.add(\"Laplacians\") .   Changes:   Added  sparsify , an implementation of sparsification by effective resistance sampling, following Spielman and Srivastava.  Added  approxQual  and  conditionNumber  for checking how well one graph approximates another.  Fixed a bug in the solution of Laplacian systems in disconnected graphs.", 
            "title": "Version 0.1.4, June 6, 2017"
        }, 
        {
            "location": "/index.html#version-013-june-2-2017", 
            "text": "Major Changes:   Changed the name of the approximate Cholesky solver from  edgeElimLap  to  approxCholLap .  Made improvements in this solver.  Improved PCG so that it can now detect stagnation.  Made options to do this even better when using it with a good preconditioner, like  approxCholLap .  Added in code for comparing the running times of solvers.  The difficulty here is that we need to stop them if they run too long.  Added code to do this with threads inside Julia, and with  gtimeout  when calling Matlab to use icc, CMG, or LAMG.", 
            "title": "Version 0.1.3, June 2, 2017"
        }, 
        {
            "location": "/index.html#version-012-april-2-2017", 
            "text": "This is the current version.  It is what you retrieve when you run  Pkg.add(\"Laplacians\") .   Major Changes:   added  edgeElimLap  - a fast Laplacian solver.  fixed a bug in the unweighted version of  akpw .", 
            "title": "Version 0.1.2, April 2, 2017"
        }, 
        {
            "location": "/index.html#version-011-december-26-2016", 
            "text": "Changelist:   All of the linear equation solvers now have the same interface, and the Laplacian solvers work for disconnected graphs.  Some support for calling solvers from Matlab has been added.  Documentation is now through Documenter.jl.", 
            "title": "Version 0.1.1, December 26, 2016"
        }, 
        {
            "location": "/index.html#version-003-010-november-20-2016", 
            "text": "Versions 0.0.3 and 0.1.0 are the same. These versions works with Julia 0.5.  Warning: the behavior of chimera and wtedChimera differs between Julia 0.4 and Julia 0.5 because randperm acts differently in these.", 
            "title": "Version 0.0.3 / 0.1.0, November 20, 2016"
        }, 
        {
            "location": "/index.html#version-002-november-19-2016", 
            "text": "This is the version that works with Julia 0.4. It was captured right before the upgrade to Julia 0.5", 
            "title": "Version 0.0.2, November 19, 2016"
        }, 
        {
            "location": "/Installation/index.html", 
            "text": "Installation\n\n\nBefore you can use Laplacians, you need Julia. So, we'll begin with instructions for installing Julia.  I (Dan S.) found that it worked best if I installed Python first.  So, I'll suggest that you do the same.\n\n\nAll of these instruction assume you are using a Mac. \n\n\n\n\nPython\n\n\nInstall python.  I recommend the anaconda distribution \nhttps://www.continuum.io/downloads\n.\n\n\nOnce you install python, you are going to want two packages: a plotting package that Julia will use, and jupyter notebooks for interacting with Julia.  Install them like\n\n\nconda install matplotlib\nconda install mathjax\nconda install jupyter\n\n\n\n\n\n\nJulia\n\n\nYou can get Julia from  \nhttp://julialang.org/\n.   If you are using a Mac, you may wish to create a symnolic link to the Julia executable so that you can call it from a terminal.  For example, you can do this like:\n\n\ncd /usr/local/bin/\nln -s julia /Applications/Julia-0.5.app/Contents/Resources/julia/bin/julia\n\n\n\n\nOnce you have this, you will want Julia notebooks.  To install this, run \njulia\n and type\n\n\njulia\n Pkg.add(\nIJulia\n)\njulia\n using IJulia\n\n\n\n\nThis will install the package, and put the current julia kernel into \njupyter\n.  In the future, you can launch the Julia notebook by typing (in a terminal)\n\n\njupyter notebook\n\n\n\n\n\n\nLaplacians\n\n\nIn theory, all you need to do now is type either\n\n\njulia\n Pkg.add(\nLaplacians\n)\n\n\n\n\nTo use the package, you then type\n\n\njulia\n using Laplacians\n\n\n\n\nThis should add all the packages upon which Laplacians explicitly depends. \n\n\nLaplacians might add some packages that you do not want, or which might not be available on your system.  If you do not want to load PyPlot, then set \n\n\njulia\n LAPLACIANS_NOPLOT = true\n\n\n\n\nbefore typing \nusing Laplacians\n. Similarly, you can avoid loading PyAmg by setting\n\n\njulia\n LAPLACIANS_NOAMG = true\n\n\n\n\nActually, defining these variables to anything will have the same effect.  So, setting them to false has the same effect as setting them to true.\n\n\nTo see if Laplacians is working, try typing\n\n\na = chimera(100,6)\nspectralDrawing(a)\n\n\n\n\nor\n\n\na = generalizedNecklace(grid2(6),grid2(3),2)\nspectralDrawing(a)", 
            "title": "Installation"
        }, 
        {
            "location": "/Installation/index.html#installation", 
            "text": "Before you can use Laplacians, you need Julia. So, we'll begin with instructions for installing Julia.  I (Dan S.) found that it worked best if I installed Python first.  So, I'll suggest that you do the same.  All of these instruction assume you are using a Mac.", 
            "title": "Installation"
        }, 
        {
            "location": "/Installation/index.html#python", 
            "text": "Install python.  I recommend the anaconda distribution  https://www.continuum.io/downloads .  Once you install python, you are going to want two packages: a plotting package that Julia will use, and jupyter notebooks for interacting with Julia.  Install them like  conda install matplotlib\nconda install mathjax\nconda install jupyter", 
            "title": "Python"
        }, 
        {
            "location": "/Installation/index.html#julia", 
            "text": "You can get Julia from   http://julialang.org/ .   If you are using a Mac, you may wish to create a symnolic link to the Julia executable so that you can call it from a terminal.  For example, you can do this like:  cd /usr/local/bin/\nln -s julia /Applications/Julia-0.5.app/Contents/Resources/julia/bin/julia  Once you have this, you will want Julia notebooks.  To install this, run  julia  and type  julia  Pkg.add( IJulia )\njulia  using IJulia  This will install the package, and put the current julia kernel into  jupyter .  In the future, you can launch the Julia notebook by typing (in a terminal)  jupyter notebook", 
            "title": "Julia"
        }, 
        {
            "location": "/Installation/index.html#laplacians", 
            "text": "In theory, all you need to do now is type either  julia  Pkg.add( Laplacians )  To use the package, you then type  julia  using Laplacians  This should add all the packages upon which Laplacians explicitly depends.   Laplacians might add some packages that you do not want, or which might not be available on your system.  If you do not want to load PyPlot, then set   julia  LAPLACIANS_NOPLOT = true  before typing  using Laplacians . Similarly, you can avoid loading PyAmg by setting  julia  LAPLACIANS_NOAMG = true  Actually, defining these variables to anything will have the same effect.  So, setting them to false has the same effect as setting them to true.  To see if Laplacians is working, try typing  a = chimera(100,6)\nspectralDrawing(a)  or  a = generalizedNecklace(grid2(6),grid2(3),2)\nspectralDrawing(a)", 
            "title": "Laplacians"
        }, 
        {
            "location": "/Julia/index.html", 
            "text": "Using Julia\n\n\nThese are some things you might want to know about using Julia if it is new to you.  There are now many other resources that can explain Julia to you.  But, we keep this section here for reference.\n\n\n\n\nDocstrings\n\n\nJulia 0.5 lets you take advantage of docstrings. For example, \n?ringGraph\n produces\n\n\nThe simple ring on n vertices\n\n\n\n\nWhen having a multiline comment, make sure that lines don't have starting and trailing spaces. This will mess up the indentation when calling '?func_name'.\n\n\n\n\nJulia Notebooks\n\n\nTo get the Julia notebooks working, I presently type \njupyter notebook\n. I then select the kernel to be Julia-0.5.0. It seems important to run this command from a directory that contains all the directories that have notebooks that you will use.  In particular, I advise against \"uploading\" notebooks from other directories.  That has only given me trouble.\n\n\nTo turn a notebook into html, you type something like\n\n\njupyter nbconvert Laplacians.ipynb\n\n\n\n\nor\n\n\njupyter nbconvert --to markdown --stdout Sampler.ipynb \n SamplerNotebook.md\n\n\n\n\n\n\nWorkflows\n\n\nJulia has an IDE called Juno.  Both Dan and Serban have encountered some trouble with it: we have both found that it sometimes refuses to reload .jl code that we have written.  Please document workflows that you have found useful here:\n\n\n\n\nDan's current workflow:\n\n\n\n\nI use emacs (which has a mode for Julia) and the notebooks.\n\n\nI develop Julia code in a \"temporary\" file with a name like develX.jl.  While I am developing, this code is not included by the module to which it will eventually belong.\n\n\nAfter modifying code, I reload it with \ninclude(\"develX.jl\")\n.  This works fine for reloading methods.  It is not a good way to reload modules or types.  So, I usually put the types either in a separate file, or in my julia notebook.\n\n\nI am writing this documention in MacDown.\n\n\n\n\n\n\nAdd your current workflow here:\n\n\n\n\nThings to be careful of (common bugs)\n\n\n\n\nJulia passes vectors and matrices to routines by reference, rather than by copying them.  If you type \nx = y\n when x and y are arrays, then this will make x a pointer to y.  If you want x to be a copy of y, type \nx = copy(y)\n.  This can really mess up matlab programmers.  I wrote many functions that were modifying their arguments without realizing it.\n\n\nOn the other hand, if you type \nx = x + y\n, then x becomes a newly allocated vector and no longer refers to the original.  This is true even if you type \nx += y\n.  Here is an example that shows two of the possible behaviors, and the difference between what happens inside functions.\n\n\n\n\n\n\nAdds b in to a\n\nfunction addA2B(a,b)\n    for i in 1:length(a)\n        a[i] += b[i]\n    end\nend\n\n\nFails to add b in to a\n\nfunction addA2Bfail(a,b)\n    a += b\nend\n\na = [1 0]\nb = [2 2]\naddA2B(a,b)\na\n\n1x2 Array{Int64,2}:\n 3  2\n\na = [1 0]\nb = [2 2]\naddA2Bfail(a,b)\na\n\n1x2 Array{Int64,2}:\n 1  0\n\na += b\na\n\n1x2 Array{Int64,2}:\n 3  2\n\n\n\n\n\n\n\nIf you are used to programming in Matlab, you might be tempted to type a line like \nfor i in 1:10,\n.  \nDo not put extra commas in Julia!\n  It will cause bad things to happen.\n\n\nTo get a vector with entries 1 through n, type \ncollect(1:n)\n.  The object \n1:n\n is a range, rather than a vector.\n\n\nJulia sparse matrix entries dissapear if they are set to 0. In order to overcome this, use the \nsetValue\n function. \nsetValue(G, u, i, 0)\n will set \nweighti(G, u, i)\n to 0 while also leaving \n(u, nbri(G, u, i))\n in the matrix.  \nNote This problem may have been fixed with Julia version 0.5.\n\n\n\n\n\n\nUseful Julia functions\n\n\nI am going to make a short list of Julia functions/features that I find useful.  Please add those that you use often as well.\n\n\n\n\ndocstrings: in the above example, I used a docstring to document each function.  You can get these by typing \n?addA2B\n.  You can also  \nwrite longer docstrings and use markdown\n.  I suggest putting them in front of every function.\n\n\nmethods(foo)\n lists all methods with the name foo.\n\n\nfieldnames(footype)\n tells you all the fields of footype.  Note that this is 0.4.  In 0.3.11, you type \nnames(footype)\n\n\n\n\njulia\n a = sparse(rand(3,3));\njulia\n fieldnames(a)\n5-element Array{Symbol,1}:\n :m\n :n\n :colptr\n :rowval\n :nzval\n\n\n\n\n\n\nOptimizing code in Julia\n\n\nThe best way that I've found of figuring out what's slowing down my code has been to use \n@code_warntype\n.  \n\n\nNote that the first time you run a piece of code in Julia, it gets compiled.  So, you should run it on a small example before trying to time it.  Then, use \n@time\n to time your code.\n\n\nI recommend reading the Performance Tips in the Julia documentation, not that I've understood all of it yet.\n\n\n\n\nHow should notebooks play with Git?\n\n\nThe great thing about the notebooks is that they contain live code, so that you can play with them.  But, sometimes you get a version that serves as great documentation, and you don't want to klobber it my mistake later (or evern worse, have someone else klobber it).  Presumably if someone accidently commits a messed up version we can unwind that.  But, is there a good way to keep track of this?", 
            "title": "Using Julia"
        }, 
        {
            "location": "/Julia/index.html#using-julia", 
            "text": "These are some things you might want to know about using Julia if it is new to you.  There are now many other resources that can explain Julia to you.  But, we keep this section here for reference.", 
            "title": "Using Julia"
        }, 
        {
            "location": "/Julia/index.html#docstrings", 
            "text": "Julia 0.5 lets you take advantage of docstrings. For example,  ?ringGraph  produces  The simple ring on n vertices  When having a multiline comment, make sure that lines don't have starting and trailing spaces. This will mess up the indentation when calling '?func_name'.", 
            "title": "Docstrings"
        }, 
        {
            "location": "/Julia/index.html#julia-notebooks", 
            "text": "To get the Julia notebooks working, I presently type  jupyter notebook . I then select the kernel to be Julia-0.5.0. It seems important to run this command from a directory that contains all the directories that have notebooks that you will use.  In particular, I advise against \"uploading\" notebooks from other directories.  That has only given me trouble.  To turn a notebook into html, you type something like  jupyter nbconvert Laplacians.ipynb  or  jupyter nbconvert --to markdown --stdout Sampler.ipynb   SamplerNotebook.md", 
            "title": "Julia Notebooks"
        }, 
        {
            "location": "/Julia/index.html#workflows", 
            "text": "Julia has an IDE called Juno.  Both Dan and Serban have encountered some trouble with it: we have both found that it sometimes refuses to reload .jl code that we have written.  Please document workflows that you have found useful here:", 
            "title": "Workflows"
        }, 
        {
            "location": "/Julia/index.html#dans-current-workflow", 
            "text": "I use emacs (which has a mode for Julia) and the notebooks.  I develop Julia code in a \"temporary\" file with a name like develX.jl.  While I am developing, this code is not included by the module to which it will eventually belong.  After modifying code, I reload it with  include(\"develX.jl\") .  This works fine for reloading methods.  It is not a good way to reload modules or types.  So, I usually put the types either in a separate file, or in my julia notebook.  I am writing this documention in MacDown.", 
            "title": "Dan's current workflow:"
        }, 
        {
            "location": "/Julia/index.html#add-your-current-workflow-here", 
            "text": "", 
            "title": "Add your current workflow here:"
        }, 
        {
            "location": "/Julia/index.html#things-to-be-careful-of-common-bugs", 
            "text": "Julia passes vectors and matrices to routines by reference, rather than by copying them.  If you type  x = y  when x and y are arrays, then this will make x a pointer to y.  If you want x to be a copy of y, type  x = copy(y) .  This can really mess up matlab programmers.  I wrote many functions that were modifying their arguments without realizing it.  On the other hand, if you type  x = x + y , then x becomes a newly allocated vector and no longer refers to the original.  This is true even if you type  x += y .  Here is an example that shows two of the possible behaviors, and the difference between what happens inside functions.    Adds b in to a \nfunction addA2B(a,b)\n    for i in 1:length(a)\n        a[i] += b[i]\n    end\nend Fails to add b in to a \nfunction addA2Bfail(a,b)\n    a += b\nend\n\na = [1 0]\nb = [2 2]\naddA2B(a,b)\na\n\n1x2 Array{Int64,2}:\n 3  2\n\na = [1 0]\nb = [2 2]\naddA2Bfail(a,b)\na\n\n1x2 Array{Int64,2}:\n 1  0\n\na += b\na\n\n1x2 Array{Int64,2}:\n 3  2   If you are used to programming in Matlab, you might be tempted to type a line like  for i in 1:10, .   Do not put extra commas in Julia!   It will cause bad things to happen.  To get a vector with entries 1 through n, type  collect(1:n) .  The object  1:n  is a range, rather than a vector.  Julia sparse matrix entries dissapear if they are set to 0. In order to overcome this, use the  setValue  function.  setValue(G, u, i, 0)  will set  weighti(G, u, i)  to 0 while also leaving  (u, nbri(G, u, i))  in the matrix.   Note This problem may have been fixed with Julia version 0.5.", 
            "title": "Things to be careful of (common bugs)"
        }, 
        {
            "location": "/Julia/index.html#useful-julia-functions", 
            "text": "I am going to make a short list of Julia functions/features that I find useful.  Please add those that you use often as well.   docstrings: in the above example, I used a docstring to document each function.  You can get these by typing  ?addA2B .  You can also   write longer docstrings and use markdown .  I suggest putting them in front of every function.  methods(foo)  lists all methods with the name foo.  fieldnames(footype)  tells you all the fields of footype.  Note that this is 0.4.  In 0.3.11, you type  names(footype)   julia  a = sparse(rand(3,3));\njulia  fieldnames(a)\n5-element Array{Symbol,1}:\n :m\n :n\n :colptr\n :rowval\n :nzval", 
            "title": "Useful Julia functions"
        }, 
        {
            "location": "/Julia/index.html#optimizing-code-in-julia", 
            "text": "The best way that I've found of figuring out what's slowing down my code has been to use  @code_warntype .    Note that the first time you run a piece of code in Julia, it gets compiled.  So, you should run it on a small example before trying to time it.  Then, use  @time  to time your code.  I recommend reading the Performance Tips in the Julia documentation, not that I've understood all of it yet.", 
            "title": "Optimizing code in Julia"
        }, 
        {
            "location": "/Julia/index.html#how-should-notebooks-play-with-git", 
            "text": "The great thing about the notebooks is that they contain live code, so that you can play with them.  But, sometimes you get a version that serves as great documentation, and you don't want to klobber it my mistake later (or evern worse, have someone else klobber it).  Presumably if someone accidently commits a messed up version we can unwind that.  But, is there a good way to keep track of this?", 
            "title": "How should notebooks play with Git?"
        }, 
        {
            "location": "/Examples/index.html", 
            "text": "Examples\n\n\nThe following are links to html files of Julia notebooks. These notebooks are also in the notebook directory, and can be open there so that you can run the code live. You should be able to find them under ~/.julia/v0.4/Laplacians/notebooks.\n\n\n\n\nFirstNotebook\n\n\nSolvers\n\n\nSampler\n\n\nLocalClustering\n\n\nLocalClustering Statistics", 
            "title": "Examples"
        }, 
        {
            "location": "/Examples/index.html#examples", 
            "text": "The following are links to html files of Julia notebooks. These notebooks are also in the notebook directory, and can be open there so that you can run the code live. You should be able to find them under ~/.julia/v0.4/Laplacians/notebooks.   FirstNotebook  Solvers  Sampler  LocalClustering  LocalClustering Statistics", 
            "title": "Examples"
        }, 
        {
            "location": "/CSCgraph/index.html", 
            "text": "Using sparse matrices as graphs\n\n\nThe routines \ndeg\n, \nnbri\n and \nweighti\n will let you treat a sparse matrix like a graph.\n\n\ndeg(graph, u)\n is the degree of node u. \nnbri(graph, u, i)\n is the ith neighbor of node u. \nweighti(graph, u, i)\n is the weight of the edge to the ith neighbor of node u.\n\n\nNote that we start indexing from 1.\n\n\nFor example, to iterate over the neighbors of node v,   and play with the attached nodes, you could write code like:\n\n\n  for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end\n\n\n\n\nBut, this turns out to be much slower than working with the structure directly, like\n\n\n  for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end\n\n\n\n\n\n\n[ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.\n\n\n\n\n\n\nThe SparseMatrixCSC data structure\n\n\nYou can explore what is going on with the data structure by looking at some examples.  For example, here is a randomly weighted complete graph on 4 vertices, first displayed as a matrix:\n\n\ngr = round(10*uniformWeight(completeGraph(4)))\n\n4x4 sparse matrix with 12 Float64 entries:\n    [2, 1]  =  3.0\n    [3, 1]  =  3.0\n    [4, 1]  =  6.0\n    [1, 2]  =  3.0\n    [3, 2]  =  1.0\n    [4, 2]  =  2.0\n    [1, 3]  =  3.0\n    [2, 3]  =  1.0\n    [4, 3]  =  7.0\n    [1, 4]  =  6.0\n    [2, 4]  =  2.0\n    [3, 4]  =  7.0\n\nfull(gr)\n\n4x4 Array{Float64,2}:\n 0.0  3.0  3.0  6.0\n 3.0  0.0  1.0  2.0\n 3.0  1.0  0.0  7.0\n 6.0  2.0  7.0  0.0\n\n\n\n\nTo see the underlying data structure, use \nfieldnames\n.\n\n\nfieldnames(gr)\n\n5-element Array{Symbol,1}:\n :m     \n :n     \n :colptr\n :rowval\n :nzval \n\n\n\n\nm\n and \nn\n are the dimensions of the matrix. The entries of the matrix are stored in nzval. colptr[i] is the index in nzval of the first nonzero entry in column i.  rowval tells you which rows in each column are nonzero. The indices of the nonzero entries in column i are stored in  rowval[colptr[i]] through rowval[colptr[i+1]-1].\n\n\ngr.colptr \n\n5-element Array{Int64,1}:\n  1\n  4\n  7\n 10\n 13\n\n [gr.rowval gr.nzval]\n\n 12x2 Array{Float64,2}:\n 2.0  3.0\n 3.0  3.0\n 4.0  6.0\n 1.0  3.0\n 3.0  1.0\n 4.0  2.0\n 1.0  3.0\n 2.0  1.0\n 4.0  7.0\n 1.0  6.0\n 2.0  2.0\n 3.0  7.0", 
            "title": "Sparse matrices as graphs"
        }, 
        {
            "location": "/CSCgraph/index.html#using-sparse-matrices-as-graphs", 
            "text": "The routines  deg ,  nbri  and  weighti  will let you treat a sparse matrix like a graph.  deg(graph, u)  is the degree of node u.  nbri(graph, u, i)  is the ith neighbor of node u.  weighti(graph, u, i)  is the weight of the edge to the ith neighbor of node u.  Note that we start indexing from 1.  For example, to iterate over the neighbors of node v,   and play with the attached nodes, you could write code like:    for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end  But, this turns out to be much slower than working with the structure directly, like    for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end   [ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.", 
            "title": "Using sparse matrices as graphs"
        }, 
        {
            "location": "/CSCgraph/index.html#the-sparsematrixcsc-data-structure", 
            "text": "You can explore what is going on with the data structure by looking at some examples.  For example, here is a randomly weighted complete graph on 4 vertices, first displayed as a matrix:  gr = round(10*uniformWeight(completeGraph(4)))\n\n4x4 sparse matrix with 12 Float64 entries:\n    [2, 1]  =  3.0\n    [3, 1]  =  3.0\n    [4, 1]  =  6.0\n    [1, 2]  =  3.0\n    [3, 2]  =  1.0\n    [4, 2]  =  2.0\n    [1, 3]  =  3.0\n    [2, 3]  =  1.0\n    [4, 3]  =  7.0\n    [1, 4]  =  6.0\n    [2, 4]  =  2.0\n    [3, 4]  =  7.0\n\nfull(gr)\n\n4x4 Array{Float64,2}:\n 0.0  3.0  3.0  6.0\n 3.0  0.0  1.0  2.0\n 3.0  1.0  0.0  7.0\n 6.0  2.0  7.0  0.0  To see the underlying data structure, use  fieldnames .  fieldnames(gr)\n\n5-element Array{Symbol,1}:\n :m     \n :n     \n :colptr\n :rowval\n :nzval   m  and  n  are the dimensions of the matrix. The entries of the matrix are stored in nzval. colptr[i] is the index in nzval of the first nonzero entry in column i.  rowval tells you which rows in each column are nonzero. The indices of the nonzero entries in column i are stored in  rowval[colptr[i]] through rowval[colptr[i+1]-1].  gr.colptr \n\n5-element Array{Int64,1}:\n  1\n  4\n  7\n 10\n 13\n\n [gr.rowval gr.nzval]\n\n 12x2 Array{Float64,2}:\n 2.0  3.0\n 3.0  3.0\n 4.0  6.0\n 1.0  3.0\n 3.0  1.0\n 4.0  2.0\n 1.0  3.0\n 2.0  1.0\n 4.0  7.0\n 1.0  6.0\n 2.0  2.0\n 3.0  7.0", 
            "title": "The SparseMatrixCSC data structure"
        }, 
        {
            "location": "/usingSolvers/index.html", 
            "text": "Solving linear equations in Laplacians and SDD matrices\n\n\nThe Solver Interface\n\n\nCholesky Factorization\n\n\nCG and PCG\n\n\nLow-Stretch Spanning Trees\n\n\nAugmented Spanning Tree Preconditioners\n\n\nThe solvers of Koutis, Miller and Peng.\n\n\nSampling Solvers of Kyng and Sachdeva\n\n\nAlgebraic Multigrid\n\n\nSolvers from Matlab\n\n\nIncomplete Cholesky Factorizations\n\n\nKoutis's Combinatorial Multigrid (CMG)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving linear equations in Laplacians and SDD matrices\n\n\nThe main purpose of this package is to experiment with the implementation of algorithms for solving systems of linear equations in Laplacian and symmetric, diagonally dominant, M-matrices (SDDM).\n\n\nAt present, the fastest solver in this package for Laplacians is \napproxCholLap\n. For SDDM systems, one should use \napproxCholSddm\n.  Here is a quick demo.  Read more for other solvers and other options you can pass to the solvers.\n\n\njulia\n a = grid3(50); # an adjacency matrix\njulia\n la = lap(a); # it's Laplacian\njulia\n sol = approxCholLap(a); # a solver for la\njulia\n b = randn(size(la,1)); b = b - mean(b); # a right-hand-side\njulia\n x = sol(b); # the solution\njulia\n norm(la*x-b) / norm(b)\n5.911931368666469e-7\njulia\n x = sol(b, tol=1e-12); # a higher accuracy solution\njulia\n norm(la*x-b) / norm(b)\n7.555529748070115e-11\njulia\n x = sol(b, tol=1e-1, verbose=true); # faster, lower accuracy, with info\nPCG stopped after: 0.022 seconds and 3 iterations with relative error 0.07929402690389374.\n\njulia\n sddm = copy(la); # doing it with a SDDM matrix\njulia\n sddm[1,1] += 1;\njulia\n sol = approxCholSddm(sddm, verbose=true); # solver, with output\nUsing greedy degree ordering. Factorization time: 0.7143130302429199\nRatio of operator edges to original edges: 2.1120548223350255\nratio of max to min diagonal of laplacian : 6.0\nSolver build time: 0.747 seconds.\n\njulia\n x = sol(b, verbose=false); # a solve, supressing output\njulia\n norm(sddm*x - b) / norm(b)\n8.739618692868002e-7\n\n\n\n\nWe recall that a matrix $ L $ is a \nLaplacian\n matrix if:\n\n\n\n\nIt is symmetric,\n\n\nits off-diagonal entries are non-positive, and\n\n\nall of its row sums are 0.\n\n\n\n\nThese conditions imply that the diagonal entries are non-negative, and that the matrix is singular.  So, we only hope to solve equations of the form  $ Lx = b $ when $b$ is in the span of the matrix.  When the graph of the nonzero entries of the matrix is connected, this is precisely when the sum of the entries in $ b $ is zero.  Laplacian matrices are always positive semidefinite.\n\n\nA matrix $ M $ is a symmetric \nM-matrix\n if:\n\n\n\n\nIt is symmetric,\n\n\nits off-diagonal entries are non-positive, and\n\n\nit is positive definite.\n\n\n\n\nA matrix symmetric $ M $ is \ndiagonally dominant\n if each of its diagonals is at least the sum of the absolute values of the off-diagonal entries in its row.  A Laplacians matrix is diagonally dominant.  A diagonally dominant matrix is always positive semidefinite.\n\n\nA \nSDDM\n matrix (symmetric, diagonally-dominant M-matrix) is a matrix that is both diagonally dominant and an M-matrix.  You may think of a SDDM matrix as a Laplacian plus a non-negative, non-zero, diagonal matrix.  However, this is only guaranteed to produce a SDDM matrix when the graph underlying the Laplacian is connected.\n\n\nLaplacians.jl contains code for solving systems of linear equations in both Laplacian and SDDM matrices.  In fact, these problems are equivalent.  So, usually a solver for one type of system is implemented, and then wrapped to solve the other. The same ideas can be used to solve systems of equations in SDD matrices (the off-diagonals can be positive or negative), but a wrapper for these has not yet been written.\n\n\n\n\nThe Solver Interface\n\n\nAll of the SDDM solvers take the SDDM matrix as input.\n\n\nAll of the Laplacian solvers take the adjacency matrix of the underlying graph as input.\n\n\nTo solve a system of linear equations, one first passes the matrix defining the system to a linear equation solving algorithm.  This will return a function that solves systems of linear equations in that matrix.  For example,\n\n\njulia\n n = 1000;\njulia\n a = wtedChimera(n);  # produces a graph, as a sparse adjacency matrix\njulia\n b = randn(n); \njulia\n b = b - mean(b); # so there is a solution\njulia\n f = cholLap(a)\n(::#79) (generic function with 1 method)\njulia\n x = f(b);\njulia\n la = lap(a);  # construct the Laplacian of a\njulia\n norm(la*x-b)\n2.9565023548855584e-13\n\n\n\n\nAll of the solvers take the following keyword arguments. This means that they are optional, and will be set to their default values if not specified.\n\n\n\n\ntol\n : the relative accuracy required: $ | M x - b | / | b | $.\n\n\nmaxits\n : the maximum number of iterations, for iterative algorithms.\n\n\nmaxtime\n : quit if it takes more than this many seconds.  Not all routines obey this, but they try.\n\n\nverbose\n : set to \ntrue\n to display diagnostic information.\n\n\npcgIts\n : If the algorithm is iterative, this allows it to return the number of iterations it performed.  If \npcgIts\n is an array of positive length, then its first entry is set to the number of iterations.  Where \nverbose\n prints this information, \npcgIts\n allows it to be returned to other code.  To disable this set \npcgIts\n to a zero length array, like \nInt[]\n.\n\n\n\n\nMost of the solvers are iterative, exploiting the preconditioned conjugate gradient. These are the solvers for which \nmaxits\n, \nmaxtime\n and \npcgIts\n make the most sense.  Some solvers, like Cholesky factorization, just ignore these parameters.\n\n\nAll of these parameters may be set in the call that constructs \nf\n.  They may then be over-ridden by again setting them in the call to \nf\n. Let's see how this works when using the conjugate gradient.\n\n\njulia\n f = cgLapSolver(a, tol=1e-2, verbose=true)\n(::f) (generic function with 1 method)\njulia\n x = f(b);\nCG BLAS stopped after: 78 iterations with relative error 0.009590493139133275.\njulia\n norm(la*x-b)/norm(b)\n0.00959049313913375\n\njulia\n pcgIts = [0]\n1-element Array{Int64,1}:\n 0\njulia\n x = f(b,verbose=false, pcgIts=pcgIts);\njulia\n pcgIts\n1-element Array{Int64,1}:\n 78\n\njulia\n x = f(b,verbose=true, maxits=50);\nCG BLAS stopped after: 50 iterations with relative error 0.050483096216933886.\n\njulia\n x = f(b, tol=1e-4);\nCG BLAS stopped after: 131 iterations with relative error 8.886882933346416e-5.\njulia\n norm(la*x-b)/norm(b)\n8.886882933294668e-5\n\n\n\n\nFor some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.\n\n\nIn the following, we document many of the solvers that have been implemented in this package.\n\n\n\n\nCholesky Factorization\n\n\nCholesky factorization, the version of Gaussian Elimination for symmetric matrices, should be the first solver you try.  It will be very fast for matrices of dimension less than 1000, and for larger matrices coming from two-dimensional problems.\n\n\nYou can compute a cholesky factor directly with \ncholfact\n.  It does  more than just compute the factor, and it saves its result in a data structure that implements \n\\\n.  It uses SuiteSparse by Tim Davis.\n\n\nHere is an example of how you would use it to solve a general non-singular linear system.\n\n\na = grid2(5)\nla = lap(a)\nsddm = copy(la)\nsddm[1,1] = sddm[1,1] + 1\nF = cholfact(sddm)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(sddm*x-b)\n\n    1.0598778281116327e-14\n\n\n\n\nAs \ncholfact\n does not satisfy our interface, we wrap it in a routine \ncholSDDM\n that does.\n\n\nTo solve systems in Laplacian matrices, use \ncholLap\n.  Recall that this routine should be passed the adjacency matrix.\n\n\nf = cholLap(a)\nb = randn(n); \nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15\n\n\n\n\n\n\nCG and PCG\n\n\nWe have written implementations of Conjugate Gradient (CG) and Preconditioned Conjugate Gradient (PCG) that satisfy the interface. These routines use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  \n\n\nsrand(1)\nn = 50\nM = randn(n,n); M = M * M';\nb = randn(n)\nx = cg(M,b,maxits=100,verbose=true);\nCG BLAS stopped after: 66 iterations with relative error 2.0166243927814765e-7.\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(M,bbig,maxits=100,tol=1e-30)\nCG Slow stopped after: 50 iterations with relative error 2.18672511297479336887519117065525148757254642683072581090418060286711737398731e-38.\n\nnorm(M*xbig - bbig)\n1.605742093628722039938504001423963138146137896744531914963345296279741402982296e-37\n\n\n\n\nTo create a function \nf\n that uses cg to solve systems in M, use \ncgSolver\n.  For Laplacians, use \ncgLapSolver\n.\n\n\njulia\n n = 1000;\njulia\n a = wtedChimera(n,1);\njulia\n f = Laplacians.cgLapSolver(a,maxits=100);\n\njulia\n b = randn(n);\njulia\n b = b - mean(b);\njulia\n x = f(b,verbose=true);\nCG BLAS stopped after: 100 iterations with relative error 0.012102058751548373.\n\n\njulia\n la = lap(a);\njulia\n sddm = copy(la);\njulia\n sddm = sddm + spdiagm(rand(n)/100);\njulia\n g = cgSolver(sddm,verbose=true)\n(::f) (generic function with 1 method)\n\njulia\n x = g(b);\nCG BLAS stopped after: 253 iterations with relative error 7.860172210007891e-7.\n\n\n\n\nPCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.\n\n\nsrand(1)\na = mapweight(grid2(200),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\nprec(x) = x ./ d\n@time x = pcg(la,b,prec,maxtime=1,tol=1e-2,verbose=true);\n\nPCG BLAS stopped at maxtime.\nPCG BLAS stopped after: 530 iterations with relative error 0.07732478003311881.\n  1.007756 seconds (10.32 k allocations: 648.525 MB, 9.69% gc time)\n\n@time x = pcg(la,b,prec,maxtime=3,tol=1e-2,verbose=true);\nPCG BLAS stopped after: 1019 iterations with relative error 0.009984013184429813.\n  2.086828 seconds (19.57 k allocations: 1.216 GB, 9.92% gc time) \n\n\n\n\nWithout the preconditioner, CG takes much longer on this example.\n\n\n@time x = cg(la,b,tol=1e-2,maxtime=10,verbose=true);\n\nCG BLAS stopped at maxtime.\nCG BLAS stopped after: 8879 iterations with relative error 0.054355534834831624.\n 10.001998 seconds (97.91 k allocations: 2.649 GB, 4.48% gc time)\n\n\n\n\npcgSolver\n creates a function that uses the preconditioner to solve systems in the matrix.\n\n\nf = pcgSolver(la,prec)\n@time x = f(b,maxtime=3,tol=1e-2,verbose=true);\nPCG BLAS stopped after: 1019 iterations with relative error 0.009984013184429813.\n  1.892217 seconds (19.58 k allocations: 1.216 GB, 9.47% gc time)\n\n\n\n\npcgLapSolver\n uses the Laplacian of one matrix as a preconditioner for the first.  It solves systems of linear equations in the preconditioner by Cholesky factorization.  It performs the Cholesky factorization when \npcgLapSolver\n is called.  This is why we do the work of creating \nf\n only once.  Here is an example using a Low-Stretch Spanning Tree preconditioner.\n\n\n@time t = akpw(a)\n  0.210467 seconds (1.43 M allocations: 91.226 MB, 19.23% gc time)\n\n@time f = pcgLapSolver(a,t)\n  0.160210 seconds (288 allocations: 28.076 MB, 72.28% gc time)\n\n@time x = f(b,maxtime=3,tol=1e-2,verbose=true);\nPCG BLAS stopped after: 260 iterations with relative error 0.009864463201800925.\n  1.014897 seconds (28.02 k allocations: 1.008 GB, 9.81% gc time)\n\n\n\n\n\n\nLow-Stretch Spanning Trees\n\n\nIn order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code that is guaranteed to produce these.  Instead, we supply three heuristics: \nakpw\n which is inspired by the algorith of Alon, Karp, Peleg and West, and  randomized versions of Prim and Kruskal's algorithm. \nrandishKruskal\n samples the remaining edges with probability proportional to their weight.  \nrandishPrim\n samples edges on the boundary while using the same rule.  We recommend using \nakpw\n.\n\n\nSee \nLow Stretch Spanning Trees\n to learn more about these.\n\n\n\n\nAugmented Spanning Tree Preconditioners\n\n\nThese are obtained by constructing a spanning tree of a graph, and then adding back some more edges from the graph.  The tree should have low stretch.  The edges to add back are chosen at random with probabilities proportional to their stretches.\n\n\nThese are implemented in the routines\n\n\n\n\naugTreeSddm\n, for SDDM matrices\n\n\naugTreeLap\n\n\naugTreePrecon\n\n\naugmentTree\n\n\n\n\n\n\nThe solvers of Koutis, Miller and Peng.\n\n\nSolvers inspired by the algorithm from \"Approaching optimality for solving SDD systems\" by Koutis, Miller, and Peng, \nSIAM Journal on Computing\n, 2014.\n\n\n\n\nKMPLapSolver\n\n\nKMPSDDMSolver\n\n\n\n\n\n\nSampling Solvers of Kyng and Sachdeva\n\n\nThese are inspired by the paper \"Approximate Gaussian Elimination for Laplacians: Fast, Sparse, and Simple\" by Rasmus Kyng and Sushant Sachdeva, FOCS 2016. \n\n\nThese first two follow that paper reasonably closely.\n\n\n\n\nsamplingSDDMSolver\n\n\nsamplingLapSolver\n\n\n\n\nThe following is a modification of the algorithm that eliminates edges one at a time.  The code is by Daniel Spielman.  The algorithm has not yet been analyzed.  It is presently the fastest in this package.\n\n\n\n\napproxCholLap\n\n\n\n\n\n\nAlgebraic Multigrid\n\n\nThis is an interface to the algebraic multigrid solvers from the PyAMG package.\n\n\n\n\nAMGLapSolver\n\n\nAMGSolver\n, for SDDM systems.\n\n\n\n\n\n\nSolvers from Matlab\n\n\nThe \nMATLAB.jl\n package allows Julia to call routines from Matlab, provided you have Matlab installed.  It does this in a very efficient fashion: it starts up the Matlab process when you type \nusing MATLAB\n, and then communicates with it.  So, we have wrapped some solvers from Matlab so that they obey the same interface.\n\n\nThese are not part of the Laplacians module, but are included in the package under \nsrc/matlabSolvers.jl\n.  To include them, type\n\n\ninclude(string(Pkg.dir(\nLaplacians\n) , \n/src/matlabSolvers.jl\n))\n\n\n\n\nWe provide the docstrings for these here.\n\n\n\n\nIncomplete Cholesky Factorizations\n\n\nThese use the no-fill incomplete Cholesky factorizations implemented in Matlab.  They first order the vertices by the \nsymrcm\n ordering.\n\n\nThe solvers are:\n\n\n\n\nf = matlab_ichol_sddm(sddm; tol, maxtime, maxits, pctIts, verbose)\n\n\nf = matlab_ichol_lap(A; tol, maxtime, maxits, pctIts, verbose)\n\n\n\n\nA routine that just wraps the function that solves equations in the preconditioner is provided as well:\n\n\n\n\nf = matlab_ichol(sddm)\n\n\n\n\n\n\nKoutis's Combinatorial Multigrid (CMG)\n\n\nYou must have installed Yiannis Koutis's \nCombinatorial Multigrid Code\n, and it must be on Matlab's default path.  As this code returns a function rather than a preconditioner, it would be inefficient to make it use our PCG code and satisfy our interface.  So, it does not.\n\n\n\n\nx = matlabCmgSolver(mat, b; tol::Real=1e-6, maxits=10000)\n\n\n\n\nThe matrix \nmat\n can either be SDDM or a Laplacian.  This solves the system in \nb\n.\n\n\nIf you need to specify the solver separately from \nb\n, you can call\n\n\n\n\nx = matlabCmgSolver(mat; tol::Real=1e-6, maxits=10000)\n\n\n\n\nor, for the Laplacians of the adjacency matrix \nA\n,\n\n\n\n\nx = matlabCmgLap(A; tol::Real=1e-6, maxits=10000)\n\n\n\n\nHowever, this does not create the solver.  It merely returns a call to the previous routine.", 
            "title": "Solving Linear Equations"
        }, 
        {
            "location": "/usingSolvers/index.html#solving-linear-equations-in-laplacians-and-sdd-matrices", 
            "text": "The main purpose of this package is to experiment with the implementation of algorithms for solving systems of linear equations in Laplacian and symmetric, diagonally dominant, M-matrices (SDDM).  At present, the fastest solver in this package for Laplacians is  approxCholLap . For SDDM systems, one should use  approxCholSddm .  Here is a quick demo.  Read more for other solvers and other options you can pass to the solvers.  julia  a = grid3(50); # an adjacency matrix\njulia  la = lap(a); # it's Laplacian\njulia  sol = approxCholLap(a); # a solver for la\njulia  b = randn(size(la,1)); b = b - mean(b); # a right-hand-side\njulia  x = sol(b); # the solution\njulia  norm(la*x-b) / norm(b)\n5.911931368666469e-7\njulia  x = sol(b, tol=1e-12); # a higher accuracy solution\njulia  norm(la*x-b) / norm(b)\n7.555529748070115e-11\njulia  x = sol(b, tol=1e-1, verbose=true); # faster, lower accuracy, with info\nPCG stopped after: 0.022 seconds and 3 iterations with relative error 0.07929402690389374.\n\njulia  sddm = copy(la); # doing it with a SDDM matrix\njulia  sddm[1,1] += 1;\njulia  sol = approxCholSddm(sddm, verbose=true); # solver, with output\nUsing greedy degree ordering. Factorization time: 0.7143130302429199\nRatio of operator edges to original edges: 2.1120548223350255\nratio of max to min diagonal of laplacian : 6.0\nSolver build time: 0.747 seconds.\n\njulia  x = sol(b, verbose=false); # a solve, supressing output\njulia  norm(sddm*x - b) / norm(b)\n8.739618692868002e-7  We recall that a matrix $ L $ is a  Laplacian  matrix if:   It is symmetric,  its off-diagonal entries are non-positive, and  all of its row sums are 0.   These conditions imply that the diagonal entries are non-negative, and that the matrix is singular.  So, we only hope to solve equations of the form  $ Lx = b $ when $b$ is in the span of the matrix.  When the graph of the nonzero entries of the matrix is connected, this is precisely when the sum of the entries in $ b $ is zero.  Laplacian matrices are always positive semidefinite.  A matrix $ M $ is a symmetric  M-matrix  if:   It is symmetric,  its off-diagonal entries are non-positive, and  it is positive definite.   A matrix symmetric $ M $ is  diagonally dominant  if each of its diagonals is at least the sum of the absolute values of the off-diagonal entries in its row.  A Laplacians matrix is diagonally dominant.  A diagonally dominant matrix is always positive semidefinite.  A  SDDM  matrix (symmetric, diagonally-dominant M-matrix) is a matrix that is both diagonally dominant and an M-matrix.  You may think of a SDDM matrix as a Laplacian plus a non-negative, non-zero, diagonal matrix.  However, this is only guaranteed to produce a SDDM matrix when the graph underlying the Laplacian is connected.  Laplacians.jl contains code for solving systems of linear equations in both Laplacian and SDDM matrices.  In fact, these problems are equivalent.  So, usually a solver for one type of system is implemented, and then wrapped to solve the other. The same ideas can be used to solve systems of equations in SDD matrices (the off-diagonals can be positive or negative), but a wrapper for these has not yet been written.", 
            "title": "Solving linear equations in Laplacians and SDD matrices"
        }, 
        {
            "location": "/usingSolvers/index.html#the-solver-interface", 
            "text": "All of the SDDM solvers take the SDDM matrix as input.  All of the Laplacian solvers take the adjacency matrix of the underlying graph as input.  To solve a system of linear equations, one first passes the matrix defining the system to a linear equation solving algorithm.  This will return a function that solves systems of linear equations in that matrix.  For example,  julia  n = 1000;\njulia  a = wtedChimera(n);  # produces a graph, as a sparse adjacency matrix\njulia  b = randn(n); \njulia  b = b - mean(b); # so there is a solution\njulia  f = cholLap(a)\n(::#79) (generic function with 1 method)\njulia  x = f(b);\njulia  la = lap(a);  # construct the Laplacian of a\njulia  norm(la*x-b)\n2.9565023548855584e-13  All of the solvers take the following keyword arguments. This means that they are optional, and will be set to their default values if not specified.   tol  : the relative accuracy required: $ | M x - b | / | b | $.  maxits  : the maximum number of iterations, for iterative algorithms.  maxtime  : quit if it takes more than this many seconds.  Not all routines obey this, but they try.  verbose  : set to  true  to display diagnostic information.  pcgIts  : If the algorithm is iterative, this allows it to return the number of iterations it performed.  If  pcgIts  is an array of positive length, then its first entry is set to the number of iterations.  Where  verbose  prints this information,  pcgIts  allows it to be returned to other code.  To disable this set  pcgIts  to a zero length array, like  Int[] .   Most of the solvers are iterative, exploiting the preconditioned conjugate gradient. These are the solvers for which  maxits ,  maxtime  and  pcgIts  make the most sense.  Some solvers, like Cholesky factorization, just ignore these parameters.  All of these parameters may be set in the call that constructs  f .  They may then be over-ridden by again setting them in the call to  f . Let's see how this works when using the conjugate gradient.  julia  f = cgLapSolver(a, tol=1e-2, verbose=true)\n(::f) (generic function with 1 method)\njulia  x = f(b);\nCG BLAS stopped after: 78 iterations with relative error 0.009590493139133275.\njulia  norm(la*x-b)/norm(b)\n0.00959049313913375\n\njulia  pcgIts = [0]\n1-element Array{Int64,1}:\n 0\njulia  x = f(b,verbose=false, pcgIts=pcgIts);\njulia  pcgIts\n1-element Array{Int64,1}:\n 78\n\njulia  x = f(b,verbose=true, maxits=50);\nCG BLAS stopped after: 50 iterations with relative error 0.050483096216933886.\n\njulia  x = f(b, tol=1e-4);\nCG BLAS stopped after: 131 iterations with relative error 8.886882933346416e-5.\njulia  norm(la*x-b)/norm(b)\n8.886882933294668e-5  For some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.  In the following, we document many of the solvers that have been implemented in this package.", 
            "title": "The Solver Interface"
        }, 
        {
            "location": "/usingSolvers/index.html#cholesky-factorization", 
            "text": "Cholesky factorization, the version of Gaussian Elimination for symmetric matrices, should be the first solver you try.  It will be very fast for matrices of dimension less than 1000, and for larger matrices coming from two-dimensional problems.  You can compute a cholesky factor directly with  cholfact .  It does  more than just compute the factor, and it saves its result in a data structure that implements  \\ .  It uses SuiteSparse by Tim Davis.  Here is an example of how you would use it to solve a general non-singular linear system.  a = grid2(5)\nla = lap(a)\nsddm = copy(la)\nsddm[1,1] = sddm[1,1] + 1\nF = cholfact(sddm)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(sddm*x-b)\n\n    1.0598778281116327e-14  As  cholfact  does not satisfy our interface, we wrap it in a routine  cholSDDM  that does.  To solve systems in Laplacian matrices, use  cholLap .  Recall that this routine should be passed the adjacency matrix.  f = cholLap(a)\nb = randn(n); \nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15", 
            "title": "Cholesky Factorization"
        }, 
        {
            "location": "/usingSolvers/index.html#cg-and-pcg", 
            "text": "We have written implementations of Conjugate Gradient (CG) and Preconditioned Conjugate Gradient (PCG) that satisfy the interface. These routines use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.    srand(1)\nn = 50\nM = randn(n,n); M = M * M';\nb = randn(n)\nx = cg(M,b,maxits=100,verbose=true);\nCG BLAS stopped after: 66 iterations with relative error 2.0166243927814765e-7.\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(M,bbig,maxits=100,tol=1e-30)\nCG Slow stopped after: 50 iterations with relative error 2.18672511297479336887519117065525148757254642683072581090418060286711737398731e-38.\n\nnorm(M*xbig - bbig)\n1.605742093628722039938504001423963138146137896744531914963345296279741402982296e-37  To create a function  f  that uses cg to solve systems in M, use  cgSolver .  For Laplacians, use  cgLapSolver .  julia  n = 1000;\njulia  a = wtedChimera(n,1);\njulia  f = Laplacians.cgLapSolver(a,maxits=100);\n\njulia  b = randn(n);\njulia  b = b - mean(b);\njulia  x = f(b,verbose=true);\nCG BLAS stopped after: 100 iterations with relative error 0.012102058751548373.\n\n\njulia  la = lap(a);\njulia  sddm = copy(la);\njulia  sddm = sddm + spdiagm(rand(n)/100);\njulia  g = cgSolver(sddm,verbose=true)\n(::f) (generic function with 1 method)\n\njulia  x = g(b);\nCG BLAS stopped after: 253 iterations with relative error 7.860172210007891e-7.  PCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.  srand(1)\na = mapweight(grid2(200),x- 1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\nprec(x) = x ./ d\n@time x = pcg(la,b,prec,maxtime=1,tol=1e-2,verbose=true);\n\nPCG BLAS stopped at maxtime.\nPCG BLAS stopped after: 530 iterations with relative error 0.07732478003311881.\n  1.007756 seconds (10.32 k allocations: 648.525 MB, 9.69% gc time)\n\n@time x = pcg(la,b,prec,maxtime=3,tol=1e-2,verbose=true);\nPCG BLAS stopped after: 1019 iterations with relative error 0.009984013184429813.\n  2.086828 seconds (19.57 k allocations: 1.216 GB, 9.92% gc time)   Without the preconditioner, CG takes much longer on this example.  @time x = cg(la,b,tol=1e-2,maxtime=10,verbose=true);\n\nCG BLAS stopped at maxtime.\nCG BLAS stopped after: 8879 iterations with relative error 0.054355534834831624.\n 10.001998 seconds (97.91 k allocations: 2.649 GB, 4.48% gc time)  pcgSolver  creates a function that uses the preconditioner to solve systems in the matrix.  f = pcgSolver(la,prec)\n@time x = f(b,maxtime=3,tol=1e-2,verbose=true);\nPCG BLAS stopped after: 1019 iterations with relative error 0.009984013184429813.\n  1.892217 seconds (19.58 k allocations: 1.216 GB, 9.47% gc time)  pcgLapSolver  uses the Laplacian of one matrix as a preconditioner for the first.  It solves systems of linear equations in the preconditioner by Cholesky factorization.  It performs the Cholesky factorization when  pcgLapSolver  is called.  This is why we do the work of creating  f  only once.  Here is an example using a Low-Stretch Spanning Tree preconditioner.  @time t = akpw(a)\n  0.210467 seconds (1.43 M allocations: 91.226 MB, 19.23% gc time)\n\n@time f = pcgLapSolver(a,t)\n  0.160210 seconds (288 allocations: 28.076 MB, 72.28% gc time)\n\n@time x = f(b,maxtime=3,tol=1e-2,verbose=true);\nPCG BLAS stopped after: 260 iterations with relative error 0.009864463201800925.\n  1.014897 seconds (28.02 k allocations: 1.008 GB, 9.81% gc time)", 
            "title": "CG and PCG"
        }, 
        {
            "location": "/usingSolvers/index.html#low-stretch-spanning-trees", 
            "text": "In order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code that is guaranteed to produce these.  Instead, we supply three heuristics:  akpw  which is inspired by the algorith of Alon, Karp, Peleg and West, and  randomized versions of Prim and Kruskal's algorithm.  randishKruskal  samples the remaining edges with probability proportional to their weight.   randishPrim  samples edges on the boundary while using the same rule.  We recommend using  akpw .  See  Low Stretch Spanning Trees  to learn more about these.", 
            "title": "Low-Stretch Spanning Trees"
        }, 
        {
            "location": "/usingSolvers/index.html#augmented-spanning-tree-preconditioners", 
            "text": "These are obtained by constructing a spanning tree of a graph, and then adding back some more edges from the graph.  The tree should have low stretch.  The edges to add back are chosen at random with probabilities proportional to their stretches.  These are implemented in the routines   augTreeSddm , for SDDM matrices  augTreeLap  augTreePrecon  augmentTree", 
            "title": "Augmented Spanning Tree Preconditioners"
        }, 
        {
            "location": "/usingSolvers/index.html#the-solvers-of-koutis-miller-and-peng", 
            "text": "Solvers inspired by the algorithm from \"Approaching optimality for solving SDD systems\" by Koutis, Miller, and Peng,  SIAM Journal on Computing , 2014.   KMPLapSolver  KMPSDDMSolver", 
            "title": "The solvers of Koutis, Miller and Peng."
        }, 
        {
            "location": "/usingSolvers/index.html#sampling-solvers-of-kyng-and-sachdeva", 
            "text": "These are inspired by the paper \"Approximate Gaussian Elimination for Laplacians: Fast, Sparse, and Simple\" by Rasmus Kyng and Sushant Sachdeva, FOCS 2016.   These first two follow that paper reasonably closely.   samplingSDDMSolver  samplingLapSolver   The following is a modification of the algorithm that eliminates edges one at a time.  The code is by Daniel Spielman.  The algorithm has not yet been analyzed.  It is presently the fastest in this package.   approxCholLap", 
            "title": "Sampling Solvers of Kyng and Sachdeva"
        }, 
        {
            "location": "/usingSolvers/index.html#algebraic-multigrid", 
            "text": "This is an interface to the algebraic multigrid solvers from the PyAMG package.   AMGLapSolver  AMGSolver , for SDDM systems.", 
            "title": "Algebraic Multigrid"
        }, 
        {
            "location": "/usingSolvers/index.html#solvers-from-matlab", 
            "text": "The  MATLAB.jl  package allows Julia to call routines from Matlab, provided you have Matlab installed.  It does this in a very efficient fashion: it starts up the Matlab process when you type  using MATLAB , and then communicates with it.  So, we have wrapped some solvers from Matlab so that they obey the same interface.  These are not part of the Laplacians module, but are included in the package under  src/matlabSolvers.jl .  To include them, type  include(string(Pkg.dir( Laplacians ) ,  /src/matlabSolvers.jl ))  We provide the docstrings for these here.", 
            "title": "Solvers from Matlab"
        }, 
        {
            "location": "/usingSolvers/index.html#incomplete-cholesky-factorizations", 
            "text": "These use the no-fill incomplete Cholesky factorizations implemented in Matlab.  They first order the vertices by the  symrcm  ordering.  The solvers are:   f = matlab_ichol_sddm(sddm; tol, maxtime, maxits, pctIts, verbose)  f = matlab_ichol_lap(A; tol, maxtime, maxits, pctIts, verbose)   A routine that just wraps the function that solves equations in the preconditioner is provided as well:   f = matlab_ichol(sddm)", 
            "title": "Incomplete Cholesky Factorizations"
        }, 
        {
            "location": "/usingSolvers/index.html#koutiss-combinatorial-multigrid-cmg", 
            "text": "You must have installed Yiannis Koutis's  Combinatorial Multigrid Code , and it must be on Matlab's default path.  As this code returns a function rather than a preconditioner, it would be inefficient to make it use our PCG code and satisfy our interface.  So, it does not.   x = matlabCmgSolver(mat, b; tol::Real=1e-6, maxits=10000)   The matrix  mat  can either be SDDM or a Laplacian.  This solves the system in  b .  If you need to specify the solver separately from  b , you can call   x = matlabCmgSolver(mat; tol::Real=1e-6, maxits=10000)   or, for the Laplacians of the adjacency matrix  A ,   x = matlabCmgLap(A; tol::Real=1e-6, maxits=10000)   However, this does not create the solver.  It merely returns a call to the previous routine.", 
            "title": "Koutis's Combinatorial Multigrid (CMG)"
        }, 
        {
            "location": "/LSST/index.html", 
            "text": "Low Stretch Spanning Trees\n\n\nWe have implemented a variant of the algorithm of Alon, Karp, Peleg and West for computing low stretch spanning trees.  It is called \nakpw\n.  For unweighted graphs we provide a faster variant called \nakpwU\n.  If you require faster algorithms, with possibly higher average stretch, you can try the heuristics \nrandishPrim\n or \nrandishKruskal\n.\n\n\nYou can compute the stretch of a graph with respect to a spanning tree with the routine \ncompStretches\n.  It returns a sparse matrix with one entry for each edge in the graph giving its stretch.  For example:\n\n\njulia\n graph = grid2(4)\n16x16 sparse matrix with 48 Float64 entries:\n\njulia\n tree = akpwU(graph)\n16x16 sparse matrix with 30 Float64 entries:\n\njulia\n st = compStretches(tree,graph)\n16x16 sparse matrix with 48 Float64 entries:\n    [2 ,  1]  =  1.0\n    [5 ,  1]  =  1.0\n    [1 ,  2]  =  1.0\n    [3 ,  2]  =  1.0\n    [6 ,  2]  =  3.0\n    [2 ,  3]  =  1.0\n    [4 ,  3]  =  1.0\n    [7 ,  3]  =  1.0\n    [3 ,  4]  =  1.0\n    [8 ,  4]  =  3.0\n    [1 ,  5]  =  1.0\n    [6 ,  5]  =  5.0\n    \u22ee\n    [8 , 12]  =  3.0\n    [11, 12]  =  1.0\n    [16, 12]  =  1.0\n    [9 , 13]  =  1.0\n    [14, 13]  =  3.0\n    [10, 14]  =  1.0\n    [13, 14]  =  3.0\n    [15, 14]  =  3.0\n    [11, 15]  =  1.0\n    [14, 15]  =  3.0\n    [16, 15]  =  3.0\n    [12, 16]  =  1.0\n    [15, 16]  =  3.0\n\n\n\n\nHere is an example demonstrating the average stretches and times taken by these algorithms on a large graph.\n\n\n\njulia\n graph = chimera(1000000,1);\n\njulia\n @time tree = akpw(graph);\n  5.700285 seconds (16.10 M allocations: 1.263 GB, 11.16% gc time)\n\njulia\n aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n8.793236275779616\n\njulia\n @time tree = randishPrim(graph);\n  3.736225 seconds (3.21 M allocations: 566.887 MB, 6.40% gc time)\n\njulia\n aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n10.800094649795756\n\njulia\n @time tree = randishKruskal(graph);\n  2.515443 seconds (3.21 M allocations: 423.529 MB, 4.35% gc time)\n\njulia\n aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n37.819948689847564\n\n\n\n\n\nOf course, you can get very different results on very different graphs.  But, the ordering of these algorithms respect to time and stretch will usually follow this pattern.", 
            "title": "Low Stretch Spanning Trees"
        }, 
        {
            "location": "/LSST/index.html#low-stretch-spanning-trees", 
            "text": "We have implemented a variant of the algorithm of Alon, Karp, Peleg and West for computing low stretch spanning trees.  It is called  akpw .  For unweighted graphs we provide a faster variant called  akpwU .  If you require faster algorithms, with possibly higher average stretch, you can try the heuristics  randishPrim  or  randishKruskal .  You can compute the stretch of a graph with respect to a spanning tree with the routine  compStretches .  It returns a sparse matrix with one entry for each edge in the graph giving its stretch.  For example:  julia  graph = grid2(4)\n16x16 sparse matrix with 48 Float64 entries:\n\njulia  tree = akpwU(graph)\n16x16 sparse matrix with 30 Float64 entries:\n\njulia  st = compStretches(tree,graph)\n16x16 sparse matrix with 48 Float64 entries:\n    [2 ,  1]  =  1.0\n    [5 ,  1]  =  1.0\n    [1 ,  2]  =  1.0\n    [3 ,  2]  =  1.0\n    [6 ,  2]  =  3.0\n    [2 ,  3]  =  1.0\n    [4 ,  3]  =  1.0\n    [7 ,  3]  =  1.0\n    [3 ,  4]  =  1.0\n    [8 ,  4]  =  3.0\n    [1 ,  5]  =  1.0\n    [6 ,  5]  =  5.0\n    \u22ee\n    [8 , 12]  =  3.0\n    [11, 12]  =  1.0\n    [16, 12]  =  1.0\n    [9 , 13]  =  1.0\n    [14, 13]  =  3.0\n    [10, 14]  =  1.0\n    [13, 14]  =  3.0\n    [15, 14]  =  3.0\n    [11, 15]  =  1.0\n    [14, 15]  =  3.0\n    [16, 15]  =  3.0\n    [12, 16]  =  1.0\n    [15, 16]  =  3.0  Here is an example demonstrating the average stretches and times taken by these algorithms on a large graph.  \njulia  graph = chimera(1000000,1);\n\njulia  @time tree = akpw(graph);\n  5.700285 seconds (16.10 M allocations: 1.263 GB, 11.16% gc time)\n\njulia  aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n8.793236275779616\n\njulia  @time tree = randishPrim(graph);\n  3.736225 seconds (3.21 M allocations: 566.887 MB, 6.40% gc time)\n\njulia  aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n10.800094649795756\n\njulia  @time tree = randishKruskal(graph);\n  2.515443 seconds (3.21 M allocations: 423.529 MB, 4.35% gc time)\n\njulia  aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n37.819948689847564  Of course, you can get very different results on very different graphs.  But, the ordering of these algorithms respect to time and stretch will usually follow this pattern.", 
            "title": "Low Stretch Spanning Trees"
        }, 
        {
            "location": "/Developing/index.html", 
            "text": "Developing Laplacians.jl\n\n\n\n\nLearn to use git\n\n\n\n\nIf you don't know anything about git, then just know that you should make a branch for you own code.  Type\n\n\n\n\ngit checkout -b MyName\n\n\n\n\n\n\nMake sure that your .gitignore file contains the lines\n\n\n\n\n*~\n*#\n.ipynb_*\n.DS_Store\n*.cov\ndocs/build\ndocs/site\n\n\n\n\n\n\nNow, read about Git.  I recommend the book \nPro Git\n, which is available online for free.\n\n\nStop thinking about Git like subversion or dropbox.\n\n\nThe master branch will be the one for public consumption. It should (mostly) work.\n\n\nYou should also read the\n\n\n\n\nsection of the Julia docs about building packages.\n\n\n\n\nTests\n\n\nEvery piece of code should have a test in the \"tests\" directory.  Ideally, it should have enough tests to run every line of the code.  To run all the tests from the command prompt, type\n\n\njulia -e 'Pkg.test(\nLaplacians\n)'\n\n\n\n\n\n\nFast code?\n\n\nJust go for it. Don't worry about writing fast code at first. Just get it to work. We can speed it up later.\n\n\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"\n\n\n\n\nDocumentation\n\n\nThis documentation is still very rough. It is generated by a combination of Markdown and semi-automatic generation, using the \nDocumenter.jl\n package.  The one annoying feature of this package is that it will not allow the inclusion of a docstring on more than one page.  I don't know why.\n\n\nThe steps to generate and improve the documentation are:\n\n\n\n\nEdit Markdown files in the \ndocs\n directory.  For example, you could use MacDown to do this.\n\n\nIf you want to add a new page to the documention, create one.  Edit the file mkdocs.yml so show where it should appear.\n\n\nAdd docstrings to everything that needs it, and in particular to the routines you create.  The API is built from the docstrings.\n\n\nRun \njulia make.jl; mkdocs build\n in the \ndocs\n directory to generate the documentation from the Markdown.  This will generate a local copy of the documentation that you can use for reference.\n\n\n\n\nWARNING\n: You should not include any pages that are generated in the git repository.  So, make sure that your .gitignore file contains the line \ndocs/build\n and \ndocs/site\n.\n\n\n\n\n\n\nWhen you push to the master branch on GitHub, Travis will automatically build and update the docs.  \nDO NOT RUN \nmkdocs gh-deploy\n\n\n\n\n\n\nIf you create a Julia notebook that you would like to include as documentation.   You should  put it in the notebooks directory (.julia/v0.5/Laplacians/notebooks) and then link to it's page on GitHub.  While it seems that one should convert it to html (and one can), and then include it in MkDocs, MkDocs does something funny to the resulting html that does not look nice.\n\n\n\n\n\n\n\n\nParametric Types\n\n\nA sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\n\n\n\n\nTv\n, sometimes written \nTval\n denotes the types of the values, and \nTi\n or \nTind\n denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths, \nstart\n is of type \nTi\n.  Inside the code, whenever we write something like \npArray = zeros(Ti,n)\n, it creates an array of zeros of type Ti.  Using these parameteric types is \nmuch\n faster than leaving the types unfixed.\n\n\n\n\nData structures:\n\n\n\n\nIntHeap\n a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.\n\n\n\n\n\n\nInterface issue:\n\n\nThere are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.\n\n\n\n\nIntegration with other packages.\n\n\nThere are other graph packages that we might want to sometimes use.\n\n\n\n\nGraphs.jl\n : I found this one to be too slow and awkward to be useful.\n\n\nLightGraphs.jl\n : this looks more promising.  We will have to check it out.  It is reasonably fast, and the code looks pretty.", 
            "title": "Developing Laplacians"
        }, 
        {
            "location": "/Developing/index.html#developing-laplaciansjl", 
            "text": "", 
            "title": "Developing Laplacians.jl"
        }, 
        {
            "location": "/Developing/index.html#learn-to-use-git", 
            "text": "If you don't know anything about git, then just know that you should make a branch for you own code.  Type   git checkout -b MyName   Make sure that your .gitignore file contains the lines   *~\n*#\n.ipynb_*\n.DS_Store\n*.cov\ndocs/build\ndocs/site   Now, read about Git.  I recommend the book  Pro Git , which is available online for free.  Stop thinking about Git like subversion or dropbox.  The master branch will be the one for public consumption. It should (mostly) work.  You should also read the   section of the Julia docs about building packages.", 
            "title": "Learn to use git"
        }, 
        {
            "location": "/Developing/index.html#tests", 
            "text": "Every piece of code should have a test in the \"tests\" directory.  Ideally, it should have enough tests to run every line of the code.  To run all the tests from the command prompt, type  julia -e 'Pkg.test( Laplacians )'", 
            "title": "Tests"
        }, 
        {
            "location": "/Developing/index.html#fast-code", 
            "text": "Just go for it. Don't worry about writing fast code at first. Just get it to work. We can speed it up later.  Within some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"", 
            "title": "Fast code?"
        }, 
        {
            "location": "/Developing/index.html#documentation", 
            "text": "This documentation is still very rough. It is generated by a combination of Markdown and semi-automatic generation, using the  Documenter.jl  package.  The one annoying feature of this package is that it will not allow the inclusion of a docstring on more than one page.  I don't know why.  The steps to generate and improve the documentation are:   Edit Markdown files in the  docs  directory.  For example, you could use MacDown to do this.  If you want to add a new page to the documention, create one.  Edit the file mkdocs.yml so show where it should appear.  Add docstrings to everything that needs it, and in particular to the routines you create.  The API is built from the docstrings.  Run  julia make.jl; mkdocs build  in the  docs  directory to generate the documentation from the Markdown.  This will generate a local copy of the documentation that you can use for reference.   WARNING : You should not include any pages that are generated in the git repository.  So, make sure that your .gitignore file contains the line  docs/build  and  docs/site .    When you push to the master branch on GitHub, Travis will automatically build and update the docs.   DO NOT RUN  mkdocs gh-deploy    If you create a Julia notebook that you would like to include as documentation.   You should  put it in the notebooks directory (.julia/v0.5/Laplacians/notebooks) and then link to it's page on GitHub.  While it seems that one should convert it to html (and one can), and then include it in MkDocs, MkDocs does something funny to the resulting html that does not look nice.", 
            "title": "Documentation"
        }, 
        {
            "location": "/Developing/index.html#parametric-types", 
            "text": "A sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,  shortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)  Tv , sometimes written  Tval  denotes the types of the values, and  Ti  or  Tind  denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths,  start  is of type  Ti .  Inside the code, whenever we write something like  pArray = zeros(Ti,n) , it creates an array of zeros of type Ti.  Using these parameteric types is  much  faster than leaving the types unfixed.", 
            "title": "Parametric Types"
        }, 
        {
            "location": "/Developing/index.html#data-structures", 
            "text": "IntHeap  a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.", 
            "title": "Data structures:"
        }, 
        {
            "location": "/Developing/index.html#interface-issue", 
            "text": "There are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.", 
            "title": "Interface issue:"
        }, 
        {
            "location": "/Developing/index.html#integration-with-other-packages", 
            "text": "There are other graph packages that we might want to sometimes use.   Graphs.jl  : I found this one to be too slow and awkward to be useful.  LightGraphs.jl  : this looks more promising.  We will have to check it out.  It is reasonably fast, and the code looks pretty.", 
            "title": "Integration with other packages."
        }, 
        {
            "location": "/graphGenerators/index.html", 
            "text": "Generators\n\n\nLaplacians.jl\n implements generators for many standard graphs. The \nchimera\n and \nwtedChimera\n generators are designed to stress code by combining these standard graphs in tricky ways.  While no one of these graphs need be a hard case for any application, the goal is for these generators to explore the space of graphs in such a way that running on many of them should exercise your code.\n\n\nchimera(n)\n generates a random chimera graph. \nchimera(n,k)\n first sets the seed of the psrg to k. In this way, it generates the kth chimera graph, and messes with your psrg. \nwtedChimera\n is similar, but it generates weighted graphs.\n\n\n\n\nFunction list\n\n\n\n\nBase.Random.randperm\n\n\nLaplacians.ErdosRenyi\n\n\nLaplacians.ErdosRenyiCluster\n\n\nLaplacians.ErdosRenyiClusterFix\n\n\nLaplacians.chimera\n\n\nLaplacians.chimera\n\n\nLaplacians.completeBinaryTree\n\n\nLaplacians.completeGraph\n\n\nLaplacians.generalizedRing\n\n\nLaplacians.grid2\n\n\nLaplacians.grid2coords\n\n\nLaplacians.grid3\n\n\nLaplacians.grownGraph\n\n\nLaplacians.grownGraphD\n\n\nLaplacians.hyperCube\n\n\nLaplacians.pathGraph\n\n\nLaplacians.prefAttach\n\n\nLaplacians.pureRandomGraph\n\n\nLaplacians.randGenRing\n\n\nLaplacians.randMatching\n\n\nLaplacians.randRegular\n\n\nLaplacians.randWeight\n\n\nLaplacians.ringGraph\n\n\nLaplacians.semiWtedChimera\n\n\nLaplacians.wGrid2\n\n\nLaplacians.wGrid3\n\n\nLaplacians.wtedChimera\n\n\nLaplacians.wtedChimera\n\n\n\n\n#\n\n\nBase.Random.randperm\n \n \nMethod\n.\n\n\ngraph = randperm(mat::AbstractMatrix)\n        randperm(f::Expr)\n\n\n\n\nRandomly permutes the vertex indices\n\n\nsource\n\n\n#\n\n\nLaplacians.ErdosRenyi\n \n \nMethod\n.\n\n\ngraph = ErdosRenyi(n::Integer, m::Integer)\n\n\n\n\nGenerate a random graph on n vertices with m edges. The actual number of edges will probably be smaller, as we sample with replacement\n\n\nsource\n\n\n#\n\n\nLaplacians.ErdosRenyiCluster\n \n \nMethod\n.\n\n\ngraph = ErdosRenyiCluster(n::Integer, k::Integer)\n\n\n\n\nGenerate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.\n\n\nsource\n\n\n#\n\n\nLaplacians.ErdosRenyiClusterFix\n \n \nMethod\n.\n\n\ngraph = ErdosRenyiClusterFix(n::Integer, k::Integer)\n\n\n\n\nLike an Erdos-Renyi cluster, but add back a tree so it has n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.chimera\n \n \nMethod\n.\n\n\ngraph = chimera(n::Integer, k::Integer; verbose=false)\n\n\n\n\nBuilds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nsource\n\n\n#\n\n\nLaplacians.chimera\n \n \nMethod\n.\n\n\ngraph = chimera(n::Integer; verbose=false)\n\n\n\n\nBuilds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace\n\n\nsource\n\n\n#\n\n\nLaplacians.completeBinaryTree\n \n \nMethod\n.\n\n\ngraph = completeBinaryTree(n::Int64)\n\n\n\n\nThe complete binary tree on n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.completeGraph\n \n \nMethod\n.\n\n\ngraph = completeGraph(n::Int64)\n\n\n\n\nThe complete graph\n\n\nsource\n\n\n#\n\n\nLaplacians.generalizedRing\n \n \nMethod\n.\n\n\ngraph = generalizedRing(n::Int64, gens)\n\n\n\n\nA generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example,\n\n\ngeneralizedRing(17, [1 5])\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.grid2\n \n \nMethod\n.\n\n\ngraph = grid2(n::Int64, m::Int64; isotropy=1)\n\n\n\n\nAn n-by-m grid graph.  iostropy is the weighting on edges in one direction.\n\n\nsource\n\n\n#\n\n\nLaplacians.grid2coords\n \n \nMethod\n.\n\n\ngraph = grid2coords(n::Int64, m::Int64)\ngraph = grid2coords(n::Int64)\n\n\n\n\nCoordinates for plotting the vertices of the n-by-m grid graph\n\n\nsource\n\n\n#\n\n\nLaplacians.grid3\n \n \nMethod\n.\n\n\ngraph = grid3{Ti}(n1::Ti, n2::Ti, n3::Ti)\ngraph = grid3(n)\n\n\n\n\nAn n1-by-n2-by-n3 grid graph.\n\n\nsource\n\n\n#\n\n\nLaplacians.grownGraph\n \n \nMethod\n.\n\n\ngraph = grownGraph(n::Int64, k::Int64)\n\n\n\n\nCreate a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.\n\n\nsource\n\n\n#\n\n\nLaplacians.grownGraphD\n \n \nMethod\n.\n\n\ngraph = grownGraphD(n::Int64, k::Int64)\n\n\n\n\nLike a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.hyperCube\n \n \nMethod\n.\n\n\ngraph = hyperCube(d::Int64)\n\n\n\n\nThe d dimensional hypercube.  Has 2^d vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.pathGraph\n \n \nMethod\n.\n\n\ngraph = pathGraph(n::Int64)\n\n\n\n\nThe path graph on n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.prefAttach\n \n \nMethod\n.\n\n\ngraph = prefAttach(n::Int64, k::Int64, p::Float64)\n\n\n\n\nA preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.\n\n\nsource\n\n\n#\n\n\nLaplacians.pureRandomGraph\n \n \nMethod\n.\n\n\ngraph = pureRandomGraph(n::Integer; verbose=false)\n\n\n\n\nGenerate a random graph with n vertices from one of our natural distributions\n\n\nsource\n\n\n#\n\n\nLaplacians.randGenRing\n \n \nMethod\n.\n\n\ngraph = randGenRing(n::Int64, k::Integer)\n\n\n\n\nA random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution\n\n\nsource\n\n\n#\n\n\nLaplacians.randMatching\n \n \nMethod\n.\n\n\ngraph = randMatching(n::Int64)\n\n\n\n\nA random matching on n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.randRegular\n \n \nMethod\n.\n\n\ngraph = randRegular(n::Int64, k::Int64)\n\n\n\n\nA sum of k random matchings on n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.randWeight\n \n \nMethod\n.\n\n\ngraph = randWeight(graph)\n\n\n\n\nApplies one of a number of random weighting schemes to the edges of the graph\n\n\nsource\n\n\n#\n\n\nLaplacians.ringGraph\n \n \nMethod\n.\n\n\ngraph = ringGraph(n::Int64)\n\n\n\n\nThe simple ring on n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.semiWtedChimera\n \n \nMethod\n.\n\n\ngraph = semiWtedChimera(n::Integer; verbose=false)\n\n\n\n\nA Chimera graph with some weights.  The weights just appear when graphs are combined. For more interesting weights, use \nwtedChimera\n\n\nsource\n\n\n#\n\n\nLaplacians.wGrid2\n \n \nMethod\n.\n\n\ngraph = wGrid2(n::Int64; weightGen::Function=rand)\n\n\n\n\nAn n by n grid with random weights. User can specify the weighting scheme.\n\n\nsource\n\n\n#\n\n\nLaplacians.wGrid3\n \n \nMethod\n.\n\n\ngraph = wGrid3(n::Int64; weightGen::Function=rand)\n\n\n\n\nAn n^3 grid with random weights. User can specify the weighting scheme.\n\n\nsource\n\n\n#\n\n\nLaplacians.wtedChimera\n \n \nMethod\n.\n\n\ngraph = wtedChimera(n::Integer, k::Integer; verbose=false)\n\n\n\n\nBuilds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nsource\n\n\n#\n\n\nLaplacians.wtedChimera\n \n \nMethod\n.\n\n\ngraph = wtedChimera(n::Integer)\n\n\n\n\nGenerate a chimera, and then apply a random weighting scheme\n\n\nsource", 
            "title": "generators"
        }, 
        {
            "location": "/graphGenerators/index.html#generators", 
            "text": "Laplacians.jl  implements generators for many standard graphs. The  chimera  and  wtedChimera  generators are designed to stress code by combining these standard graphs in tricky ways.  While no one of these graphs need be a hard case for any application, the goal is for these generators to explore the space of graphs in such a way that running on many of them should exercise your code.  chimera(n)  generates a random chimera graph.  chimera(n,k)  first sets the seed of the psrg to k. In this way, it generates the kth chimera graph, and messes with your psrg.  wtedChimera  is similar, but it generates weighted graphs.", 
            "title": "Generators"
        }, 
        {
            "location": "/graphGenerators/index.html#function-list", 
            "text": "Base.Random.randperm  Laplacians.ErdosRenyi  Laplacians.ErdosRenyiCluster  Laplacians.ErdosRenyiClusterFix  Laplacians.chimera  Laplacians.chimera  Laplacians.completeBinaryTree  Laplacians.completeGraph  Laplacians.generalizedRing  Laplacians.grid2  Laplacians.grid2coords  Laplacians.grid3  Laplacians.grownGraph  Laplacians.grownGraphD  Laplacians.hyperCube  Laplacians.pathGraph  Laplacians.prefAttach  Laplacians.pureRandomGraph  Laplacians.randGenRing  Laplacians.randMatching  Laplacians.randRegular  Laplacians.randWeight  Laplacians.ringGraph  Laplacians.semiWtedChimera  Laplacians.wGrid2  Laplacians.wGrid3  Laplacians.wtedChimera  Laplacians.wtedChimera   #  Base.Random.randperm     Method .  graph = randperm(mat::AbstractMatrix)\n        randperm(f::Expr)  Randomly permutes the vertex indices  source  #  Laplacians.ErdosRenyi     Method .  graph = ErdosRenyi(n::Integer, m::Integer)  Generate a random graph on n vertices with m edges. The actual number of edges will probably be smaller, as we sample with replacement  source  #  Laplacians.ErdosRenyiCluster     Method .  graph = ErdosRenyiCluster(n::Integer, k::Integer)  Generate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.  source  #  Laplacians.ErdosRenyiClusterFix     Method .  graph = ErdosRenyiClusterFix(n::Integer, k::Integer)  Like an Erdos-Renyi cluster, but add back a tree so it has n vertices  source  #  Laplacians.chimera     Method .  graph = chimera(n::Integer, k::Integer; verbose=false)  Builds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  source  #  Laplacians.chimera     Method .  graph = chimera(n::Integer; verbose=false)  Builds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace  source  #  Laplacians.completeBinaryTree     Method .  graph = completeBinaryTree(n::Int64)  The complete binary tree on n vertices  source  #  Laplacians.completeGraph     Method .  graph = completeGraph(n::Int64)  The complete graph  source  #  Laplacians.generalizedRing     Method .  graph = generalizedRing(n::Int64, gens)  A generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example,  generalizedRing(17, [1 5])  source  #  Laplacians.grid2     Method .  graph = grid2(n::Int64, m::Int64; isotropy=1)  An n-by-m grid graph.  iostropy is the weighting on edges in one direction.  source  #  Laplacians.grid2coords     Method .  graph = grid2coords(n::Int64, m::Int64)\ngraph = grid2coords(n::Int64)  Coordinates for plotting the vertices of the n-by-m grid graph  source  #  Laplacians.grid3     Method .  graph = grid3{Ti}(n1::Ti, n2::Ti, n3::Ti)\ngraph = grid3(n)  An n1-by-n2-by-n3 grid graph.  source  #  Laplacians.grownGraph     Method .  graph = grownGraph(n::Int64, k::Int64)  Create a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.  source  #  Laplacians.grownGraphD     Method .  graph = grownGraphD(n::Int64, k::Int64)  Like a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices  source  #  Laplacians.hyperCube     Method .  graph = hyperCube(d::Int64)  The d dimensional hypercube.  Has 2^d vertices  source  #  Laplacians.pathGraph     Method .  graph = pathGraph(n::Int64)  The path graph on n vertices  source  #  Laplacians.prefAttach     Method .  graph = prefAttach(n::Int64, k::Int64, p::Float64)  A preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.  source  #  Laplacians.pureRandomGraph     Method .  graph = pureRandomGraph(n::Integer; verbose=false)  Generate a random graph with n vertices from one of our natural distributions  source  #  Laplacians.randGenRing     Method .  graph = randGenRing(n::Int64, k::Integer)  A random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution  source  #  Laplacians.randMatching     Method .  graph = randMatching(n::Int64)  A random matching on n vertices  source  #  Laplacians.randRegular     Method .  graph = randRegular(n::Int64, k::Int64)  A sum of k random matchings on n vertices  source  #  Laplacians.randWeight     Method .  graph = randWeight(graph)  Applies one of a number of random weighting schemes to the edges of the graph  source  #  Laplacians.ringGraph     Method .  graph = ringGraph(n::Int64)  The simple ring on n vertices  source  #  Laplacians.semiWtedChimera     Method .  graph = semiWtedChimera(n::Integer; verbose=false)  A Chimera graph with some weights.  The weights just appear when graphs are combined. For more interesting weights, use  wtedChimera  source  #  Laplacians.wGrid2     Method .  graph = wGrid2(n::Int64; weightGen::Function=rand)  An n by n grid with random weights. User can specify the weighting scheme.  source  #  Laplacians.wGrid3     Method .  graph = wGrid3(n::Int64; weightGen::Function=rand)  An n^3 grid with random weights. User can specify the weighting scheme.  source  #  Laplacians.wtedChimera     Method .  graph = wtedChimera(n::Integer, k::Integer; verbose=false)  Builds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  source  #  Laplacians.wtedChimera     Method .  graph = wtedChimera(n::Integer)  Generate a chimera, and then apply a random weighting scheme  source", 
            "title": "Function list"
        }, 
        {
            "location": "/operators/index.html", 
            "text": "Operators\n\n\nOperators transform graphs to produce new graphs.\n\n\n\n\nFunction list\n\n\n#\n\n\nLaplacians.adj\n \n \nMethod\n.\n\n\na,d = adj(sddm)\n\n\n\n\nCreate an adjacency matrix and a diagonal vector from an SDD M-matrix. That is, from a Laplacian with added diagonal weights\n\n\nsource\n\n\n#\n\n\nLaplacians.diagmat\n \n \nMethod\n.\n\n\nd = diagmat(a)\n\n\n\n\nReturns the diagonal weighted degree matrix(as a sparse matrix) of a graph\n\n\nsource\n\n\n#\n\n\nLaplacians.disjoin\n \n \nMethod\n.\n\n\ngraph = disjoin(a,b)\n\n\n\n\nCreate a disjoint union of graphs a and b,   with no edges between them.\n\n\nsource\n\n\n#\n\n\nLaplacians.edgeVertexMat\n \n \nMethod\n.\n\n\nU = edgeVertexMat(a)\n\n\n\n\nThe signed edge-vertex adjacency matrix\n\n\nsource\n\n\n#\n\n\nLaplacians.floatGraph\n \n \nMethod\n.\n\n\ngraph = floatGraph(a::SparseMatrixCSC)\n\n\nConvert the nonzero entries in a graph to Float64.\n\n\nsource\n\n\n#\n\n\nLaplacians.generalizedNecklace\n \n \nMethod\n.\n\n\ngraph = generalizedNecklace(A, H, k::Int64)\n\n\n\n\nConstructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.\n\n\nsource\n\n\n#\n\n\nLaplacians.joinGraphs\n \n \nMethod\n.\n\n\ngraph = joinGraphs(a, b, k::Integer)\n\n\n\n\nCreate a disjoint union of graphs a and b,  and then put k random edges between them\n\n\nsource\n\n\n#\n\n\nLaplacians.lap\n \n \nMethod\n.\n\n\nl = lap(a)\n\n\n\n\nCreate a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry\n\n\nsource\n\n\n#\n\n\nLaplacians.mapweight\n \n \nMethod\n.\n\n\nb = mapweight(a, x-\nrand(1)[1])\n\n\n\n\nCreate a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write.\n\n\nsource\n\n\n#\n\n\nLaplacians.plotGraph\n \n \nFunction\n.\n\n\nplotGraph(gr,x,y,color=[0,0,1];dots=true,setaxis=true,number=false)\n\n\n\n\nPlots graph gr with coordinates (x,y)\n\n\nsource\n\n\n#\n\n\nLaplacians.power\n \n \nMethod\n.\n\n\nap = power(a::SparseMatrixCSC, k::Int)\n\n\n\n\nReturns the kth power of a.\n\n\nsource\n\n\n#\n\n\nLaplacians.productGraph\n \n \nMethod\n.\n\n\naprod = productGraph(a0, a1)\n\n\n\n\nThe Cartesian product of two graphs.  When applied to two paths, it gives a grid.\n\n\nsource\n\n\n#\n\n\nLaplacians.shortIntGraph\n \n \nMethod\n.\n\n\ngraph = shortIntGraph(a::SparseMatrixCSC)\n\n\n\n\nConvert the indices in a graph to 32-bit ints. This takes less storage, but does not speed up much.\n\n\nsource\n\n\n#\n\n\nLaplacians.spectralCoords\n \n \nMethod\n.\n\n\nspectralCoords(a)\n\n\n\n\nComputes the spectral coordinates of a graph\n\n\nsource\n\n\n#\n\n\nLaplacians.spectralDrawing\n \n \nMethod\n.\n\n\nspectralDrawing(a)\n\n\n\n\nComputes spectral coordinates, and then uses plotGraph to draw\n\n\nsource\n\n\n#\n\n\nLaplacians.subsampleEdges\n \n \nMethod\n.\n\n\ngraph = subsampleEdges(a::SparseMatrixCSC, p::Float64)\n\n\n\n\nCreate a new graph from the old, but keeping edge edge with probability \np\n\n\nsource\n\n\n#\n\n\nLaplacians.thicken\n \n \nMethod\n.\n\n\na_new = thicken(A,k)\n\n\n\n\nCreate a new graph with at least k times as many edges as A By connecting nodes with common neighbors at random. When this stops working (not enough new edges), repeat on the most recently produced graph. If k is too big, it is decreased so the average degree will not be pushed much above n/2.\n\n\nWhen called without k, it just runs thicken_once.\n\n\nFor example:\n\n\na = grid2(5)\na2 = thicken(a,3)\n(x,y) = grid2coords(5,5);\nplotGraph(a2,x,y)\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.thicken_once\n \n \nMethod\n.\n\n\na_new = thicken_once(a)\n\n\n\n\nCreates one edge for every vertex in a of degree \n 1 by connecting two of its random neighbors. To use this to thicken a, return unweight(a + a_new).\n\n\na = grid2(5)\na2 = unweight(a + thicken_once(a))\n(x,y) = grid2coords(5,5);\nplotGraph(a2,x,y)\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.twoLift\n \n \nMethod\n.\n\n\ngraph = twoLift(a, flip::AbstractArray{Bool,1})\ngraph = twoLift(a)\ngraph = twoLift(a, k::Integer)\n\n\n\n\nCreats a 2-lift of a.  \nflip\n is a boolean indicating which edges cross. In the third version, k is the number of edges that cross.\n\n\nsource\n\n\n#\n\n\nLaplacians.uniformWeight!\n \n \nMethod\n.\n\n\nuniformWeight!(a)\n\n\n\n\nSet the weight of every edge to random uniform [0,1]\n\n\nsource\n\n\n#\n\n\nLaplacians.uniformWeight\n \n \nMethod\n.\n\n\nwted = uniformWeight(unwted)\n\n\n\n\nPut a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.\n\n\nsource\n\n\n#\n\n\nLaplacians.unweight!\n \n \nMethod\n.\n\n\nunweight!(a)\n\n\n\n\nChange the weight of every edge in a to 1\n\n\nsource\n\n\n#\n\n\nLaplacians.unweight\n \n \nMethod\n.\n\n\nwt1 = unweight(a)\n\n\n\n\nCreate a new graph in that is the same as the original, but with all edge weights 1\n\n\nsource\n\n\n#\n\n\nLaplacians.wtedEdgeVertexMat\n \n \nMethod\n.\n\n\nU = wtedEdgeVertexMat(a)\n\n\n\n\nThe signed and weighted edge-vertex adjacency matrix, so U'*U = L\n\n\nsource", 
            "title": "operators"
        }, 
        {
            "location": "/operators/index.html#operators", 
            "text": "Operators transform graphs to produce new graphs.", 
            "title": "Operators"
        }, 
        {
            "location": "/operators/index.html#function-list", 
            "text": "#  Laplacians.adj     Method .  a,d = adj(sddm)  Create an adjacency matrix and a diagonal vector from an SDD M-matrix. That is, from a Laplacian with added diagonal weights  source  #  Laplacians.diagmat     Method .  d = diagmat(a)  Returns the diagonal weighted degree matrix(as a sparse matrix) of a graph  source  #  Laplacians.disjoin     Method .  graph = disjoin(a,b)  Create a disjoint union of graphs a and b,   with no edges between them.  source  #  Laplacians.edgeVertexMat     Method .  U = edgeVertexMat(a)  The signed edge-vertex adjacency matrix  source  #  Laplacians.floatGraph     Method .  graph = floatGraph(a::SparseMatrixCSC)  Convert the nonzero entries in a graph to Float64.  source  #  Laplacians.generalizedNecklace     Method .  graph = generalizedNecklace(A, H, k::Int64)  Constructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.  source  #  Laplacians.joinGraphs     Method .  graph = joinGraphs(a, b, k::Integer)  Create a disjoint union of graphs a and b,  and then put k random edges between them  source  #  Laplacians.lap     Method .  l = lap(a)  Create a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry  source  #  Laplacians.mapweight     Method .  b = mapweight(a, x- rand(1)[1])  Create a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write.  source  #  Laplacians.plotGraph     Function .  plotGraph(gr,x,y,color=[0,0,1];dots=true,setaxis=true,number=false)  Plots graph gr with coordinates (x,y)  source  #  Laplacians.power     Method .  ap = power(a::SparseMatrixCSC, k::Int)  Returns the kth power of a.  source  #  Laplacians.productGraph     Method .  aprod = productGraph(a0, a1)  The Cartesian product of two graphs.  When applied to two paths, it gives a grid.  source  #  Laplacians.shortIntGraph     Method .  graph = shortIntGraph(a::SparseMatrixCSC)  Convert the indices in a graph to 32-bit ints. This takes less storage, but does not speed up much.  source  #  Laplacians.spectralCoords     Method .  spectralCoords(a)  Computes the spectral coordinates of a graph  source  #  Laplacians.spectralDrawing     Method .  spectralDrawing(a)  Computes spectral coordinates, and then uses plotGraph to draw  source  #  Laplacians.subsampleEdges     Method .  graph = subsampleEdges(a::SparseMatrixCSC, p::Float64)  Create a new graph from the old, but keeping edge edge with probability  p  source  #  Laplacians.thicken     Method .  a_new = thicken(A,k)  Create a new graph with at least k times as many edges as A By connecting nodes with common neighbors at random. When this stops working (not enough new edges), repeat on the most recently produced graph. If k is too big, it is decreased so the average degree will not be pushed much above n/2.  When called without k, it just runs thicken_once.  For example:  a = grid2(5)\na2 = thicken(a,3)\n(x,y) = grid2coords(5,5);\nplotGraph(a2,x,y)  source  #  Laplacians.thicken_once     Method .  a_new = thicken_once(a)  Creates one edge for every vertex in a of degree   1 by connecting two of its random neighbors. To use this to thicken a, return unweight(a + a_new).  a = grid2(5)\na2 = unweight(a + thicken_once(a))\n(x,y) = grid2coords(5,5);\nplotGraph(a2,x,y)  source  #  Laplacians.twoLift     Method .  graph = twoLift(a, flip::AbstractArray{Bool,1})\ngraph = twoLift(a)\ngraph = twoLift(a, k::Integer)  Creats a 2-lift of a.   flip  is a boolean indicating which edges cross. In the third version, k is the number of edges that cross.  source  #  Laplacians.uniformWeight!     Method .  uniformWeight!(a)  Set the weight of every edge to random uniform [0,1]  source  #  Laplacians.uniformWeight     Method .  wted = uniformWeight(unwted)  Put a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.  source  #  Laplacians.unweight!     Method .  unweight!(a)  Change the weight of every edge in a to 1  source  #  Laplacians.unweight     Method .  wt1 = unweight(a)  Create a new graph in that is the same as the original, but with all edge weights 1  source  #  Laplacians.wtedEdgeVertexMat     Method .  U = wtedEdgeVertexMat(a)  The signed and weighted edge-vertex adjacency matrix, so U'*U = L  source", 
            "title": "Function list"
        }, 
        {
            "location": "/graphUtils/index.html", 
            "text": "Graph Utilities\n\n\nThese are utilities to facilitate the use of sparse matrices as graphs.\n\n\n\n\nLaplacians.backIndices\n\n\nLaplacians.backIndices\n\n\nLaplacians.compConductance\n\n\nLaplacians.findEntries\n\n\nLaplacians.flipIndex\n\n\nLaplacians.getObound\n\n\nLaplacians.getVolume\n\n\nLaplacians.setValue\n\n\nLaplacians.wdeg\n\n\n\n\n#\n\n\nLaplacians.backIndices\n \n \nMethod\n.\n\n\nSame as the above, but now the graph is in adjacency list form \n\n\nsource\n\n\n#\n\n\nLaplacians.backIndices\n \n \nMethod\n.\n\n\nComputes the back indices in a graph in O(M+N). works if for every edge (u,v), (v,u) is also in the graph \n\n\nsource\n\n\n#\n\n\nLaplacians.compConductance\n \n \nMethod\n.\n\n\nReturns the quality of the cut for a given graph and a given cut set s.   the result will be |outgoing edges| / min(|vertices in set|, |N - vertices in set|)\n\n\nsource\n\n\n#\n\n\nLaplacians.findEntries\n \n \nMethod\n.\n\n\nSimilar to findnz, but also returns 0 entries that have an edge in the sparse matrix \n\n\nsource\n\n\n#\n\n\nLaplacians.flipIndex\n \n \nMethod\n.\n\n\nFor a symmetric matrix, this gives the correspondance between pairs of entries in an ijv. So, ai[ind] = aj[flip[ind]].  For example, \n\n\n(ai,aj,av) = findnz(a);\nfl = flipIndex(a)\nind = 10\n@show backind = fl[10]\n@show [ai[ind], aj[ind], av[ind]]\n@show [ai[backind], aj[backind], av[backind]];\n\nbackind = fl[10] = 4\n[ai[ind],aj[ind],av[ind]] = [2.0,4.0,0.7]\n[ai[backind],aj[backind],av[backind]] = [4.0,2.0,0.7]\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.getObound\n \n \nMethod\n.\n\n\nComputes the number of edges leaving s \n\n\nsource\n\n\n#\n\n\nLaplacians.getVolume\n \n \nMethod\n.\n\n\nComputes the volume of subset s in an unweighted graph G \n\n\nsource\n\n\n#\n\n\nLaplacians.setValue\n \n \nMethod\n.\n\n\nSets the value of a certain edge in a sparse graph; value can be 0 without the edges dissapearing \n\n\nsource\n\n\n#\n\n\nLaplacians.wdeg\n \n \nMethod\n.\n\n\nFinds the weighted degree of a vertex in the graph \n\n\nsource", 
            "title": "graphUtils"
        }, 
        {
            "location": "/graphUtils/index.html#graph-utilities", 
            "text": "These are utilities to facilitate the use of sparse matrices as graphs.   Laplacians.backIndices  Laplacians.backIndices  Laplacians.compConductance  Laplacians.findEntries  Laplacians.flipIndex  Laplacians.getObound  Laplacians.getVolume  Laplacians.setValue  Laplacians.wdeg   #  Laplacians.backIndices     Method .  Same as the above, but now the graph is in adjacency list form   source  #  Laplacians.backIndices     Method .  Computes the back indices in a graph in O(M+N). works if for every edge (u,v), (v,u) is also in the graph   source  #  Laplacians.compConductance     Method .  Returns the quality of the cut for a given graph and a given cut set s.   the result will be |outgoing edges| / min(|vertices in set|, |N - vertices in set|)  source  #  Laplacians.findEntries     Method .  Similar to findnz, but also returns 0 entries that have an edge in the sparse matrix   source  #  Laplacians.flipIndex     Method .  For a symmetric matrix, this gives the correspondance between pairs of entries in an ijv. So, ai[ind] = aj[flip[ind]].  For example,   (ai,aj,av) = findnz(a);\nfl = flipIndex(a)\nind = 10\n@show backind = fl[10]\n@show [ai[ind], aj[ind], av[ind]]\n@show [ai[backind], aj[backind], av[backind]];\n\nbackind = fl[10] = 4\n[ai[ind],aj[ind],av[ind]] = [2.0,4.0,0.7]\n[ai[backind],aj[backind],av[backind]] = [4.0,2.0,0.7]  source  #  Laplacians.getObound     Method .  Computes the number of edges leaving s   source  #  Laplacians.getVolume     Method .  Computes the volume of subset s in an unweighted graph G   source  #  Laplacians.setValue     Method .  Sets the value of a certain edge in a sparse graph; value can be 0 without the edges dissapearing   source  #  Laplacians.wdeg     Method .  Finds the weighted degree of a vertex in the graph   source", 
            "title": "Graph Utilities"
        }, 
        {
            "location": "/graphAlgs/index.html", 
            "text": "Graph Algorithms\n\n\nThese are basic graph algorithms.\n\n\n\n\nFunction list\n\n\n\n\nLaplacians.biggestComp\n\n\nLaplacians.components\n\n\nLaplacians.isConnected\n\n\nLaplacians.kruskal\n\n\nLaplacians.prim\n\n\nLaplacians.shortestPathTree\n\n\nLaplacians.shortestPaths\n\n\nLaplacians.vecToComps\n\n\n\n\n#\n\n\nLaplacians.biggestComp\n \n \nMethod\n.\n\n\nReturn the biggest component in a graph, as a graph\n\n\nsource\n\n\n#\n\n\nLaplacians.components\n \n \nMethod\n.\n\n\nComputes the connected components of a graph. Returns them as a vector of length equal to the number of vertices. The vector numbers the components from 1 through the maximum number. For example,\n\n\ngr = ErdosRenyi(10,11)\nc = components(gr)\n\n10-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 1\n 1\n 1\n 3\n 2\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.isConnected\n \n \nMethod\n.\n\n\nReturns true if graph is connected.  Calls components.\n\n\nsource\n\n\n#\n\n\nLaplacians.kruskal\n \n \nMethod\n.\n\n\n(kruskal::SparseMatrixCSC; kind=:max)\n Uses Kruskal's algorithm to compute a minimum (or maximum) spanning tree. Set kind=:min if you want the min spanning tree. It returns it a a graph\n\n\nsource\n\n\n#\n\n\nLaplacians.prim\n \n \nMethod\n.\n\n\nprim(mat::SparseMatrixCSC; kind=:max)\n Compute a maximum spanning tree of the matrix \nmat\n.   If \nkind=:min\n, computes a minimum spanning tree.\n\n\nsource\n\n\n#\n\n\nLaplacians.shortestPathTree\n \n \nMethod\n.\n\n\nComputes the shortest path tree, and returns it as a sparse matrix. Treats edge weights as reciprocals of lengths. For example:\n\n\na = [0 2 1; 2 0 3; 1 3 0]\ntr = full(shortestPathTree(sparse(a),1))\n\n3x3 Array{Float64,2}:\n 0.0  2.0  0.0\n 2.0  0.0  3.0\n 0.0  3.0  0.0\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.shortestPaths\n \n \nMethod\n.\n\n\nComputes the lenghts of shortest paths from \nstart\n. Returns both a vector of the lenghts, and the parent array in the shortest path tree.\n\n\nThis algorithm treats edge weights as reciprocals of distances. DOC BETTER\n\n\nsource\n\n\n#\n\n\nLaplacians.vecToComps\n \n \nMethod\n.\n\n\nThis turns a component vector, like that generated by components, into an array of arrays of indices of vertices in each component.  For example,\n\n\ncomps = vecToComps(c)\n\n3-element Array{Array{Int64,1},1}:\n [1,2,3,4,6,7,8]\n [5,10]\n [9]\n\n\n\n\nsource", 
            "title": "graphAlgs"
        }, 
        {
            "location": "/graphAlgs/index.html#graph-algorithms", 
            "text": "These are basic graph algorithms.", 
            "title": "Graph Algorithms"
        }, 
        {
            "location": "/graphAlgs/index.html#function-list", 
            "text": "Laplacians.biggestComp  Laplacians.components  Laplacians.isConnected  Laplacians.kruskal  Laplacians.prim  Laplacians.shortestPathTree  Laplacians.shortestPaths  Laplacians.vecToComps   #  Laplacians.biggestComp     Method .  Return the biggest component in a graph, as a graph  source  #  Laplacians.components     Method .  Computes the connected components of a graph. Returns them as a vector of length equal to the number of vertices. The vector numbers the components from 1 through the maximum number. For example,  gr = ErdosRenyi(10,11)\nc = components(gr)\n\n10-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 1\n 1\n 1\n 3\n 2  source  #  Laplacians.isConnected     Method .  Returns true if graph is connected.  Calls components.  source  #  Laplacians.kruskal     Method .  (kruskal::SparseMatrixCSC; kind=:max)  Uses Kruskal's algorithm to compute a minimum (or maximum) spanning tree. Set kind=:min if you want the min spanning tree. It returns it a a graph  source  #  Laplacians.prim     Method .  prim(mat::SparseMatrixCSC; kind=:max)  Compute a maximum spanning tree of the matrix  mat .   If  kind=:min , computes a minimum spanning tree.  source  #  Laplacians.shortestPathTree     Method .  Computes the shortest path tree, and returns it as a sparse matrix. Treats edge weights as reciprocals of lengths. For example:  a = [0 2 1; 2 0 3; 1 3 0]\ntr = full(shortestPathTree(sparse(a),1))\n\n3x3 Array{Float64,2}:\n 0.0  2.0  0.0\n 2.0  0.0  3.0\n 0.0  3.0  0.0  source  #  Laplacians.shortestPaths     Method .  Computes the lenghts of shortest paths from  start . Returns both a vector of the lenghts, and the parent array in the shortest path tree.  This algorithm treats edge weights as reciprocals of distances. DOC BETTER  source  #  Laplacians.vecToComps     Method .  This turns a component vector, like that generated by components, into an array of arrays of indices of vertices in each component.  For example,  comps = vecToComps(c)\n\n3-element Array{Array{Int64,1},1}:\n [1,2,3,4,6,7,8]\n [5,10]\n [9]  source", 
            "title": "Function list"
        }, 
        {
            "location": "/IO/index.html", 
            "text": "IO\n\n\n#\n\n\nLaplacians.readIJ\n \n \nFunction\n.\n\n\nTo read a simple edge list, each line being an (i, j) pair\n\n\nsource\n\n\n#\n\n\nLaplacians.readIJV\n \n \nMethod\n.\n\n\nTo read a simple edge list, each line being an (i, j, v) pair. The parens should not be there in the format, just commas separating. To generate this format in Matlab, you just need to be careful to write the vertex indices with sufficient precision.  For example, you can do this\n\n\n [ai,aj,av] = find(triu(a));\n\n dlmwrite('graph.txt',[ai,aj,av],'precision',9);\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.writeIJV\n \n \nMethod\n.\n\n\nWrites the upper portion of a matrix in ijv format, one row for each edge, separated by commas.  Only writes the upper triangular portion. The result can be read from Matlab like this:\n\n\n dl = dlmread('graph.txt');\n\n a = sparse(dl(:,1),dl(:,2),dl(:,3));\n\n n = max(size(a))\n\n a(n,n) = 0;\n\n a = a + a';\n\n\n\n\nsource", 
            "title": "IO"
        }, 
        {
            "location": "/IO/index.html#io", 
            "text": "#  Laplacians.readIJ     Function .  To read a simple edge list, each line being an (i, j) pair  source  #  Laplacians.readIJV     Method .  To read a simple edge list, each line being an (i, j, v) pair. The parens should not be there in the format, just commas separating. To generate this format in Matlab, you just need to be careful to write the vertex indices with sufficient precision.  For example, you can do this   [ai,aj,av] = find(triu(a));  dlmwrite('graph.txt',[ai,aj,av],'precision',9);  source  #  Laplacians.writeIJV     Method .  Writes the upper portion of a matrix in ijv format, one row for each edge, separated by commas.  Only writes the upper triangular portion. The result can be read from Matlab like this:   dl = dlmread('graph.txt');  a = sparse(dl(:,1),dl(:,2),dl(:,3));  n = max(size(a))  a(n,n) = 0;  a = a + a';  source", 
            "title": "IO"
        }, 
        {
            "location": "/solvers/index.html", 
            "text": "Linear Equation Solvers\n\n\nFor more, see the page \non using solvers\n.\n\n\n\n\nLaplacians.ApproxCholParams\n\n\nLaplacians.KMPParams\n\n\nLaplacians.samplingParams\n\n\nLaplacians.KMPLapSolver\n\n\nLaplacians.KMPSDDMSolver\n\n\nLaplacians.approxCholLap\n\n\nLaplacians.approxCholSddm\n\n\nLaplacians.augTreeLap\n\n\nLaplacians.augTreeLapPrecon\n\n\nLaplacians.augTreePrecon\n\n\nLaplacians.augTreeSddm\n\n\nLaplacians.augmentTree\n\n\nLaplacians.cg\n\n\nLaplacians.cgLapSolver\n\n\nLaplacians.cgSolver\n\n\nLaplacians.cholLap\n\n\nLaplacians.cholSDDM\n\n\nLaplacians.condNumber\n\n\nLaplacians.lapWrapSDDM\n\n\nLaplacians.pcg\n\n\nLaplacians.pcgLapSolver\n\n\nLaplacians.pcgSolver\n\n\nLaplacians.samplingLapSolver\n\n\nLaplacians.samplingSDDMSolver\n\n\n\n\n#\n\n\nLaplacians.cholLap\n \n \nFunction\n.\n\n\nsolver = cholLap(A::AbstractArray)\n\n\n\n\nUses Cholesky Factorization to solve systems in Laplacians.\n\n\nsource\n\n\n#\n\n\nLaplacians.cholSDDM\n \n \nFunction\n.\n\n\nsolveSDDM = cholSDDM(sddm::AbstractMatrix; tol, maxits, maxtime, verbose, pcgIts=Int[])\n\n\n\n\nThis functions wraps cholfact so that it satsfies our interface. It ignores all the keyword arguments.\n\n\nsource\n\n\n#\n\n\nLaplacians.lapWrapSDDM\n \n \nMethod\n.\n\n\nf = lapWrapSDDM(sddmSolver, A::AbstractArray; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params...)\nf = lapWrapSDDM(sddmSolver)\n\n\n\n\nUses a \nsddmSolver\n to solve systems of linear equations in Laplacian matrices.\n\n\nsource\n\n\n#\n\n\nLaplacians.cg\n \n \nFunction\n.\n\n\nx = cg(mat, b; tol, maxits, maxtime, verbose, pcgIts)\n\n\n\n\nsolves a symmetric linear system \nmat x = b\n.\n\n\nArguments\n\n\n\n\ntol\n is set to 1e-6 by default,\n\n\nmaxits\n defaults to Inf\n\n\nmaxtime\n defaults to Inf.  It measures seconds.\n\n\nverbose\n defaults to false\n\n\npcgIts\n is an array for returning the number of pcgIterations.  Default is length 0, in which case nothing is returned.\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.cgLapSolver\n \n \nMethod\n.\n\n\nx = cgLapSolver(A::AbstractMatrix; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[])\n\n\n\n\nCreate a solver that uses cg to solve Laplacian systems in the laplacian of A. This just exists to satisfy our interface. It does nothing more than create the Laplacian and call cg on each connected component.\n\n\nsource\n\n\n#\n\n\nLaplacians.cgSolver\n \n \nFunction\n.\n\n\nx = cgSolver(mat; tol, maxits, maxtime, verbose, pcgIts)\n\n\n\n\ncreates a solver for a PSD system \nmat\n. The parameters are as described in cg.\n\n\nsource\n\n\n#\n\n\nLaplacians.pcg\n \n \nFunction\n.\n\n\nx = pcg(mat, b, pre; tol, maxits, maxtime, verbose, pcgIts, stag_test)`\n\n\n\n\nsolves a symmetric linear system using preconditioner \npre\n.\n\n\nArguments\n\n\n\n\npre\n can be a function or a matrix.  If a matrix, a function to solve it is created with cholFact.\n\n\ntol\n is set to 1e-6 by default,\n\n\nmaxits\n defaults to Inf\n\n\nmaxtime\n defaults to Inf.  It measures seconds.\n\n\nverbose\n defaults to false\n\n\npcgIts\n is an array for returning the number of pcgIterations.  Default is length 0, in which case nothing is returned.\n\n\nstag_test=k\n stops the code if rho[it] \n (1-1/k) rho[it-k].  Set to 0 to deactivate.\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.pcgLapSolver\n \n \nMethod\n.\n\n\nx = pcgLapSolver(A, B; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[])\n\n\n\n\nCreate a solver that uses pcg to solve Laplacian systems in \nA\n Specialized for the case when the preconditioner the Laplacian matrix of \nB\n. It solves the preconditioner by Cholesky Factorization.\n\n\nsource\n\n\n#\n\n\nLaplacians.pcgSolver\n \n \nFunction\n.\n\n\nx = pcgSolver(mat, pre; tol, maxits, maxtime, verbose, pcgIts)\n\n\n\n\ncreates a solver for a PSD system using preconditioner \npre\n. The parameters are as described in pcg.\n\n\nsource\n\n\n#\n\n\nLaplacians.ApproxCholParams\n \n \nType\n.\n\n\nparams = ApproxCholParams(order, output)\n\n\n\n\norder can be one of\n\n\n\n\n:deg (by degree, adaptive),\n\n\n:wdeg (by original wted degree, nonadaptive),\n\n\n:given\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.approxCholLap\n \n \nMethod\n.\n\n\nsolver = approxCholLap(a); x = solver(b);\nsolver = approxCholLap(a; tol::Real=1e-6, maxits=1000, maxtime=Inf, verbose=false, pcgIts=Int[], params=ApproxCholParams())\n\n\n\n\nA heuristic by Daniel Spielman inspired by the linear system solver in https://arxiv.org/abs/1605.02353 by Rasmus Kyng and Sushant Sachdeva.  Whereas that paper eliminates vertices one at a time, this eliminates edges one at a time.  It is probably possible to analyze it. The \nApproxCholParams\n let you choose one of three orderings to perform the elimination.\n\n\n\n\nApproxCholParams(:given) - in the order given.   This is the fastest for construction the preconditioner, but the slowest solve.\n\n\nApproxCholParams(:deg) - always eliminate the node of lowest degree.   This is the slowest build, but the fastest solve.\n\n\nApproxCholParams(:wdeg) - go by a perturbed order of wted degree.\n\n\n\n\nFor more info, see http://danspielman.github.io/Laplacians.jl/latest/usingSolvers/index.html\n\n\nsource\n\n\n#\n\n\nLaplacians.approxCholSddm\n \n \nFunction\n.\n\n\nsolver = approxCholSddm(sddm); x = solver(b);\nsolver = approxCholSddm(sddm; tol=1e-6, maxits=1000, maxtime=Inf, verbose=false, pcgIts=Int[], params=ApproxCholParams())\n\n\n\n\nSolves sddm systems by wrapping approxCholLap. Not yet optimized directly for sddm.\n\n\nFor more info, see http://danspielman.github.io/Laplacians.jl/latest/usingSolvers/index.html \n\n\nsource\n\n\n#\n\n\nLaplacians.condNumber\n \n \nMethod\n.\n\n\ncn = condNumber(a, ldli; verbose=false)\n\n\n\n\nGiven an adjacency matrix a and an ldli computed by approxChol, this computes the condition number.\n\n\nsource\n\n\n#\n\n\nLaplacians.augTreeLap\n \n \nMethod\n.\n\n\nsolver = augTreeLap(A; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params=AugTreeParams())\n\n\n\n\nAn \"augmented spanning tree\" solver for Laplacians.  It works by adding edges to a low stretch spanning tree.  It calls \naugTreePrecon\n to form the preconditioner.  \nparams\n has entries\n\n\n\n\nparams.treeAlg\n default to \nakpw\n\n\nparams.opt\n if true, it interacts with cholmod to choose a good number of edges to add back.  If false, it adds back 2*sqrt(n).\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.augTreeLapPrecon\n \n \nMethod\n.\n\n\npre = augTreeLapPrecon{Tv,Ti}(A; params=AugTreeParams())\n\n\n\n\nThis is an augmented spanning tree preconditioner for Laplacians. It takes as optional input a tree growing algorithm. It adds back 2sqrt(n) edges via \naugmentTree\n: the sqrt(n) of highest stretch and another sqrt(n) sampled according to stretch. For most purposes, one should directly call \naugTreeLapSolver\n.\n\n\nsource\n\n\n#\n\n\nLaplacians.augTreePrecon\n \n \nMethod\n.\n\n\npre = augTreePrecon{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti}; params=AugTreeParams())\n\n\n\n\nThis is an augmented spanning tree preconditioner for diagonally dominant linear systems.  It takes as optional input a tree growing algorithm. It adds back 2sqrt(n) edges via augmentTree: the sqrt(n) of highest stretch and another sqrt(n) sampled according to stretch. For most purposes, one should directly call \naugTreeSolver\n.\n\n\nsource\n\n\n#\n\n\nLaplacians.augTreeSddm\n \n \nMethod\n.\n\n\nsolver = augTreeSddm(sddm; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[],  params=AugTreeParams())\n\n\n\n\nAn \"augmented spanning tree\" solver for positive definite diagonally dominant matrices.  It works by adding edges to a low stretch spanning tree.  It calls \naugTreePrecon\n to form the preconditioner.  \nparams\n has entries\n\n\n\n\nparams.treeAlg\n default to \nakpw\n\n\nparams.opt\n if true, it interacts with cholmod to choose a good number of edges to add back.  If false, it adds back 2*sqrt(n).\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.augmentTree\n \n \nMethod\n.\n\n\nB = augmentTree{Tv,Ti}(tree, A, k)\n\n\n\n\nTakes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It then adds back the k edges of highest stretch, and k edges sampled according to stretch.\n\n\nThis is the old alg.  We now recommend using augmentTreeOpt.\n\n\nsource\n\n\n#\n\n\nLaplacians.samplingParams\n \n \nType\n.\n\n\nParameters for the sampling solver.\n\n\nsource\n\n\n#\n\n\nLaplacians.samplingLapSolver\n \n \nMethod\n.\n\n\nsolver = samplingLapSolver(A)\n\n\n\n\nAn implementation of the linear system solver in https://arxiv.org/abs/1605.02353 by Rasmus Kyng and Sushant Sachdeva. In addition to the setup in the paper, we also use a low stretch tree to approximate effective resistances on edges. To perform well cache wise, we implement a cache friendly list of linked lists - found in revampedLinkedListFloatStorage.jl \n\n\nsource\n\n\n#\n\n\nLaplacians.samplingSDDMSolver\n \n \nMethod\n.\n\n\nsolver = samplingSDDMSolver(sddm)\n\n\n\n\nAn implementation of the linear system solver in https://arxiv.org/abs/1605.02353 by Rasmus Kyng and Sushant Sachdeva. In addition to the setup in the paper, we also use a low stretch tree to approximate effective resistances on edges. To perform well cache wise, we implement a cache friendly list of linked lists - found in revampedLinkedListFloatStorage.jl \n\n\nsource\n\n\n#\n\n\nLaplacians.KMPParams\n \n \nType\n.\n\n\nParameters for the KMP solver\n\n\nsource\n\n\n#\n\n\nLaplacians.KMPLapSolver\n \n \nMethod\n.\n\n\nlapSolver = KMPLapSolver(A; verbose, tol, maxits, maxtime, pcgIts, params::KMPParams)\n\n\n\n\nSolves linear equations in the Laplacian of graph with adjacency matrix \nA\n.\n\n\nBased on the paper \"Approaching optimality for solving SDD systems\" by Koutis, Miller, and Peng, \nSIAM Journal on Computing\n, 2014.\n\n\nsource\n\n\n#\n\n\nLaplacians.KMPSDDMSolver\n \n \nMethod\n.\n\n\nsddmSolver = KMPSDDMSolver(mat; verbose, tol, maxits, maxtime, pcgIts, params::KMPParams)\n\n\n\n\nSolves linear equations in symmetric, diagonally dominant matrices with non-positive off-diagonals.  Based on the paper \"Approaching optimality for solving SDD systems\" by Koutis, Miller, and Peng, \nSIAM Journal on Computing\n, 2014.\n\n\nsource", 
            "title": "solvers"
        }, 
        {
            "location": "/solvers/index.html#linear-equation-solvers", 
            "text": "For more, see the page  on using solvers .   Laplacians.ApproxCholParams  Laplacians.KMPParams  Laplacians.samplingParams  Laplacians.KMPLapSolver  Laplacians.KMPSDDMSolver  Laplacians.approxCholLap  Laplacians.approxCholSddm  Laplacians.augTreeLap  Laplacians.augTreeLapPrecon  Laplacians.augTreePrecon  Laplacians.augTreeSddm  Laplacians.augmentTree  Laplacians.cg  Laplacians.cgLapSolver  Laplacians.cgSolver  Laplacians.cholLap  Laplacians.cholSDDM  Laplacians.condNumber  Laplacians.lapWrapSDDM  Laplacians.pcg  Laplacians.pcgLapSolver  Laplacians.pcgSolver  Laplacians.samplingLapSolver  Laplacians.samplingSDDMSolver   #  Laplacians.cholLap     Function .  solver = cholLap(A::AbstractArray)  Uses Cholesky Factorization to solve systems in Laplacians.  source  #  Laplacians.cholSDDM     Function .  solveSDDM = cholSDDM(sddm::AbstractMatrix; tol, maxits, maxtime, verbose, pcgIts=Int[])  This functions wraps cholfact so that it satsfies our interface. It ignores all the keyword arguments.  source  #  Laplacians.lapWrapSDDM     Method .  f = lapWrapSDDM(sddmSolver, A::AbstractArray; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params...)\nf = lapWrapSDDM(sddmSolver)  Uses a  sddmSolver  to solve systems of linear equations in Laplacian matrices.  source  #  Laplacians.cg     Function .  x = cg(mat, b; tol, maxits, maxtime, verbose, pcgIts)  solves a symmetric linear system  mat x = b .  Arguments   tol  is set to 1e-6 by default,  maxits  defaults to Inf  maxtime  defaults to Inf.  It measures seconds.  verbose  defaults to false  pcgIts  is an array for returning the number of pcgIterations.  Default is length 0, in which case nothing is returned.   source  #  Laplacians.cgLapSolver     Method .  x = cgLapSolver(A::AbstractMatrix; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[])  Create a solver that uses cg to solve Laplacian systems in the laplacian of A. This just exists to satisfy our interface. It does nothing more than create the Laplacian and call cg on each connected component.  source  #  Laplacians.cgSolver     Function .  x = cgSolver(mat; tol, maxits, maxtime, verbose, pcgIts)  creates a solver for a PSD system  mat . The parameters are as described in cg.  source  #  Laplacians.pcg     Function .  x = pcg(mat, b, pre; tol, maxits, maxtime, verbose, pcgIts, stag_test)`  solves a symmetric linear system using preconditioner  pre .  Arguments   pre  can be a function or a matrix.  If a matrix, a function to solve it is created with cholFact.  tol  is set to 1e-6 by default,  maxits  defaults to Inf  maxtime  defaults to Inf.  It measures seconds.  verbose  defaults to false  pcgIts  is an array for returning the number of pcgIterations.  Default is length 0, in which case nothing is returned.  stag_test=k  stops the code if rho[it]   (1-1/k) rho[it-k].  Set to 0 to deactivate.   source  #  Laplacians.pcgLapSolver     Method .  x = pcgLapSolver(A, B; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[])  Create a solver that uses pcg to solve Laplacian systems in  A  Specialized for the case when the preconditioner the Laplacian matrix of  B . It solves the preconditioner by Cholesky Factorization.  source  #  Laplacians.pcgSolver     Function .  x = pcgSolver(mat, pre; tol, maxits, maxtime, verbose, pcgIts)  creates a solver for a PSD system using preconditioner  pre . The parameters are as described in pcg.  source  #  Laplacians.ApproxCholParams     Type .  params = ApproxCholParams(order, output)  order can be one of   :deg (by degree, adaptive),  :wdeg (by original wted degree, nonadaptive),  :given   source  #  Laplacians.approxCholLap     Method .  solver = approxCholLap(a); x = solver(b);\nsolver = approxCholLap(a; tol::Real=1e-6, maxits=1000, maxtime=Inf, verbose=false, pcgIts=Int[], params=ApproxCholParams())  A heuristic by Daniel Spielman inspired by the linear system solver in https://arxiv.org/abs/1605.02353 by Rasmus Kyng and Sushant Sachdeva.  Whereas that paper eliminates vertices one at a time, this eliminates edges one at a time.  It is probably possible to analyze it. The  ApproxCholParams  let you choose one of three orderings to perform the elimination.   ApproxCholParams(:given) - in the order given.   This is the fastest for construction the preconditioner, but the slowest solve.  ApproxCholParams(:deg) - always eliminate the node of lowest degree.   This is the slowest build, but the fastest solve.  ApproxCholParams(:wdeg) - go by a perturbed order of wted degree.   For more info, see http://danspielman.github.io/Laplacians.jl/latest/usingSolvers/index.html  source  #  Laplacians.approxCholSddm     Function .  solver = approxCholSddm(sddm); x = solver(b);\nsolver = approxCholSddm(sddm; tol=1e-6, maxits=1000, maxtime=Inf, verbose=false, pcgIts=Int[], params=ApproxCholParams())  Solves sddm systems by wrapping approxCholLap. Not yet optimized directly for sddm.  For more info, see http://danspielman.github.io/Laplacians.jl/latest/usingSolvers/index.html   source  #  Laplacians.condNumber     Method .  cn = condNumber(a, ldli; verbose=false)  Given an adjacency matrix a and an ldli computed by approxChol, this computes the condition number.  source  #  Laplacians.augTreeLap     Method .  solver = augTreeLap(A; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params=AugTreeParams())  An \"augmented spanning tree\" solver for Laplacians.  It works by adding edges to a low stretch spanning tree.  It calls  augTreePrecon  to form the preconditioner.   params  has entries   params.treeAlg  default to  akpw  params.opt  if true, it interacts with cholmod to choose a good number of edges to add back.  If false, it adds back 2*sqrt(n).   source  #  Laplacians.augTreeLapPrecon     Method .  pre = augTreeLapPrecon{Tv,Ti}(A; params=AugTreeParams())  This is an augmented spanning tree preconditioner for Laplacians. It takes as optional input a tree growing algorithm. It adds back 2sqrt(n) edges via  augmentTree : the sqrt(n) of highest stretch and another sqrt(n) sampled according to stretch. For most purposes, one should directly call  augTreeLapSolver .  source  #  Laplacians.augTreePrecon     Method .  pre = augTreePrecon{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti}; params=AugTreeParams())  This is an augmented spanning tree preconditioner for diagonally dominant linear systems.  It takes as optional input a tree growing algorithm. It adds back 2sqrt(n) edges via augmentTree: the sqrt(n) of highest stretch and another sqrt(n) sampled according to stretch. For most purposes, one should directly call  augTreeSolver .  source  #  Laplacians.augTreeSddm     Method .  solver = augTreeSddm(sddm; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[],  params=AugTreeParams())  An \"augmented spanning tree\" solver for positive definite diagonally dominant matrices.  It works by adding edges to a low stretch spanning tree.  It calls  augTreePrecon  to form the preconditioner.   params  has entries   params.treeAlg  default to  akpw  params.opt  if true, it interacts with cholmod to choose a good number of edges to add back.  If false, it adds back 2*sqrt(n).   source  #  Laplacians.augmentTree     Method .  B = augmentTree{Tv,Ti}(tree, A, k)  Takes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It then adds back the k edges of highest stretch, and k edges sampled according to stretch.  This is the old alg.  We now recommend using augmentTreeOpt.  source  #  Laplacians.samplingParams     Type .  Parameters for the sampling solver.  source  #  Laplacians.samplingLapSolver     Method .  solver = samplingLapSolver(A)  An implementation of the linear system solver in https://arxiv.org/abs/1605.02353 by Rasmus Kyng and Sushant Sachdeva. In addition to the setup in the paper, we also use a low stretch tree to approximate effective resistances on edges. To perform well cache wise, we implement a cache friendly list of linked lists - found in revampedLinkedListFloatStorage.jl   source  #  Laplacians.samplingSDDMSolver     Method .  solver = samplingSDDMSolver(sddm)  An implementation of the linear system solver in https://arxiv.org/abs/1605.02353 by Rasmus Kyng and Sushant Sachdeva. In addition to the setup in the paper, we also use a low stretch tree to approximate effective resistances on edges. To perform well cache wise, we implement a cache friendly list of linked lists - found in revampedLinkedListFloatStorage.jl   source  #  Laplacians.KMPParams     Type .  Parameters for the KMP solver  source  #  Laplacians.KMPLapSolver     Method .  lapSolver = KMPLapSolver(A; verbose, tol, maxits, maxtime, pcgIts, params::KMPParams)  Solves linear equations in the Laplacian of graph with adjacency matrix  A .  Based on the paper \"Approaching optimality for solving SDD systems\" by Koutis, Miller, and Peng,  SIAM Journal on Computing , 2014.  source  #  Laplacians.KMPSDDMSolver     Method .  sddmSolver = KMPSDDMSolver(mat; verbose, tol, maxits, maxtime, pcgIts, params::KMPParams)  Solves linear equations in symmetric, diagonally dominant matrices with non-positive off-diagonals.  Based on the paper \"Approaching optimality for solving SDD systems\" by Koutis, Miller, and Peng,  SIAM Journal on Computing , 2014.  source", 
            "title": "Linear Equation Solvers"
        }, 
        {
            "location": "/sparsification/index.html", 
            "text": "sparsification\n\n\n\n\nLaplacians.approxQual\n\n\nLaplacians.conditionNumber\n\n\nLaplacians.conditionNumber\n\n\nLaplacians.sparsify\n\n\nLaplacians.support\n\n\n\n\n#\n\n\nLaplacians.sparsify\n \n \nMethod\n.\n\n\nas = sparsify(a; ep=0.5)\n\n\n\n\nApply Spielman-Srivastava sparsification: sampling by effective resistances. \nep\n should be less than 1.\n\n\nsource\n\n\n#\n\n\nLaplacians.approxQual\n \n \nMethod\n.\n\n\neps = approxQual(graph1, graph2; tol=1e-5, verbose=false)\n\n\n\n\nComputes the eps for which graph1 and graph2 are eps approximations of each other. That is, L1 \n= (1+eps) L2, and vice versa.\n\n\nIt is randomized, so you might want to run it again if you don't trust the answers.\n\n\nsource\n\n\n#\n\n\nLaplacians.conditionNumber\n \n \nMethod\n.\n\n\nkappa = conditionNumber(graph, precon; tol=1e-5, verbose=false)\n\n\n\n\nComputes the relative condition number of graph and a preconditioning function.\n\n\nIt is randomized, so you might want to run it again if you don't trust the answers.\n\n\nsource\n\n\n#\n\n\nLaplacians.conditionNumber\n \n \nMethod\n.\n\n\nkapps = conditionNumber(graph1, graph2; tol=1e-5, verbose=false)\n\n\n\n\nComputes the relative condition number of graph1 and graph2.\n\n\nIt is randomized, so you might want to run it again if you don't trust the answers.\n\n\nsource\n\n\n#\n\n\nLaplacians.support\n \n \nMethod\n.\n\n\nsup12, sup21 = support(graph1, graph2; tol=1e-5)\n\n\n\n\nComputes the support of graph1 wrt graph2, and the other way around. It is randomized, so you might want to run it again if you don't trust the answers.\n\n\nsource", 
            "title": "sparsification"
        }, 
        {
            "location": "/sparsification/index.html#sparsification", 
            "text": "Laplacians.approxQual  Laplacians.conditionNumber  Laplacians.conditionNumber  Laplacians.sparsify  Laplacians.support   #  Laplacians.sparsify     Method .  as = sparsify(a; ep=0.5)  Apply Spielman-Srivastava sparsification: sampling by effective resistances.  ep  should be less than 1.  source  #  Laplacians.approxQual     Method .  eps = approxQual(graph1, graph2; tol=1e-5, verbose=false)  Computes the eps for which graph1 and graph2 are eps approximations of each other. That is, L1  = (1+eps) L2, and vice versa.  It is randomized, so you might want to run it again if you don't trust the answers.  source  #  Laplacians.conditionNumber     Method .  kappa = conditionNumber(graph, precon; tol=1e-5, verbose=false)  Computes the relative condition number of graph and a preconditioning function.  It is randomized, so you might want to run it again if you don't trust the answers.  source  #  Laplacians.conditionNumber     Method .  kapps = conditionNumber(graph1, graph2; tol=1e-5, verbose=false)  Computes the relative condition number of graph1 and graph2.  It is randomized, so you might want to run it again if you don't trust the answers.  source  #  Laplacians.support     Method .  sup12, sup21 = support(graph1, graph2; tol=1e-5)  Computes the support of graph1 wrt graph2, and the other way around. It is randomized, so you might want to run it again if you don't trust the answers.  source", 
            "title": "sparsification"
        }, 
        {
            "location": "/akpw/index.html", 
            "text": "AKPW\n\n\nAlso see the page on \nLow Stretch Spanning Trees\n\n\n\n\nLaplacians.akpw\n\n\nLaplacians.akpwU\n\n\n\n\n#\n\n\nLaplacians.akpw\n \n \nMethod\n.\n\n\ntree = akpw(graph; ver=0)\n\n\n\n\nComputes a low stretch spanning tree of \ngraph\n, and returns it as a graph. The default version is 0.  In event of emergency, one can try \nver=2\n.  It is usually slower, but might have slightly better stretch.\n\n\nsource\n\n\n#\n\n\nLaplacians.akpwU\n \n \nMethod\n.\n\n\ntree = akpwU(graph)\n\n\n\n\nComputes a low stretch spanning tree of an unweighted \ngraph\n, and returns it as a graph.\n\n\nsource", 
            "title": "akpw"
        }, 
        {
            "location": "/akpw/index.html#akpw", 
            "text": "Also see the page on  Low Stretch Spanning Trees   Laplacians.akpw  Laplacians.akpwU   #  Laplacians.akpw     Method .  tree = akpw(graph; ver=0)  Computes a low stretch spanning tree of  graph , and returns it as a graph. The default version is 0.  In event of emergency, one can try  ver=2 .  It is usually slower, but might have slightly better stretch.  source  #  Laplacians.akpwU     Method .  tree = akpwU(graph)  Computes a low stretch spanning tree of an unweighted  graph , and returns it as a graph.  source", 
            "title": "AKPW"
        }, 
        {
            "location": "/treeAlgs/index.html", 
            "text": "Tree Algorithms\n\n\n\n\nLaplacians.compStretches\n\n\n\n\n#\n\n\nLaplacians.compStretches\n \n \nMethod\n.\n\n\nCompute the stretched of every edge in \nmat\n with respect to the tree \ntree\n. Returns the answer as a sparse matrix with the same nonzero structure as \nmat\n. Assumes that \nmat\n is symmetric. \ntree\n should be the adjacency matrix of a spanning tree.\n\n\nsource", 
            "title": "treeAlgs"
        }, 
        {
            "location": "/treeAlgs/index.html#tree-algorithms", 
            "text": "Laplacians.compStretches   #  Laplacians.compStretches     Method .  Compute the stretched of every edge in  mat  with respect to the tree  tree . Returns the answer as a sparse matrix with the same nonzero structure as  mat . Assumes that  mat  is symmetric.  tree  should be the adjacency matrix of a spanning tree.  source", 
            "title": "Tree Algorithms"
        }, 
        {
            "location": "/randTrees/index.html", 
            "text": "randTrees\n\n\n\n\nLaplacians.randishKruskal\n\n\nLaplacians.randishPrim\n\n\n\n\n#\n\n\nLaplacians.randishKruskal\n \n \nMethod\n.\n\n\ntree = randishKruskal(A)\n\n\n\n\nA heuristic for computing low-stretch spanning trees.  Where Kruskal's MST algorithm adds edges in order of weight, this algorithm adds them at random with probability proportional to their weight.\n\n\nsource\n\n\n#\n\n\nLaplacians.randishPrim\n \n \nMethod\n.\n\n\ntree = randishPrim(A)\n\n\n\n\nA heuristic for computing low-stretch spanning trees.  Where Prim's MST algorithm grows a cluster by always adding the edge on the boundary of maximum weight, this algorithm adds a boundary edge with probability proportional to its weight.\n\n\nsource", 
            "title": "randTrees"
        }, 
        {
            "location": "/randTrees/index.html#randtrees", 
            "text": "Laplacians.randishKruskal  Laplacians.randishPrim   #  Laplacians.randishKruskal     Method .  tree = randishKruskal(A)  A heuristic for computing low-stretch spanning trees.  Where Kruskal's MST algorithm adds edges in order of weight, this algorithm adds them at random with probability proportional to their weight.  source  #  Laplacians.randishPrim     Method .  tree = randishPrim(A)  A heuristic for computing low-stretch spanning trees.  Where Prim's MST algorithm grows a cluster by always adding the edge on the boundary of maximum weight, this algorithm adds a boundary edge with probability proportional to its weight.  source", 
            "title": "randTrees"
        }, 
        {
            "location": "/localClustering/index.html", 
            "text": "Local Clustering\n\n\nThis is a collection of clustering related algorithms,   based on Approximate Personal PageRank, and improvement by local   flow computations.   It needs more documentation.\n\n\nFor now, see the \nLocal Clustering Notebook\n\n\n\n\nLaplacians.apr\n\n\nLaplacians.dumbRefineCut\n\n\nLaplacians.localImprove\n\n\nLaplacians.prn\n\n\nLaplacians.refineCut\n\n\n\n\n#\n\n\nLaplacians.dumbRefineCut\n \n \nMethod\n.\n\n\nModify a cluster by passing through all the vertices exactly once and \nadding/removing them based on the value of (Deg_external - Deg_Internal).\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.refineCut\n \n \nMethod\n.\n\n\nModify a cluster by adding or removing vertices by picking at each step \nthe vertex that has the maximum value of (Deg_external - Deg_Internal).\nEach vertex can be added in/removed only once.\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.apr\n \n \nMethod\n.\n\n\nComputes an approximate page rank vector from a starting set s, an alpha and an epsilon The algorithm follows the Anderson,Chung,Lang paper and Dan Spielman's lecture notes\n\n\nsource\n\n\n#\n\n\nLaplacians.localImprove\n \n \nMethod\n.\n\n\nlocalImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1}; epsSigma=-1.0, err=1e-10, maxSize = max(G.n, G.m)\n\n\nThe LocalImprove function, from the Orrechia-Zhu paper. Given a graph and an initial set, finds a set of smaller conductance based on the starting set using a localized version of max-flow.\n\n\nSmall discussion: When adding in the neighbors of the initial component, if the resulting  conductance is worse than the initial one,  the algorithm will add more and more vertices until hitting a better conductance. However, if we fix a certain  maximum size for our component,  it might be the case that this new conductance will always be worse than what we had initially. Thus, if we run the algorithm with a small maxSize,  our initial conductance might be the best solution we can raech.\n\n\n\n\nG is the given graph, A is the initial set\n\n\nepsSigma is a measure of the quality of the returning set (the smaller the better). It's defaulted to volume(A) / volume(VA)\n\n\nerr is the numerical error considered throughout the algorithm. It's defaulted to 1e-10\n\n\nmaxSize is the maximum allowed size for the flow graph at any iteration of the algorithm. It's defaulted to |V|\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.prn\n \n \nMethod\n.\n\n\nprn{Tv, Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)\n\n\nThe PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper\n\n\ns is a set of starting vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]\n\n\nphi is a bound on the quality of the conductance of the cut - the smaller the phi, the higher the quality.  b is used to handle precision throughout the algorithm - the higher the b, the greater the precision.\n\n\nsource", 
            "title": "localClustering"
        }, 
        {
            "location": "/localClustering/index.html#local-clustering", 
            "text": "This is a collection of clustering related algorithms,   based on Approximate Personal PageRank, and improvement by local   flow computations.   It needs more documentation.  For now, see the  Local Clustering Notebook   Laplacians.apr  Laplacians.dumbRefineCut  Laplacians.localImprove  Laplacians.prn  Laplacians.refineCut   #  Laplacians.dumbRefineCut     Method .  Modify a cluster by passing through all the vertices exactly once and \nadding/removing them based on the value of (Deg_external - Deg_Internal).  source  #  Laplacians.refineCut     Method .  Modify a cluster by adding or removing vertices by picking at each step \nthe vertex that has the maximum value of (Deg_external - Deg_Internal).\nEach vertex can be added in/removed only once.  source  #  Laplacians.apr     Method .  Computes an approximate page rank vector from a starting set s, an alpha and an epsilon The algorithm follows the Anderson,Chung,Lang paper and Dan Spielman's lecture notes  source  #  Laplacians.localImprove     Method .  localImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1}; epsSigma=-1.0, err=1e-10, maxSize = max(G.n, G.m)  The LocalImprove function, from the Orrechia-Zhu paper. Given a graph and an initial set, finds a set of smaller conductance based on the starting set using a localized version of max-flow.  Small discussion: When adding in the neighbors of the initial component, if the resulting  conductance is worse than the initial one,  the algorithm will add more and more vertices until hitting a better conductance. However, if we fix a certain  maximum size for our component,  it might be the case that this new conductance will always be worse than what we had initially. Thus, if we run the algorithm with a small maxSize,  our initial conductance might be the best solution we can raech.   G is the given graph, A is the initial set  epsSigma is a measure of the quality of the returning set (the smaller the better). It's defaulted to volume(A) / volume(VA)  err is the numerical error considered throughout the algorithm. It's defaulted to 1e-10  maxSize is the maximum allowed size for the flow graph at any iteration of the algorithm. It's defaulted to |V|   source  #  Laplacians.prn     Method .  prn{Tv, Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)  The PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper  s is a set of starting vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]  phi is a bound on the quality of the conductance of the cut - the smaller the phi, the higher the quality.  b is used to handle precision throughout the algorithm - the higher the b, the greater the precision.  source", 
            "title": "Local Clustering"
        }, 
        {
            "location": "/privateFuncs/index.html", 
            "text": "Unexported (Private) functions.\n\n\nThis is a list of all unexported functions and types from Laplacians.\n\n\n\n\nLaplacians.ApproxCholPQ\n\n\nLaplacians.LDLinv\n\n\nLaplacians.LLmatp\n\n\nLaplacians.LLp\n\n\nLaplacians.addToGPrime\n\n\nLaplacians.approxCholLapChol\n\n\nLaplacians.approxCholPQDec!\n\n\nLaplacians.approxCholPQInc!\n\n\nLaplacians.augmentTreeOpt\n\n\nLaplacians.blockSolver\n\n\nLaplacians.extendMatrix\n\n\nLaplacians.forceLap\n\n\nLaplacians.getCutSet\n\n\nLaplacians.initDictCol!\n\n\nLaplacians.initGPrime\n\n\nLaplacians.lapWrapComponents\n\n\nLaplacians.lapWrapConnected\n\n\nLaplacians.ldli2Chol\n\n\nLaplacians.localBlockFlow\n\n\nLaplacians.localFlow\n\n\nLaplacians.print_ll_col\n\n\nLaplacians.print_ll_col\n\n\nLaplacians.pushSpeedResult!\n\n\nLaplacians.sampleByWeight\n\n\nLaplacians.sddmWrapLap\n\n\nLaplacians.sortSet\n\n\nLaplacians.testZeroDiag\n\n\nLaplacians.treeDepthDFS\n\n\nLaplacians.wrapCapture\n\n\nLaplacians.wrapCaptureRhs\n\n\nLaplacians.wrapInterface\n\n\n\n\n#\n\n\nLaplacians.ApproxCholPQ\n \n \nType\n.\n\n\nAn approximate priority queue.   Items are bundled together into doubly-linked lists with all approximately the same key.   minlist is the min list we know to be non-empty.   It should always be a lower bound.   keyMap maps keys to lists\n\n\nsource\n\n\n#\n\n\nLaplacians.LDLinv\n \n \nType\n.\n\n\nLDLinv contains the information needed to solve the Laplacian systems.   It does it by applying Linv, then Dinv, then Linv (transpose).   But, it is specially constructed for this particular solver.   It does not explicitly make the matrix triangular.   Rather, col[i] is the name of the ith col to be eliminated\n\n\nsource\n\n\n#\n\n\nLaplacians.LLmatp\n \n \nType\n.\n\n\nLLmatp is the data structure used to maintain the matrix during elimination.   It stores the elements in each column in a singly linked list (only next ptrs)   Each element is an LLp (linked list pointer).   The head of each column is pointed to by cols.\n\n\nWe probably can get rid of degs - as it is only used to store initial degrees.\n\n\nsource\n\n\n#\n\n\nLaplacians.LLp\n \n \nType\n.\n\n\nLLp elements are all in the same column.   row tells us the row, and val is the entry.   val is set to zero for some edges that we should remove.   next gives the next in the column.  It points to itself to terminate.   reverse is the index into lles of the other copy of this edge,   since every edge is stored twice as we do not know the order of elimination in advance.\n\n\nsource\n\n\n#\n\n\nLaplacians.addToGPrime\n \n \nMethod\n.\n\n\nAdd a new vertex to GPrime \n\n\nsource\n\n\n#\n\n\nLaplacians.approxCholLapChol\n \n \nMethod\n.\n\n\nThis variation of approxChol creates a cholesky factor to do the elimination. It has not yet been optimized, and does not yet make the cholesky factor lower triangular\n\n\nsource\n\n\n#\n\n\nLaplacians.approxCholPQDec!\n \n \nMethod\n.\n\n\nDecrement the key of element i\nThis could crash if i exceeds the maxkey\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.approxCholPQInc!\n \n \nMethod\n.\n\n\nIncrement the key of element i\nThis could crash if i exceeds the maxkey\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.augmentTreeOpt\n \n \nMethod\n.\n\n\nB = augmentTreeOpt{Tv,Ti}(tree, A, params)\n\n\n\n\nTakes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It uses cholmod to decide how many edge to add back, shooting for nnzL_fac times n entries in the factored augmented tree, with a number of flops to factor equal to nnz(a)*flops_fac. The edges to add back are then choen at random.\n\n\nsource\n\n\n#\n\n\nLaplacians.blockSolver\n \n \nMethod\n.\n\n\nApply the ith solver on the ith component\n\n\nsource\n\n\n#\n\n\nLaplacians.extendMatrix\n \n \nMethod\n.\n\n\nAdd a new vertex to a with weights to the other vertices corresponding to diagonal surplus weight.\n\n\nThis is an efficient way of writing [a d; d' 0]\n\n\nsource\n\n\n#\n\n\nLaplacians.forceLap\n \n \nMethod\n.\n\n\nla = forceLap(a)\n\n\n\n\nCreate a Laplacian matrix from an adjacency matrix. If the input looks like a Laplacian, throw a warning and convert it.\n\n\nsource\n\n\n#\n\n\nLaplacians.getCutSet\n \n \nMethod\n.\n\n\nGet the min cut from the source - return all vertices in the cut besides the source \n\n\nsource\n\n\n#\n\n\nLaplacians.initDictCol!\n \n \nMethod\n.\n\n\ninitDictCol!(dic, name, typ)\n\n\n\n\nFor a dictionary in which each key indexes an array. If dic does not contain an entry of \nname\n, create with set to \nArray(typ,0)\n.\n\n\nsource\n\n\n#\n\n\nLaplacians.initGPrime\n \n \nMethod\n.\n\n\nInitialize GPrime with the set A and edges of type s-\nu\n\n\nsource\n\n\n#\n\n\nLaplacians.lapWrapComponents\n \n \nMethod\n.\n\n\nf = lapWrapComponents(solver, a::AbstractArray; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params...)\n\n\n\n\nApplies a Laplacian \nsolver\n that satisfies our interface to each connected component of the graph with adjacency matrix \na\n. Passes kwargs on the solver.\n\n\nsource\n\n\n#\n\n\nLaplacians.lapWrapConnected\n \n \nMethod\n.\n\n\nf = lapWrapConnected(sddmSolver, a::AbstractMatrix; kwargs...)\n\n\n\n\nApplies a \nsddmSolver\n to the Laplacian of the adjacency matrix \na\n of a connected graph. Passes on kwargs to the solver. \nsddmSolver\n should be a solver that obeys the interface.\n\n\nsource\n\n\n#\n\n\nLaplacians.ldli2Chol\n \n \nMethod\n.\n\n\nL = ldli2Chol(ldli)\n\n\n\n\nThis produces a matrix L so that L L^T approximate the original Laplacians. It is not quite a Cholesky factor, because it is off by a perm (and the all-1s vector orthogonality.\n\n\nsource\n\n\n#\n\n\nLaplacians.localBlockFlow\n \n \nMethod\n.\n\n\nCompute block flow between s and t\n\n\nsource\n\n\n#\n\n\nLaplacians.localFlow\n \n \nMethod\n.\n\n\nThe LocalFlow function, from the Orecchia-Zhu paper \n\n\nsource\n\n\n#\n\n\nLaplacians.print_ll_col\n \n \nMethod\n.\n\n\nPrint a column in an LLMatOrd matrix.   This is here for diagnostics.\n\n\nsource\n\n\n#\n\n\nLaplacians.print_ll_col\n \n \nMethod\n.\n\n\nPrint a column in an LLmatp matrix.   This is here for diagnostics.\n\n\nsource\n\n\n#\n\n\nLaplacians.pushSpeedResult!\n \n \nMethod\n.\n\n\nret\n is the answer returned by a speed test. This pushed it into the dictionary on which we are storing the tests.\n\n\nsource\n\n\n#\n\n\nLaplacians.sampleByWeight\n \n \nMethod\n.\n\n\nind = sampleByWeight(wt)\n\n\n\n\nsample an index with probability proportional to its weight given here\n\n\nsource\n\n\n#\n\n\nLaplacians.sddmWrapLap\n \n \nMethod\n.\n\n\nf = sddmWrapLap(lapSolver, sddm::AbstractArray; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params...)\n\n\n\n\nUses a \nlapSolver\n to solve systems of linear equations in sddm matrices.\n\n\nsource\n\n\n#\n\n\nLaplacians.sortSet\n \n \nMethod\n.\n\n\nGiven a set of integers, \nset\n between 1 and n, return a sorted version of them\n\n\nsource\n\n\n#\n\n\nLaplacians.testZeroDiag\n \n \nMethod\n.\n\n\ntestZeroDiag(a)\n\n\n\n\nReturns true if \na\n has zero diagonal, false otherwise\n\n\nsource\n\n\n#\n\n\nLaplacians.treeDepthDFS\n \n \nMethod\n.\n\n\nCompute the vector of depths in a tree that is in DFS order, \nwith the root at the first position, and the leaves at the end\n\n\nsource\n\n\n#\n\n\nLaplacians.wrapCapture\n \n \nMethod\n.\n\n\nf = wrapCapture(solver::Function, mats, rhss; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params...)\n\n\n\n\nThis wraps a solver so that we can capture all the matrices that it solves and all the right-hand-sides. Those are pushed into the arrays \nmats\n and \nrhss\n. For example\n\n\njulia\n mats = []\njulia\n rhss = []\njulia\n solver = wrapCapture(approxCholLap, mats, rhss)\njulia\n a = chimera(10)\njulia\n f = solver(a);\njulia\n size(mats[1])\n(10,10)\njulia\n b = randn(10)\njulia\n x = f(b);\njulia\n rhss\n1-element Array{Any,1}:\n [0.404962,-0.827718,0.704616,-0.403223,0.204891,-0.505589,0.907015,1.90266,-0.438115,0.0464351]\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.wrapCaptureRhs\n \n \nMethod\n.\n\n\nf = wrapCaptureRhs(sola::Function, rhss; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params...)\n\n\n\n\nCaptures all the right-hand-sides that are passed to the solver \nsola\n.  It pushes them into an array called rhhs. For example\n\n\njulia\n rhss = []\njulia\n a = wtedChimera(100)\njulia\n sola = approxCholLap(a)\njulia\n wrappedSolver = wrapCaptureRhs(sola,rhss)\njulia\n b = randn(100)\njulia\n x = wrappedSolver(b,verbose=true)\n\nPCG BLAS stopped after: 0.0 seconds and 11 iterations with relative error 3.160275810360986e-7.\n\njulia\n length(rhss[1])\n\n100\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.wrapInterface\n \n \nMethod\n.\n\n\nsolveA = wrapInterface(solver::Function, A::AbstractMatrix; tol, maxits, maxtime, verbose, pcgIts=Int[],params...)\nsolverConstructor = wrapInterface(A::AbstractMatrix; tol, maxits, maxtime, verbose, pcgIts=Int[],params...)\n\n\n\n\nReturns a function that discards \ntol\n, \nmaxits\n, \nmaxtime\n and \nverbose\n, sets \npcgIts\n to 0 (because it might not be using pcg), and passes whatever \nparams\n are left to the solver.\n\n\nExamples\n\n\njulia\n a = randn(5,5);\njulia\n a = a * a';\njulia\n solvea = wrapInterface(cholfact, a, maxits=100, verbose=true);\njulia\n b = randn(5,1);\njulia\n norm(a*solvea(b, verbose=false)-b)\n1.575705319704736e-14\n\njulia\n f = wrapInterface(cholfact)\njulia\n solvea = f(a, maxits=1000, maxtime = 1)\njulia\n norm(a*solvea(b, verbose=false, maxtime = 10)-b)\n1.575705319704736e-14\n\n\n\n\nsource", 
            "title": "Private Functions"
        }, 
        {
            "location": "/privateFuncs/index.html#unexported-private-functions", 
            "text": "This is a list of all unexported functions and types from Laplacians.   Laplacians.ApproxCholPQ  Laplacians.LDLinv  Laplacians.LLmatp  Laplacians.LLp  Laplacians.addToGPrime  Laplacians.approxCholLapChol  Laplacians.approxCholPQDec!  Laplacians.approxCholPQInc!  Laplacians.augmentTreeOpt  Laplacians.blockSolver  Laplacians.extendMatrix  Laplacians.forceLap  Laplacians.getCutSet  Laplacians.initDictCol!  Laplacians.initGPrime  Laplacians.lapWrapComponents  Laplacians.lapWrapConnected  Laplacians.ldli2Chol  Laplacians.localBlockFlow  Laplacians.localFlow  Laplacians.print_ll_col  Laplacians.print_ll_col  Laplacians.pushSpeedResult!  Laplacians.sampleByWeight  Laplacians.sddmWrapLap  Laplacians.sortSet  Laplacians.testZeroDiag  Laplacians.treeDepthDFS  Laplacians.wrapCapture  Laplacians.wrapCaptureRhs  Laplacians.wrapInterface   #  Laplacians.ApproxCholPQ     Type .  An approximate priority queue.   Items are bundled together into doubly-linked lists with all approximately the same key.   minlist is the min list we know to be non-empty.   It should always be a lower bound.   keyMap maps keys to lists  source  #  Laplacians.LDLinv     Type .  LDLinv contains the information needed to solve the Laplacian systems.   It does it by applying Linv, then Dinv, then Linv (transpose).   But, it is specially constructed for this particular solver.   It does not explicitly make the matrix triangular.   Rather, col[i] is the name of the ith col to be eliminated  source  #  Laplacians.LLmatp     Type .  LLmatp is the data structure used to maintain the matrix during elimination.   It stores the elements in each column in a singly linked list (only next ptrs)   Each element is an LLp (linked list pointer).   The head of each column is pointed to by cols.  We probably can get rid of degs - as it is only used to store initial degrees.  source  #  Laplacians.LLp     Type .  LLp elements are all in the same column.   row tells us the row, and val is the entry.   val is set to zero for some edges that we should remove.   next gives the next in the column.  It points to itself to terminate.   reverse is the index into lles of the other copy of this edge,   since every edge is stored twice as we do not know the order of elimination in advance.  source  #  Laplacians.addToGPrime     Method .  Add a new vertex to GPrime   source  #  Laplacians.approxCholLapChol     Method .  This variation of approxChol creates a cholesky factor to do the elimination. It has not yet been optimized, and does not yet make the cholesky factor lower triangular  source  #  Laplacians.approxCholPQDec!     Method .  Decrement the key of element i\nThis could crash if i exceeds the maxkey  source  #  Laplacians.approxCholPQInc!     Method .  Increment the key of element i\nThis could crash if i exceeds the maxkey  source  #  Laplacians.augmentTreeOpt     Method .  B = augmentTreeOpt{Tv,Ti}(tree, A, params)  Takes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It uses cholmod to decide how many edge to add back, shooting for nnzL_fac times n entries in the factored augmented tree, with a number of flops to factor equal to nnz(a)*flops_fac. The edges to add back are then choen at random.  source  #  Laplacians.blockSolver     Method .  Apply the ith solver on the ith component  source  #  Laplacians.extendMatrix     Method .  Add a new vertex to a with weights to the other vertices corresponding to diagonal surplus weight.  This is an efficient way of writing [a d; d' 0]  source  #  Laplacians.forceLap     Method .  la = forceLap(a)  Create a Laplacian matrix from an adjacency matrix. If the input looks like a Laplacian, throw a warning and convert it.  source  #  Laplacians.getCutSet     Method .  Get the min cut from the source - return all vertices in the cut besides the source   source  #  Laplacians.initDictCol!     Method .  initDictCol!(dic, name, typ)  For a dictionary in which each key indexes an array. If dic does not contain an entry of  name , create with set to  Array(typ,0) .  source  #  Laplacians.initGPrime     Method .  Initialize GPrime with the set A and edges of type s- u  source  #  Laplacians.lapWrapComponents     Method .  f = lapWrapComponents(solver, a::AbstractArray; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params...)  Applies a Laplacian  solver  that satisfies our interface to each connected component of the graph with adjacency matrix  a . Passes kwargs on the solver.  source  #  Laplacians.lapWrapConnected     Method .  f = lapWrapConnected(sddmSolver, a::AbstractMatrix; kwargs...)  Applies a  sddmSolver  to the Laplacian of the adjacency matrix  a  of a connected graph. Passes on kwargs to the solver.  sddmSolver  should be a solver that obeys the interface.  source  #  Laplacians.ldli2Chol     Method .  L = ldli2Chol(ldli)  This produces a matrix L so that L L^T approximate the original Laplacians. It is not quite a Cholesky factor, because it is off by a perm (and the all-1s vector orthogonality.  source  #  Laplacians.localBlockFlow     Method .  Compute block flow between s and t  source  #  Laplacians.localFlow     Method .  The LocalFlow function, from the Orecchia-Zhu paper   source  #  Laplacians.print_ll_col     Method .  Print a column in an LLMatOrd matrix.   This is here for diagnostics.  source  #  Laplacians.print_ll_col     Method .  Print a column in an LLmatp matrix.   This is here for diagnostics.  source  #  Laplacians.pushSpeedResult!     Method .  ret  is the answer returned by a speed test. This pushed it into the dictionary on which we are storing the tests.  source  #  Laplacians.sampleByWeight     Method .  ind = sampleByWeight(wt)  sample an index with probability proportional to its weight given here  source  #  Laplacians.sddmWrapLap     Method .  f = sddmWrapLap(lapSolver, sddm::AbstractArray; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params...)  Uses a  lapSolver  to solve systems of linear equations in sddm matrices.  source  #  Laplacians.sortSet     Method .  Given a set of integers,  set  between 1 and n, return a sorted version of them  source  #  Laplacians.testZeroDiag     Method .  testZeroDiag(a)  Returns true if  a  has zero diagonal, false otherwise  source  #  Laplacians.treeDepthDFS     Method .  Compute the vector of depths in a tree that is in DFS order,  with the root at the first position, and the leaves at the end  source  #  Laplacians.wrapCapture     Method .  f = wrapCapture(solver::Function, mats, rhss; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params...)  This wraps a solver so that we can capture all the matrices that it solves and all the right-hand-sides. Those are pushed into the arrays  mats  and  rhss . For example  julia  mats = []\njulia  rhss = []\njulia  solver = wrapCapture(approxCholLap, mats, rhss)\njulia  a = chimera(10)\njulia  f = solver(a);\njulia  size(mats[1])\n(10,10)\njulia  b = randn(10)\njulia  x = f(b);\njulia  rhss\n1-element Array{Any,1}:\n [0.404962,-0.827718,0.704616,-0.403223,0.204891,-0.505589,0.907015,1.90266,-0.438115,0.0464351]  source  #  Laplacians.wrapCaptureRhs     Method .  f = wrapCaptureRhs(sola::Function, rhss; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, pcgIts=Int[], params...)  Captures all the right-hand-sides that are passed to the solver  sola .  It pushes them into an array called rhhs. For example  julia  rhss = []\njulia  a = wtedChimera(100)\njulia  sola = approxCholLap(a)\njulia  wrappedSolver = wrapCaptureRhs(sola,rhss)\njulia  b = randn(100)\njulia  x = wrappedSolver(b,verbose=true)\n\nPCG BLAS stopped after: 0.0 seconds and 11 iterations with relative error 3.160275810360986e-7.\n\njulia  length(rhss[1])\n\n100  source  #  Laplacians.wrapInterface     Method .  solveA = wrapInterface(solver::Function, A::AbstractMatrix; tol, maxits, maxtime, verbose, pcgIts=Int[],params...)\nsolverConstructor = wrapInterface(A::AbstractMatrix; tol, maxits, maxtime, verbose, pcgIts=Int[],params...)  Returns a function that discards  tol ,  maxits ,  maxtime  and  verbose , sets  pcgIts  to 0 (because it might not be using pcg), and passes whatever  params  are left to the solver.  Examples  julia  a = randn(5,5);\njulia  a = a * a';\njulia  solvea = wrapInterface(cholfact, a, maxits=100, verbose=true);\njulia  b = randn(5,1);\njulia  norm(a*solvea(b, verbose=false)-b)\n1.575705319704736e-14\n\njulia  f = wrapInterface(cholfact)\njulia  solvea = f(a, maxits=1000, maxtime = 1)\njulia  norm(a*solvea(b, verbose=false, maxtime = 10)-b)\n1.575705319704736e-14  source", 
            "title": "Unexported (Private) functions."
        }, 
        {
            "location": "/indexOfAll/index.html", 
            "text": "Index of all exported\n\n\nThis is an index of all the exported methods. We would include the docstrings, but Documenter.jl does not let us.\n\n\n\n\nLaplacians.ApproxCholPQ\n\n\nLaplacians.ApproxCholParams\n\n\nLaplacians.KMPParams\n\n\nLaplacians.LDLinv\n\n\nLaplacians.LLmatp\n\n\nLaplacians.LLp\n\n\nLaplacians.samplingParams\n\n\nBase.Random.randperm\n\n\nLaplacians.ErdosRenyi\n\n\nLaplacians.ErdosRenyiCluster\n\n\nLaplacians.ErdosRenyiClusterFix\n\n\nLaplacians.KMPLapSolver\n\n\nLaplacians.KMPSDDMSolver\n\n\nLaplacians.addToGPrime\n\n\nLaplacians.adj\n\n\nLaplacians.akpw\n\n\nLaplacians.akpwU\n\n\nLaplacians.approxCholLap\n\n\nLaplacians.approxCholLapChol\n\n\nLaplacians.approxCholPQDec!\n\n\nLaplacians.approxCholPQInc!\n\n\nLaplacians.approxCholSddm\n\n\nLaplacians.approxQual\n\n\nLaplacians.apr\n\n\nLaplacians.augTreeLap\n\n\nLaplacians.augTreeLapPrecon\n\n\nLaplacians.augTreePrecon\n\n\nLaplacians.augTreeSddm\n\n\nLaplacians.augmentTree\n\n\nLaplacians.augmentTreeOpt\n\n\nLaplacians.backIndices\n\n\nLaplacians.backIndices\n\n\nLaplacians.biggestComp\n\n\nLaplacians.blockSolver\n\n\nLaplacians.cg\n\n\nLaplacians.cgLapSolver\n\n\nLaplacians.cgSolver\n\n\nLaplacians.chimera\n\n\nLaplacians.chimera\n\n\nLaplacians.cholLap\n\n\nLaplacians.cholSDDM\n\n\nLaplacians.compConductance\n\n\nLaplacians.compStretches\n\n\nLaplacians.completeBinaryTree\n\n\nLaplacians.completeGraph\n\n\nLaplacians.components\n\n\nLaplacians.condNumber\n\n\nLaplacians.conditionNumber\n\n\nLaplacians.conditionNumber\n\n\nLaplacians.diagmat\n\n\nLaplacians.disjoin\n\n\nLaplacians.dumbRefineCut\n\n\nLaplacians.edgeVertexMat\n\n\nLaplacians.extendMatrix\n\n\nLaplacians.findEntries\n\n\nLaplacians.flipIndex\n\n\nLaplacians.floatGraph\n\n\nLaplacians.forceLap\n\n\nLaplacians.generalizedNecklace\n\n\nLaplacians.generalizedRing\n\n\nLaplacians.getCutSet\n\n\nLaplacians.getObound\n\n\nLaplacians.getVolume\n\n\nLaplacians.grid2\n\n\nLaplacians.grid2coords\n\n\nLaplacians.grid3\n\n\nLaplacians.grownGraph\n\n\nLaplacians.grownGraphD\n\n\nLaplacians.hyperCube\n\n\nLaplacians.initDictCol!\n\n\nLaplacians.initGPrime\n\n\nLaplacians.isConnected\n\n\nLaplacians.joinGraphs\n\n\nLaplacians.kruskal\n\n\nLaplacians.lap\n\n\nLaplacians.lapWrapComponents\n\n\nLaplacians.lapWrapConnected\n\n\nLaplacians.lapWrapSDDM\n\n\nLaplacians.ldli2Chol\n\n\nLaplacians.localBlockFlow\n\n\nLaplacians.localFlow\n\n\nLaplacians.localImprove\n\n\nLaplacians.mapweight\n\n\nLaplacians.pathGraph\n\n\nLaplacians.pcg\n\n\nLaplacians.pcgLapSolver\n\n\nLaplacians.pcgSolver\n\n\nLaplacians.plotGraph\n\n\nLaplacians.power\n\n\nLaplacians.prefAttach\n\n\nLaplacians.prim\n\n\nLaplacians.print_ll_col\n\n\nLaplacians.print_ll_col\n\n\nLaplacians.prn\n\n\nLaplacians.productGraph\n\n\nLaplacians.pureRandomGraph\n\n\nLaplacians.pushSpeedResult!\n\n\nLaplacians.randGenRing\n\n\nLaplacians.randMatching\n\n\nLaplacians.randRegular\n\n\nLaplacians.randWeight\n\n\nLaplacians.randishKruskal\n\n\nLaplacians.randishPrim\n\n\nLaplacians.readIJ\n\n\nLaplacians.readIJV\n\n\nLaplacians.refineCut\n\n\nLaplacians.ringGraph\n\n\nLaplacians.sampleByWeight\n\n\nLaplacians.samplingLapSolver\n\n\nLaplacians.samplingSDDMSolver\n\n\nLaplacians.sddmWrapLap\n\n\nLaplacians.semiWtedChimera\n\n\nLaplacians.setValue\n\n\nLaplacians.shortIntGraph\n\n\nLaplacians.shortestPathTree\n\n\nLaplacians.shortestPaths\n\n\nLaplacians.sortSet\n\n\nLaplacians.sparsify\n\n\nLaplacians.spectralCoords\n\n\nLaplacians.spectralDrawing\n\n\nLaplacians.subsampleEdges\n\n\nLaplacians.support\n\n\nLaplacians.testZeroDiag\n\n\nLaplacians.thicken\n\n\nLaplacians.thicken_once\n\n\nLaplacians.treeDepthDFS\n\n\nLaplacians.twoLift\n\n\nLaplacians.uniformWeight\n\n\nLaplacians.uniformWeight!\n\n\nLaplacians.unweight\n\n\nLaplacians.unweight!\n\n\nLaplacians.vecToComps\n\n\nLaplacians.wGrid2\n\n\nLaplacians.wGrid3\n\n\nLaplacians.wdeg\n\n\nLaplacians.wrapCapture\n\n\nLaplacians.wrapCaptureRhs\n\n\nLaplacians.wrapInterface\n\n\nLaplacians.writeIJV\n\n\nLaplacians.wtedChimera\n\n\nLaplacians.wtedChimera\n\n\nLaplacians.wtedEdgeVertexMat", 
            "title": "All of the above"
        }, 
        {
            "location": "/indexOfAll/index.html#index-of-all-exported", 
            "text": "This is an index of all the exported methods. We would include the docstrings, but Documenter.jl does not let us.   Laplacians.ApproxCholPQ  Laplacians.ApproxCholParams  Laplacians.KMPParams  Laplacians.LDLinv  Laplacians.LLmatp  Laplacians.LLp  Laplacians.samplingParams  Base.Random.randperm  Laplacians.ErdosRenyi  Laplacians.ErdosRenyiCluster  Laplacians.ErdosRenyiClusterFix  Laplacians.KMPLapSolver  Laplacians.KMPSDDMSolver  Laplacians.addToGPrime  Laplacians.adj  Laplacians.akpw  Laplacians.akpwU  Laplacians.approxCholLap  Laplacians.approxCholLapChol  Laplacians.approxCholPQDec!  Laplacians.approxCholPQInc!  Laplacians.approxCholSddm  Laplacians.approxQual  Laplacians.apr  Laplacians.augTreeLap  Laplacians.augTreeLapPrecon  Laplacians.augTreePrecon  Laplacians.augTreeSddm  Laplacians.augmentTree  Laplacians.augmentTreeOpt  Laplacians.backIndices  Laplacians.backIndices  Laplacians.biggestComp  Laplacians.blockSolver  Laplacians.cg  Laplacians.cgLapSolver  Laplacians.cgSolver  Laplacians.chimera  Laplacians.chimera  Laplacians.cholLap  Laplacians.cholSDDM  Laplacians.compConductance  Laplacians.compStretches  Laplacians.completeBinaryTree  Laplacians.completeGraph  Laplacians.components  Laplacians.condNumber  Laplacians.conditionNumber  Laplacians.conditionNumber  Laplacians.diagmat  Laplacians.disjoin  Laplacians.dumbRefineCut  Laplacians.edgeVertexMat  Laplacians.extendMatrix  Laplacians.findEntries  Laplacians.flipIndex  Laplacians.floatGraph  Laplacians.forceLap  Laplacians.generalizedNecklace  Laplacians.generalizedRing  Laplacians.getCutSet  Laplacians.getObound  Laplacians.getVolume  Laplacians.grid2  Laplacians.grid2coords  Laplacians.grid3  Laplacians.grownGraph  Laplacians.grownGraphD  Laplacians.hyperCube  Laplacians.initDictCol!  Laplacians.initGPrime  Laplacians.isConnected  Laplacians.joinGraphs  Laplacians.kruskal  Laplacians.lap  Laplacians.lapWrapComponents  Laplacians.lapWrapConnected  Laplacians.lapWrapSDDM  Laplacians.ldli2Chol  Laplacians.localBlockFlow  Laplacians.localFlow  Laplacians.localImprove  Laplacians.mapweight  Laplacians.pathGraph  Laplacians.pcg  Laplacians.pcgLapSolver  Laplacians.pcgSolver  Laplacians.plotGraph  Laplacians.power  Laplacians.prefAttach  Laplacians.prim  Laplacians.print_ll_col  Laplacians.print_ll_col  Laplacians.prn  Laplacians.productGraph  Laplacians.pureRandomGraph  Laplacians.pushSpeedResult!  Laplacians.randGenRing  Laplacians.randMatching  Laplacians.randRegular  Laplacians.randWeight  Laplacians.randishKruskal  Laplacians.randishPrim  Laplacians.readIJ  Laplacians.readIJV  Laplacians.refineCut  Laplacians.ringGraph  Laplacians.sampleByWeight  Laplacians.samplingLapSolver  Laplacians.samplingSDDMSolver  Laplacians.sddmWrapLap  Laplacians.semiWtedChimera  Laplacians.setValue  Laplacians.shortIntGraph  Laplacians.shortestPathTree  Laplacians.shortestPaths  Laplacians.sortSet  Laplacians.sparsify  Laplacians.spectralCoords  Laplacians.spectralDrawing  Laplacians.subsampleEdges  Laplacians.support  Laplacians.testZeroDiag  Laplacians.thicken  Laplacians.thicken_once  Laplacians.treeDepthDFS  Laplacians.twoLift  Laplacians.uniformWeight  Laplacians.uniformWeight!  Laplacians.unweight  Laplacians.unweight!  Laplacians.vecToComps  Laplacians.wGrid2  Laplacians.wGrid3  Laplacians.wdeg  Laplacians.wrapCapture  Laplacians.wrapCaptureRhs  Laplacians.wrapInterface  Laplacians.writeIJV  Laplacians.wtedChimera  Laplacians.wtedChimera  Laplacians.wtedEdgeVertexMat", 
            "title": "Index of all exported"
        }
    ]
}