{
    "docs": [
        {
            "location": "/about/index.html", 
            "text": "About Laplacians.jl\n\n\nLaplacians is a package containing graph algorithms, with an emphsasis on tasks related to spectral and algebraic graph theory. It contains (and will contain more) code for solving systems of linear equations in graph Laplacians, low stretch spanning trees, sparsifiation, clustering, local clustering, and optimization on graphs.\n\n\nAll graphs are represented by sparse adjacency matrices. This is both for speed, and because our main concerns are algebraic tasks. It does not handle dynamic graphs. It would be very slow to implement dynamic graphs this way.\n\n\nMost of this package is not ready for use. The routines that are ready are the graph generators (chimera and wtedChimera), and the routines for computing the stretch of spanning trees.  As soon as we release the package, we expect to discover that many things are broken.\n\n\nLaplacians.jl was started by Daniel A. Spielman.  Other contributors include Rasmus Kyng, Xiao Shi, Sushant Sachdeva, Serban Stan and Jackson Thea.", 
            "title": "About"
        }, 
        {
            "location": "/about/index.html#about-laplaciansjl", 
            "text": "Laplacians is a package containing graph algorithms, with an emphsasis on tasks related to spectral and algebraic graph theory. It contains (and will contain more) code for solving systems of linear equations in graph Laplacians, low stretch spanning trees, sparsifiation, clustering, local clustering, and optimization on graphs.  All graphs are represented by sparse adjacency matrices. This is both for speed, and because our main concerns are algebraic tasks. It does not handle dynamic graphs. It would be very slow to implement dynamic graphs this way.  Most of this package is not ready for use. The routines that are ready are the graph generators (chimera and wtedChimera), and the routines for computing the stretch of spanning trees.  As soon as we release the package, we expect to discover that many things are broken.  Laplacians.jl was started by Daniel A. Spielman.  Other contributors include Rasmus Kyng, Xiao Shi, Sushant Sachdeva, Serban Stan and Jackson Thea.", 
            "title": "About Laplacians.jl"
        }, 
        {
            "location": "/Installation/index.html", 
            "text": "Installation\n\n\nPython\n\n\nJulia\n\n\nLaplacians\n\n\nUsing Laplacians\n\n\n\n\n\n\n\n\n\n\nInstallation\n\n\nBefore you can use Laplacians, you need Julia.\nSo, we'll begin with instructions for installing Julia.  I (Dan S.) found that it worked best if I installed Python first.  So, I'll suggest that you do the same.\n\n\nAll of these instruction assume you are using a Mac.  \n\n\nPython\n\n\nInstall python.  I recommend the anaconda distribution \nhttps://www.continuum.io/downloads\n.\n\n\nOnce you install python, you are going to want two packages: a plotting package that Julia will use, and jupyter notebooks for interacting with Julia.  Install them like\n\n\nconda install matplotlib\nconda install mathjax\nconda install jupyter\n\n\n\n\nJulia\n\n\nYou can get Julia from \n\nhttp://julialang.org/\n.  I haven't had much luck with Juno, so I recommend the plain Julia installation.\n\n\nOnce you have this, you will want Julia notebooks.  To install this, run \njulia\n and type\n\n\njulia\n Pkg.add(\nIJulia\n)\njulia\n using IJulia\n\n\n\n\nThis will install the package, and put the current julia kernel into \njupyter\n.  In the future, you can launch the Julia notebook by typing (in a terminal)\n\n\njupyter notebook\n\n\n\n\nLaplacians\n\n\nIn theory, all you need to do now is type either\n\n\njulia\n Pkg.clone(\ngit://github.com/danspielman/Laplacians.jl.git\n)\n\n\n\n\nOr, in a few days,\n\n\njulia\n Pkg.add(\nLaplacians\n)\n\n\n\n\nThis should add all the packages upon which Laplacians explicitly depends.  \n\n\nSome of the options for the routine \nakpw\n require LightGraphs and Metis.  But, you probably don't want to use these, so don't worry about them for now.\n\n\nUsing Laplacians\n\n\nTo actually use the Laplacians package in a Julia session, you must type\n\n\njulia\n using Laplacians \n\n\n\n\nTo see if Laplacians is working, try typing\n\n\na = chimera(100,6)\nspectralDrawing(a)\n\n\n\n\nor\n\n\na = generalizedNecklace(grid2(6),grid2(3),2)\nspectralDrawing(a)", 
            "title": "Installation"
        }, 
        {
            "location": "/Installation/index.html#installation", 
            "text": "Before you can use Laplacians, you need Julia.\nSo, we'll begin with instructions for installing Julia.  I (Dan S.) found that it worked best if I installed Python first.  So, I'll suggest that you do the same.  All of these instruction assume you are using a Mac.", 
            "title": "Installation"
        }, 
        {
            "location": "/Installation/index.html#python", 
            "text": "Install python.  I recommend the anaconda distribution  https://www.continuum.io/downloads .  Once you install python, you are going to want two packages: a plotting package that Julia will use, and jupyter notebooks for interacting with Julia.  Install them like  conda install matplotlib\nconda install mathjax\nconda install jupyter", 
            "title": "Python"
        }, 
        {
            "location": "/Installation/index.html#julia", 
            "text": "You can get Julia from  http://julialang.org/ .  I haven't had much luck with Juno, so I recommend the plain Julia installation.  Once you have this, you will want Julia notebooks.  To install this, run  julia  and type  julia  Pkg.add( IJulia )\njulia  using IJulia  This will install the package, and put the current julia kernel into  jupyter .  In the future, you can launch the Julia notebook by typing (in a terminal)  jupyter notebook", 
            "title": "Julia"
        }, 
        {
            "location": "/Installation/index.html#laplacians", 
            "text": "In theory, all you need to do now is type either  julia  Pkg.clone( git://github.com/danspielman/Laplacians.jl.git )  Or, in a few days,  julia  Pkg.add( Laplacians )  This should add all the packages upon which Laplacians explicitly depends.    Some of the options for the routine  akpw  require LightGraphs and Metis.  But, you probably don't want to use these, so don't worry about them for now.", 
            "title": "Laplacians"
        }, 
        {
            "location": "/Installation/index.html#using-laplacians", 
            "text": "To actually use the Laplacians package in a Julia session, you must type  julia  using Laplacians   To see if Laplacians is working, try typing  a = chimera(100,6)\nspectralDrawing(a)  or  a = generalizedNecklace(grid2(6),grid2(3),2)\nspectralDrawing(a)", 
            "title": "Using Laplacians"
        }, 
        {
            "location": "/Julia/index.html", 
            "text": "Using Julia\n\n\nDocstrings\n\n\nJulia Notebooks\n\n\nWorkflows\n\n\nDan's current workflow:\n\n\nAdd your current workflow here:\n\n\n\n\n\n\nThings to be careful of (common bugs)\n\n\nUseful Julia functions\n\n\nOptimizing code in Julia\n\n\nVectorization is Bad.\n\n\n\n\n\n\nHow should notebooks play with Git?\n\n\n\n\n\n\n\n\n\n\nUsing Julia\n\n\nThese are some things you might want to know about using Julia if it is new to you.\n\n\nDocstrings\n\n\nJulia 0.4 lets you take advantage of docstrings.\nFor example, \n?ringGraph\n produces\n\n\nThe simple ring on n vertices\n\n\n\n\nWhen having a multiline comment, make sure that lines don't have starting and trailing spaces.\nThis will mess up the indentation when calling '?func_name'.\n\n\nJulia Notebooks\n\n\nTo get the Julia notebooks working, I presently type \njupyter notebook\n.\nI then select the kernel to be Julia-0.4.2.\nIt seems important to run this command from a directory that contains all the directories\nthat have notebooks that you will use.  In particular, I advise against \"uploading\" notebooks\nfrom other directories.  That has only given me trouble.\n\n\nThe calico extensions that seem to be hosted at Brynmawr seem interesting.\nI haven't yet figured out how to get them to work.\nHere are the relevent links:\n\n\n\n\nhttp://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Help.ipynb\n\n\nhttp://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Notebook%20Users%20Manual.ipynb\n\n\n\n\nTo turn a notebook into html, you type something like\n\n\nipython nbconvert Laplacians.ipynb\n\n\n\n\nor\n\n\nipython nbconvert --to markdown --stdout Sampler.ipynb \n SamplerNotebook.md\n\n\n\n\nWorkflows\n\n\nJulia has an IDE called Juno.  Both Dan and Serban have encountered some trouble with it: we have both found that it sometimes refuses to reload .jl code that we have written.  Please document workflows that you have found useful here:\n\n\nDan's current workflow:\n\n\n\n\nI use emacs (which has a mode for Julia) and the notebooks.\n\n\nI develop Julia code in a \"temporary\" file with a name like develX.jl.  While I am developing, this code is not included by the module to which it will eventually belong.\n\n\n\n\nAfter modifying code, I reload it with \ninclude(\"develX.jl\")\n.  This works fine for reloading methods.  It is not a good way to reload modules or types.  So, I usually put the types either in a separate file, or in my julia notebook.\n\n\n\n\n\n\nI am writing this documention in MacDown.\n\n\n\n\n\n\nAdd your current workflow here:\n\n\nThings to be careful of (common bugs)\n\n\n\n\n\n\nJulia passes vectors and matrices to routines by reference, rather than by copying them.  If you type \nx = y\n when x and y are arrays, then this will make x a pointer to y.  If you want x to be a copy of y, type \nx = copy(y)\n.  This can really mess up matlab programmers.  I wrote many functions that were modifying their arguments without realizing it.\n\n\n\n\n\n\nOn the other hand, if you type \nx = x + y\n, then x becomes a newly allocated vector and no longer refers to the original.  This is true even if you type \nx += y\n.  Here is an example that shows two of the possible behaviors, and the difference between what happens inside functions.\n\n\n\n\n\n\n\n\nAdds b in to a\n\nfunction addA2B(a,b)\n    for i in 1:length(a)\n        a[i] += b[i]\n    end\nend\n\n\nFails to add b in to a\n\nfunction addA2Bfail(a,b)\n    a += b\nend\n\na = [1 0]\nb = [2 2]\naddA2B(a,b)\na\n\n1x2 Array{Int64,2}:\n 3  2\n\na = [1 0]\nb = [2 2]\naddA2Bfail(a,b)\na\n\n1x2 Array{Int64,2}:\n 1  0\n\na += b\na\n\n1x2 Array{Int64,2}:\n 3  2\n\n\n\n\n\n\n\n\n\nIf you are used to programming in Matlab, you might be tempted to type a line like \nfor i in 1:10,\n.  \nDo not put extra commas in Julia!\n  It will cause bad things to happen.\n\n\n\n\n\n\nTo get a vector with entries 1 through n, type \ncollect(1:n)\n.  The object \n1:n\n is a range, rather than a vector.\n\n\n\n\n\n\nJulia sparse matrix entries dissapear if they are set to 0. In order to overcome this, use the \nsetValue\n function. \nsetValue(G, u, i, 0)\n will set \nweighti(G, u, i)\n to 0 while also leaving \n(u, nbri(G, u, i))\n in the matrix.\n\n\n\n\n\n\nUseful Julia functions\n\n\nI am going to make a short list of Julia functions/features that I find useful.  Please add those that you use often as well.\n\n\n\n\n\n\ndocstrings: in the above example, I used a docstring to document each function.  You can get these by typing \n?addA2B\n.  You can also  \nwrite longer docstrings and use markdown\n.  I suggest putting them in front of every function.\n\n\n\n\n\n\nmethods(foo)\n lists all methods with the name foo.\n\n\n\n\nfieldnames(footype)\n tells you all the fields of footype.  Note that this is 0.4.  In 0.3.11, you type \nnames(footype)\n\n\n\n\njulia\n a = sparse(rand(3,3));\njulia\n fieldnames(a)\n5-element Array{Symbol,1}:\n :m\n :n\n :colptr\n :rowval\n :nzval\n\n\n\n\nOptimizing code in Julia\n\n\nThe best way that I've found of figuring out what's slowing down my code has been to use \n@code_warntype\n.  It only exists in version 4 of Julia.  For this reason, I keep one of those around.\n\n\nNote that the first time you run a piece of code in Julia, it gets compiled.  So, you should run it on a small example before trying to time it.  Then, use \n@time\n to time your code.\n\n\nI recommend reading the Performance Tips in the Julia documentation, not that I've understood all of it yet.\n\n\nVectorization is Bad.\n\n\nJulia is the anti-matlab in that vectorization is slow.\nStill it is a good way to write your code the first time.\nHere are some examples of code that adds one vector into another.\nThe first is vectorized, the second turns that into a loop, and the fastest uses BLAS.  Note that this was done in Julia 0.3.11.  The vectorized code is much faster, but still not fast, in 0.4.\n\nAlso note that you have to run each routine once before it will be fast.  This is because it compiles it the first time your run it\n\n\nn = 10^7\na = rand(n)\nb = rand(n)\n@time a += b;\n\nelapsed time: 0.155296017 seconds (80000312 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time add2(a,b);\n\nelapsed time: 0.021190554 seconds (80 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time BLAS.axpy!(1.0,b,a);\n\nelapsed time: 0.015894922 seconds (80 bytes allocated)\n\n\n\n\n\nOne reason that \na += b\n was slow was that it seems to allocate a lot of memory.\n\n\nHow should notebooks play with Git?\n\n\nThe great thing about the notebooks is that they contain live code, so that you can play with them.  But, sometimes you get a version that serves as great documentation, and you don't want to klobber it my mistake later (or evern worse, have someone else klobber it).  Presumably if someone accidently commits a messed up version we can unwind that.  But, is there a good way to keep track of this?", 
            "title": "Using Julia"
        }, 
        {
            "location": "/Julia/index.html#using-julia", 
            "text": "These are some things you might want to know about using Julia if it is new to you.", 
            "title": "Using Julia"
        }, 
        {
            "location": "/Julia/index.html#docstrings", 
            "text": "Julia 0.4 lets you take advantage of docstrings.\nFor example,  ?ringGraph  produces  The simple ring on n vertices  When having a multiline comment, make sure that lines don't have starting and trailing spaces.\nThis will mess up the indentation when calling '?func_name'.", 
            "title": "Docstrings"
        }, 
        {
            "location": "/Julia/index.html#julia-notebooks", 
            "text": "To get the Julia notebooks working, I presently type  jupyter notebook .\nI then select the kernel to be Julia-0.4.2.\nIt seems important to run this command from a directory that contains all the directories\nthat have notebooks that you will use.  In particular, I advise against \"uploading\" notebooks\nfrom other directories.  That has only given me trouble.  The calico extensions that seem to be hosted at Brynmawr seem interesting.\nI haven't yet figured out how to get them to work.\nHere are the relevent links:   http://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Help.ipynb  http://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Notebook%20Users%20Manual.ipynb   To turn a notebook into html, you type something like  ipython nbconvert Laplacians.ipynb  or  ipython nbconvert --to markdown --stdout Sampler.ipynb   SamplerNotebook.md", 
            "title": "Julia Notebooks"
        }, 
        {
            "location": "/Julia/index.html#workflows", 
            "text": "Julia has an IDE called Juno.  Both Dan and Serban have encountered some trouble with it: we have both found that it sometimes refuses to reload .jl code that we have written.  Please document workflows that you have found useful here:", 
            "title": "Workflows"
        }, 
        {
            "location": "/Julia/index.html#dans-current-workflow", 
            "text": "I use emacs (which has a mode for Julia) and the notebooks.  I develop Julia code in a \"temporary\" file with a name like develX.jl.  While I am developing, this code is not included by the module to which it will eventually belong.   After modifying code, I reload it with  include(\"develX.jl\") .  This works fine for reloading methods.  It is not a good way to reload modules or types.  So, I usually put the types either in a separate file, or in my julia notebook.    I am writing this documention in MacDown.", 
            "title": "Dan's current workflow:"
        }, 
        {
            "location": "/Julia/index.html#add-your-current-workflow-here", 
            "text": "", 
            "title": "Add your current workflow here:"
        }, 
        {
            "location": "/Julia/index.html#things-to-be-careful-of-common-bugs", 
            "text": "Julia passes vectors and matrices to routines by reference, rather than by copying them.  If you type  x = y  when x and y are arrays, then this will make x a pointer to y.  If you want x to be a copy of y, type  x = copy(y) .  This can really mess up matlab programmers.  I wrote many functions that were modifying their arguments without realizing it.    On the other hand, if you type  x = x + y , then x becomes a newly allocated vector and no longer refers to the original.  This is true even if you type  x += y .  Here is an example that shows two of the possible behaviors, and the difference between what happens inside functions.     Adds b in to a \nfunction addA2B(a,b)\n    for i in 1:length(a)\n        a[i] += b[i]\n    end\nend Fails to add b in to a \nfunction addA2Bfail(a,b)\n    a += b\nend\n\na = [1 0]\nb = [2 2]\naddA2B(a,b)\na\n\n1x2 Array{Int64,2}:\n 3  2\n\na = [1 0]\nb = [2 2]\naddA2Bfail(a,b)\na\n\n1x2 Array{Int64,2}:\n 1  0\n\na += b\na\n\n1x2 Array{Int64,2}:\n 3  2    If you are used to programming in Matlab, you might be tempted to type a line like  for i in 1:10, .   Do not put extra commas in Julia!   It will cause bad things to happen.    To get a vector with entries 1 through n, type  collect(1:n) .  The object  1:n  is a range, rather than a vector.    Julia sparse matrix entries dissapear if they are set to 0. In order to overcome this, use the  setValue  function.  setValue(G, u, i, 0)  will set  weighti(G, u, i)  to 0 while also leaving  (u, nbri(G, u, i))  in the matrix.", 
            "title": "Things to be careful of (common bugs)"
        }, 
        {
            "location": "/Julia/index.html#useful-julia-functions", 
            "text": "I am going to make a short list of Julia functions/features that I find useful.  Please add those that you use often as well.    docstrings: in the above example, I used a docstring to document each function.  You can get these by typing  ?addA2B .  You can also   write longer docstrings and use markdown .  I suggest putting them in front of every function.    methods(foo)  lists all methods with the name foo.   fieldnames(footype)  tells you all the fields of footype.  Note that this is 0.4.  In 0.3.11, you type  names(footype)   julia  a = sparse(rand(3,3));\njulia  fieldnames(a)\n5-element Array{Symbol,1}:\n :m\n :n\n :colptr\n :rowval\n :nzval", 
            "title": "Useful Julia functions"
        }, 
        {
            "location": "/Julia/index.html#optimizing-code-in-julia", 
            "text": "The best way that I've found of figuring out what's slowing down my code has been to use  @code_warntype .  It only exists in version 4 of Julia.  For this reason, I keep one of those around.  Note that the first time you run a piece of code in Julia, it gets compiled.  So, you should run it on a small example before trying to time it.  Then, use  @time  to time your code.  I recommend reading the Performance Tips in the Julia documentation, not that I've understood all of it yet.", 
            "title": "Optimizing code in Julia"
        }, 
        {
            "location": "/Julia/index.html#vectorization-is-bad", 
            "text": "Julia is the anti-matlab in that vectorization is slow.\nStill it is a good way to write your code the first time.\nHere are some examples of code that adds one vector into another.\nThe first is vectorized, the second turns that into a loop, and the fastest uses BLAS.  Note that this was done in Julia 0.3.11.  The vectorized code is much faster, but still not fast, in 0.4. Also note that you have to run each routine once before it will be fast.  This is because it compiles it the first time your run it  n = 10^7\na = rand(n)\nb = rand(n)\n@time a += b;\n\nelapsed time: 0.155296017 seconds (80000312 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time add2(a,b);\n\nelapsed time: 0.021190554 seconds (80 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time BLAS.axpy!(1.0,b,a);\n\nelapsed time: 0.015894922 seconds (80 bytes allocated)  One reason that  a += b  was slow was that it seems to allocate a lot of memory.", 
            "title": "Vectorization is Bad."
        }, 
        {
            "location": "/Julia/index.html#how-should-notebooks-play-with-git", 
            "text": "The great thing about the notebooks is that they contain live code, so that you can play with them.  But, sometimes you get a version that serves as great documentation, and you don't want to klobber it my mistake later (or evern worse, have someone else klobber it).  Presumably if someone accidently commits a messed up version we can unwind that.  But, is there a good way to keep track of this?", 
            "title": "How should notebooks play with Git?"
        }, 
        {
            "location": "/Examples/index.html", 
            "text": "Examples\n\n\nThe following are links to html files of Julia notebooks.\nThese notebooks are also in the notebook directory, and can be open there so that you can run the code live.\nYou should be able to find them under ~/.julia/v0.4/Laplacians/notebooks.\n\n\n\n\nFirstNotebook\n\n\nSolvers\n\n\nSampler\n\n\nLocalClustering", 
            "title": "Examples"
        }, 
        {
            "location": "/Examples/index.html#examples", 
            "text": "The following are links to html files of Julia notebooks.\nThese notebooks are also in the notebook directory, and can be open there so that you can run the code live.\nYou should be able to find them under ~/.julia/v0.4/Laplacians/notebooks.   FirstNotebook  Solvers  Sampler  LocalClustering", 
            "title": "Examples"
        }, 
        {
            "location": "/CSCgraph/index.html", 
            "text": "Using sparse matrices as graphs\n\n\nThe routines \ndeg\n, \nnbri\n and \nweighti\n will let you treat a sparse matrix like a graph.\n\n\ndeg(graph, u)\n is the degree of node u.\n\nnbri(graph, u, i)\n is the ith neighbor of node u.\n\nweighti(graph, u, i)\n is the weight of the edge to the ith neighbor of node u.\n\n\nNote that we start indexing from 1.\n\n\nFor example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:\n\n\n  for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end\n\n\n\n\nBut, this turns out to be much slower than working with the structure directly, like\n\n\n  for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end\n\n\n\n\n\n\n[ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.\n\n\n\n\nThe SparseMatrixCSC data structure\n\n\nYou can explore what is going on with the data structure by looking at some examples.  For example, here is a randomly weighted complete graph on 4 vertices, first displayed as a matrix:\n\n\ngr = round(10*uniformWeight(completeGraph(4)))\n\n4x4 sparse matrix with 12 Float64 entries:\n    [2, 1]  =  3.0\n    [3, 1]  =  3.0\n    [4, 1]  =  6.0\n    [1, 2]  =  3.0\n    [3, 2]  =  1.0\n    [4, 2]  =  2.0\n    [1, 3]  =  3.0\n    [2, 3]  =  1.0\n    [4, 3]  =  7.0\n    [1, 4]  =  6.0\n    [2, 4]  =  2.0\n    [3, 4]  =  7.0\n\nfull(gr)\n\n4x4 Array{Float64,2}:\n 0.0  3.0  3.0  6.0\n 3.0  0.0  1.0  2.0\n 3.0  1.0  0.0  7.0\n 6.0  2.0  7.0  0.0\n\n\n\n\nTo see the underlying data structure, use \nfieldnames\n.\n\n\nfieldnames(gr)\n\n5-element Array{Symbol,1}:\n :m     \n :n     \n :colptr\n :rowval\n :nzval \n\n\n\n\nm\n and \nn\n are the dimensions of the matrix.\nThe entries of the matrix are stored in nzval.\ncolptr[i] is the index in nzval of the first nonzero entry\nin column i.  rowval tells you which rows in each column are nonzero.\nThe indices of the nonzero entries in column i are stored in \nrowval[colptr[i]] through rowval[colptr[i+1]-1].\n\n\ngr.colptr \n\n5-element Array{Int64,1}:\n  1\n  4\n  7\n 10\n 13\n\n [gr.rowval gr.nzval]\n\n 12x2 Array{Float64,2}:\n 2.0  3.0\n 3.0  3.0\n 4.0  6.0\n 1.0  3.0\n 3.0  1.0\n 4.0  2.0\n 1.0  3.0\n 2.0  1.0\n 4.0  7.0\n 1.0  6.0\n 2.0  2.0\n 3.0  7.0", 
            "title": "Sparse matrices as graphs"
        }, 
        {
            "location": "/CSCgraph/index.html#using-sparse-matrices-as-graphs", 
            "text": "The routines  deg ,  nbri  and  weighti  will let you treat a sparse matrix like a graph.  deg(graph, u)  is the degree of node u. nbri(graph, u, i)  is the ith neighbor of node u. weighti(graph, u, i)  is the weight of the edge to the ith neighbor of node u.  Note that we start indexing from 1.  For example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:    for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end  But, this turns out to be much slower than working with the structure directly, like    for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end   [ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.", 
            "title": "Using sparse matrices as graphs"
        }, 
        {
            "location": "/CSCgraph/index.html#the-sparsematrixcsc-data-structure", 
            "text": "You can explore what is going on with the data structure by looking at some examples.  For example, here is a randomly weighted complete graph on 4 vertices, first displayed as a matrix:  gr = round(10*uniformWeight(completeGraph(4)))\n\n4x4 sparse matrix with 12 Float64 entries:\n    [2, 1]  =  3.0\n    [3, 1]  =  3.0\n    [4, 1]  =  6.0\n    [1, 2]  =  3.0\n    [3, 2]  =  1.0\n    [4, 2]  =  2.0\n    [1, 3]  =  3.0\n    [2, 3]  =  1.0\n    [4, 3]  =  7.0\n    [1, 4]  =  6.0\n    [2, 4]  =  2.0\n    [3, 4]  =  7.0\n\nfull(gr)\n\n4x4 Array{Float64,2}:\n 0.0  3.0  3.0  6.0\n 3.0  0.0  1.0  2.0\n 3.0  1.0  0.0  7.0\n 6.0  2.0  7.0  0.0  To see the underlying data structure, use  fieldnames .  fieldnames(gr)\n\n5-element Array{Symbol,1}:\n :m     \n :n     \n :colptr\n :rowval\n :nzval   m  and  n  are the dimensions of the matrix.\nThe entries of the matrix are stored in nzval.\ncolptr[i] is the index in nzval of the first nonzero entry\nin column i.  rowval tells you which rows in each column are nonzero.\nThe indices of the nonzero entries in column i are stored in \nrowval[colptr[i]] through rowval[colptr[i+1]-1].  gr.colptr \n\n5-element Array{Int64,1}:\n  1\n  4\n  7\n 10\n 13\n\n [gr.rowval gr.nzval]\n\n 12x2 Array{Float64,2}:\n 2.0  3.0\n 3.0  3.0\n 4.0  6.0\n 1.0  3.0\n 3.0  1.0\n 4.0  2.0\n 1.0  3.0\n 2.0  1.0\n 4.0  7.0\n 1.0  6.0\n 2.0  2.0\n 3.0  7.0", 
            "title": "The SparseMatrixCSC data structure"
        }, 
        {
            "location": "/solvers/index.html", 
            "text": "Solving linear equations in Laplacians\n\n\nDirect Solvers\n\n\nIterative Solvers\n\n\nLow-Stretch Spanning Trees\n\n\nAugmented spanning tree preconditioners\n\n\n\n\n\n\n\n\n\n\nSolving linear equations in Laplacians\n\n\nFor some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.\n\n\nDirect Solvers\n\n\nYou can compute a cholesky factor directly with \ncholfact\n.  It does  more than just compute the factor, and it saves its result in a data structure that implements \n\\\n.  It uses SuiteSparse by Tim Davis.\n\n\nHere is an example of how you would use it to solve a general non-singular linear system.\n\n\na = grid2(5)\nla = lap(a)\nla[1,1] = la[1,1] + 1\nF = cholfact(la)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(la*x-b)\n\n    1.0598778281116327e-14\n\n\n\n\nLaplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.\n\n\nla = lap(a)\nf = lapWrapSolver(cholfact,la)\nb = randn(n); b = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15\n\n\n\n\nHere are two other ways of using the wrapper:\n\n\nlapChol = lapWrapSolver(cholfact)\nf = lapChol(la)\nb = randn(n);\nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.6924696662484416e-15\n\nx = lapWrapSolver(cholfact,la,b)\nnorm(la*x - b)\n    2.6924696662484416e-15\n\n\n\n\nIterative Solvers\n\n\nThe first, of course, is the Conjugate Gradient (cg).\n\n\nOur implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance \ntol\n and the maximum number of iterations, \nmaxits\n.  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  Here are examples.\n\n\nn = 50\na = randn(n,n); a = a * a';\nb = randn(n)\nx = cg(a,b,maxits=100)\nnorm(a*x - b)\n    1.2191649497921835e-6\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(a,bbig,maxits=100)\nnorm(a*xbig - bbig)\n    1.494919244242202629856363570306545126541716514824419323325986374186529786019681e-33\n\n\n\n\nAs a sanity check, we do two speed tests against Matlab.\n\n\nla = lap(grid2(200))\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n@time x = cg(la,b,maxits=1000)\n    0.813791 seconds (2.77 k allocations: 211.550 MB, 3.56% gc time)\n\nnorm(la*x-b)\n    0.0001900620047823064\n\n\n\n\nAnd, in Matlab:\n\n\n a = grid2(200);\n\n la = lap(a);\n\n b = randn(length(a),1); b = b - mean(b);\n\n tic; x = pcg(la,b,[],1000); toc\npcg converged at iteration 688 to a solution with relative residual 9.8e-07.\nElapsed time is 1.244917 seconds.\n\n norm(la*x-b)\n\nans =\n\n   1.9730e-04\n\n\n\n\nPCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.\n\n\na = mapweight(grid2(200),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\npre(x) = x ./ d\n@time x = pcg(la,b,pre,maxits=2000)\n    3.322035 seconds (42.21 k allocations: 1.194 GB, 5.11% gc time)\nnorm(la*x-b)\n    0.008508746034886803\n\n\n\n\nIf our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.\n\n\n@time x = pcg(la,b,pre,tol=1e-1,maxits=10^5)\n    0.747042 seconds (9.65 k allocations: 275.819 MB, 4.87% gc time)\nnorm(la*x-b)\n    19.840756251253442\n\n@time x = cg(la,b,tol=1e-1,maxits=10^5)\n    6.509665 seconds (22.55 k allocations: 1.680 GB, 3.68% gc time)\nnorm(la*x-b)\n    19.222483530605043\n\n\n\n\nLow-Stretch Spanning Trees\n\n\nIn order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code in Julia that is guaranteed to produce these.  Instead, for now, we have two routines that can be thought of as randomized versions of Prim and Kruskall's algorithm.\n\nrandishKruskall\n samples the remaining edges with probability proportional to their weight.  \nrandishPrim\n samples edges on the boundary while using the same rule.\n\n\nBoth use a data structure called \nSampler\n that allows you to store integers with real values, and to sample according to those real values.\n\n\nWe also have code for computing the stretches.\nHere are some examples.\n\n\na = grid2(1000)\nt = randishKruskal(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    43.410262262262265\n\nt = randishPrim(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    33.14477077077077\n\n\n\n\n\nAugmented spanning tree preconditioners\n\n\nHere is code that will invoke one.\nIt is designed for positive definite systems.  So, let's give it one.\nRight now, it is using a randomized version of a MST.  There is no real reason to think that this should work.\n\n\na = mapweight(grid2(1000),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nla[1,1] = la[1,1] + 1\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000)\n    6.529052 seconds (4.00 M allocations: 1.858 GB, 15.34% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.058915 seconds (9.74 k allocations: 23.209 GB, 6.84% gc time)\n\nnorm(la*x - b)\n    99.74452367765869\n\n# Now, let's contrast with using CG\n\n@time y = cg(la,b,tol=1e-1,maxits=1000)\n    28.719631 seconds (4.01 k allocations: 7.473 GB, 3.74% gc time)\n\nnorm(la*y-b)\n    3243.6014713600766\n\n\n\n\n\nThat was not too impressive.  We will have to investigate.  By default, it presently uses randishKruskal.  Let's try randishPrim.  You can pass the treeAlg as a parameter.\n\n\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000,treeAlg=randishPrim);\n    6.319489 seconds (4.00 M allocations: 2.030 GB, 18.81% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.503484 seconds (9.76 k allocations: 23.268 GB, 7.31% gc time)\n\nnorm(la*x - b)\n    99.29610874176991\n\n\n\n\nTo solve systems in a Laplacian, we could wrap it.\n\n\nn = 40000\nla = lap(randRegular(n,3))\nf = lapWrapSolver(augTreeSolver,la,tol=1e-6,maxits=1000)\nb = randn(n); b = b - mean(b)\nx = f(b)\nnorm(la*x-b)\n    0.00019304778073388\n\n\n\n\nAs you can see, lapWrapSolver can pass tol and maxits arguments to its solver, if they are given to it.", 
            "title": "Solvers"
        }, 
        {
            "location": "/solvers/index.html#solving-linear-equations-in-laplacians", 
            "text": "For some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.", 
            "title": "Solving linear equations in Laplacians"
        }, 
        {
            "location": "/solvers/index.html#direct-solvers", 
            "text": "You can compute a cholesky factor directly with  cholfact .  It does  more than just compute the factor, and it saves its result in a data structure that implements  \\ .  It uses SuiteSparse by Tim Davis.  Here is an example of how you would use it to solve a general non-singular linear system.  a = grid2(5)\nla = lap(a)\nla[1,1] = la[1,1] + 1\nF = cholfact(la)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(la*x-b)\n\n    1.0598778281116327e-14  Laplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.  la = lap(a)\nf = lapWrapSolver(cholfact,la)\nb = randn(n); b = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15  Here are two other ways of using the wrapper:  lapChol = lapWrapSolver(cholfact)\nf = lapChol(la)\nb = randn(n);\nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.6924696662484416e-15\n\nx = lapWrapSolver(cholfact,la,b)\nnorm(la*x - b)\n    2.6924696662484416e-15", 
            "title": "Direct Solvers"
        }, 
        {
            "location": "/solvers/index.html#iterative-solvers", 
            "text": "The first, of course, is the Conjugate Gradient (cg).  Our implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance  tol  and the maximum number of iterations,  maxits .  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  Here are examples.  n = 50\na = randn(n,n); a = a * a';\nb = randn(n)\nx = cg(a,b,maxits=100)\nnorm(a*x - b)\n    1.2191649497921835e-6\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(a,bbig,maxits=100)\nnorm(a*xbig - bbig)\n    1.494919244242202629856363570306545126541716514824419323325986374186529786019681e-33  As a sanity check, we do two speed tests against Matlab.  la = lap(grid2(200))\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n@time x = cg(la,b,maxits=1000)\n    0.813791 seconds (2.77 k allocations: 211.550 MB, 3.56% gc time)\n\nnorm(la*x-b)\n    0.0001900620047823064  And, in Matlab:   a = grid2(200);  la = lap(a);  b = randn(length(a),1); b = b - mean(b);  tic; x = pcg(la,b,[],1000); toc\npcg converged at iteration 688 to a solution with relative residual 9.8e-07.\nElapsed time is 1.244917 seconds.  norm(la*x-b)\n\nans =\n\n   1.9730e-04  PCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.  a = mapweight(grid2(200),x- 1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\npre(x) = x ./ d\n@time x = pcg(la,b,pre,maxits=2000)\n    3.322035 seconds (42.21 k allocations: 1.194 GB, 5.11% gc time)\nnorm(la*x-b)\n    0.008508746034886803  If our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.  @time x = pcg(la,b,pre,tol=1e-1,maxits=10^5)\n    0.747042 seconds (9.65 k allocations: 275.819 MB, 4.87% gc time)\nnorm(la*x-b)\n    19.840756251253442\n\n@time x = cg(la,b,tol=1e-1,maxits=10^5)\n    6.509665 seconds (22.55 k allocations: 1.680 GB, 3.68% gc time)\nnorm(la*x-b)\n    19.222483530605043", 
            "title": "Iterative Solvers"
        }, 
        {
            "location": "/solvers/index.html#low-stretch-spanning-trees", 
            "text": "In order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code in Julia that is guaranteed to produce these.  Instead, for now, we have two routines that can be thought of as randomized versions of Prim and Kruskall's algorithm. randishKruskall  samples the remaining edges with probability proportional to their weight.   randishPrim  samples edges on the boundary while using the same rule.  Both use a data structure called  Sampler  that allows you to store integers with real values, and to sample according to those real values.  We also have code for computing the stretches.\nHere are some examples.  a = grid2(1000)\nt = randishKruskal(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    43.410262262262265\n\nt = randishPrim(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    33.14477077077077", 
            "title": "Low-Stretch Spanning Trees"
        }, 
        {
            "location": "/solvers/index.html#augmented-spanning-tree-preconditioners", 
            "text": "Here is code that will invoke one.\nIt is designed for positive definite systems.  So, let's give it one.\nRight now, it is using a randomized version of a MST.  There is no real reason to think that this should work.  a = mapweight(grid2(1000),x- 1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nla[1,1] = la[1,1] + 1\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000)\n    6.529052 seconds (4.00 M allocations: 1.858 GB, 15.34% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.058915 seconds (9.74 k allocations: 23.209 GB, 6.84% gc time)\n\nnorm(la*x - b)\n    99.74452367765869\n\n# Now, let's contrast with using CG\n\n@time y = cg(la,b,tol=1e-1,maxits=1000)\n    28.719631 seconds (4.01 k allocations: 7.473 GB, 3.74% gc time)\n\nnorm(la*y-b)\n    3243.6014713600766  That was not too impressive.  We will have to investigate.  By default, it presently uses randishKruskal.  Let's try randishPrim.  You can pass the treeAlg as a parameter.  @time F = augTreeSolver(la,tol=1e-1,maxits=1000,treeAlg=randishPrim);\n    6.319489 seconds (4.00 M allocations: 2.030 GB, 18.81% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.503484 seconds (9.76 k allocations: 23.268 GB, 7.31% gc time)\n\nnorm(la*x - b)\n    99.29610874176991  To solve systems in a Laplacian, we could wrap it.  n = 40000\nla = lap(randRegular(n,3))\nf = lapWrapSolver(augTreeSolver,la,tol=1e-6,maxits=1000)\nb = randn(n); b = b - mean(b)\nx = f(b)\nnorm(la*x-b)\n    0.00019304778073388  As you can see, lapWrapSolver can pass tol and maxits arguments to its solver, if they are given to it.", 
            "title": "Augmented spanning tree preconditioners"
        }, 
        {
            "location": "/Developing/index.html", 
            "text": "Developing Laplacians.jl\n\n\n\n\n\n\nDeveloping Laplacians.jl\n\n\nLearn to use git\n\n\nFast code?\n\n\nDocumentation\n\n\nParametric Types\n\n\nData structures:\n\n\nInterface issue:\n\n\nWriting tests:\n\n\n\n\n\n\nIntegration with other packages.\n\n\n\n\n\n\n\n\n\n\nLearn to use git\n\n\n\n\nIf you don't know anything about git, then just know that you should make a branch for you own code.  Type\n\n\n\n\ngit checkout -b MyName\n\n\n\n\n\n\n\n\nNow, read about Git.  I recommend the book\n\nPro Git\n, which is available online for free.\n\n\n\n\n\n\nStop thinking about Git like subversion or dropbox.\n\n\n\n\n\n\nThe master branch will be the one for public consumption. It should (mostly) work.\n\n\n\n\n\n\nYou should also read the \n\nsection of the Julia docs about building packages.\n\n\n\n\n\n\nFast code?\n\n\nJust go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.\n\n\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"\n\n\nDocumentation\n\n\nThis documentation is still very rough.\nIt is generated by a combination of Markdown and semi-automatic generation.  The steps to generate and improve it are:\n\n\n\n\nEdit Markdown files in the \ndocs\n directory.  For example, you could use MacDown to do this.\n\n\nIf you want to add a new page to the documention, create one.  Edit the file mkdocs.yml so show where it should appear.\n\n\nAdd docstrings to everything that needs it, and in particular to the routines you create.  The API is built from the docstrings.  To build the API, type\n\n\n\n\ninclude(\ndocs/build.jl\n)\n\n\n\n\nor run \njulia docs/build.jl\n from the root directory.\n\n\n\n\n\n\nRun \nmkdocs build\n in the root directory to regenerate the documentation from the Markdown.\n\n\n\n\n\n\nOnce you like the documentation, you can upload it with \n\n\n\n\n\n\nmkdocs gh-deploy --clean -b gh-pages\n\n\n\n\n\n\n\n\nWarning:\n mkdocs deletes everying in gh-pages that it does not put there itself.\n\n\n\n\n\n\nIf you create a Julia notebook that you would like to include as documentation.   You should\n   put it in the notebooks directory (.julia/v0.4/Laplacians/notebooks) and then link to it's page on GitHub.  While it seems that one should convert it to html (and one can), and then include it in MkDocs, MkDocs does something funny to the resulting html that does not look nice.\n\n\n\n\n\n\nParametric Types\n\n\nA sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\n\n\n\n\nTv\n, sometimes written \nTval\n denotes the types of the values, and \nTi\n or \nTind\n denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths, \nstart\n is of type \nTi\n.  Inside the code, whenever we write something like \npArray = zeros(Ti,n)\n, it creates an array of zeros of type Ti.  Using these parameteric types is \nmuch\n faster than leaving the types unfixed.\n\n\nData structures:\n\n\n\n\nIntHeap\n a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.\n\n\n\n\nInterface issue:\n\n\nThere are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.\n\n\nWriting tests:\n\n\nI haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.\n\n\nIntegration with other packages.\n\n\nThere are other graph packages that we might want to sometimes use.\n\n\n\n\nGraphs.jl\n : I found this one to be too slow and awkward to be useful.\n\n\nLightGraphs.jl\n : this looks more promising.  We will have to check it out.  It is reasonably fast, and the code looks pretty.", 
            "title": "Devleoping Laplacians"
        }, 
        {
            "location": "/Developing/index.html#developing-laplaciansjl", 
            "text": "Developing Laplacians.jl  Learn to use git  Fast code?  Documentation  Parametric Types  Data structures:  Interface issue:  Writing tests:    Integration with other packages.", 
            "title": "Developing Laplacians.jl"
        }, 
        {
            "location": "/Developing/index.html#learn-to-use-git", 
            "text": "If you don't know anything about git, then just know that you should make a branch for you own code.  Type   git checkout -b MyName    Now, read about Git.  I recommend the book Pro Git , which is available online for free.    Stop thinking about Git like subversion or dropbox.    The master branch will be the one for public consumption. It should (mostly) work.    You should also read the  section of the Julia docs about building packages.", 
            "title": "Learn to use git"
        }, 
        {
            "location": "/Developing/index.html#fast-code", 
            "text": "Just go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.  Within some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"", 
            "title": "Fast code?"
        }, 
        {
            "location": "/Developing/index.html#documentation", 
            "text": "This documentation is still very rough.\nIt is generated by a combination of Markdown and semi-automatic generation.  The steps to generate and improve it are:   Edit Markdown files in the  docs  directory.  For example, you could use MacDown to do this.  If you want to add a new page to the documention, create one.  Edit the file mkdocs.yml so show where it should appear.  Add docstrings to everything that needs it, and in particular to the routines you create.  The API is built from the docstrings.  To build the API, type   include( docs/build.jl )  or run  julia docs/build.jl  from the root directory.    Run  mkdocs build  in the root directory to regenerate the documentation from the Markdown.    Once you like the documentation, you can upload it with     mkdocs gh-deploy --clean -b gh-pages    Warning:  mkdocs deletes everying in gh-pages that it does not put there itself.    If you create a Julia notebook that you would like to include as documentation.   You should\n   put it in the notebooks directory (.julia/v0.4/Laplacians/notebooks) and then link to it's page on GitHub.  While it seems that one should convert it to html (and one can), and then include it in MkDocs, MkDocs does something funny to the resulting html that does not look nice.", 
            "title": "Documentation"
        }, 
        {
            "location": "/Developing/index.html#parametric-types", 
            "text": "A sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,  shortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)  Tv , sometimes written  Tval  denotes the types of the values, and  Ti  or  Tind  denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths,  start  is of type  Ti .  Inside the code, whenever we write something like  pArray = zeros(Ti,n) , it creates an array of zeros of type Ti.  Using these parameteric types is  much  faster than leaving the types unfixed.", 
            "title": "Parametric Types"
        }, 
        {
            "location": "/Developing/index.html#data-structures", 
            "text": "IntHeap  a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.", 
            "title": "Data structures:"
        }, 
        {
            "location": "/Developing/index.html#interface-issue", 
            "text": "There are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.", 
            "title": "Interface issue:"
        }, 
        {
            "location": "/Developing/index.html#writing-tests", 
            "text": "I haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.", 
            "title": "Writing tests:"
        }, 
        {
            "location": "/Developing/index.html#integration-with-other-packages", 
            "text": "There are other graph packages that we might want to sometimes use.   Graphs.jl  : I found this one to be too slow and awkward to be useful.  LightGraphs.jl  : this looks more promising.  We will have to check it out.  It is reasonably fast, and the code looks pretty.", 
            "title": "Integration with other packages."
        }, 
        {
            "location": "/wholeAPI/index.html", 
            "text": "ErdosRenyi\n\n\nGenerate a random graph on n vertices with m edges. The actual number of edges will probably be smaller, as we sample with replacement\n\n\nErdosRenyi(n::Integer, m::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:337\n\n\nErdosRenyiCluster\n\n\nGenerate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.\n\n\nErdosRenyiCluster(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:351\n\n\nErdosRenyiClusterFix\n\n\nLike an Erdos-Renyi cluster, but add back a tree so it has n vertices\n\n\nErdosRenyiClusterFix(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:364\n\n\nLaplacians\n\n\nA package for graph computations related to graph Laplacians\n\n\nGraphs are represented by sparse adjacency matrices, etc.\n\n\nRootedTree\n\n\nSummary:\n\n\ntype Laplacians.RootedTree{Tval,Tind} \n: Any\n\n\n\n\nFields:\n\n\nroot     :: Tind\nparent   :: Array{Tind,1}\nchildren :: Array{Tind,1}\nweights  :: Array{Tval,1}\nnumKids  :: Array{Tind,1}\nkidsPtr  :: Array{Tind,1}\n\n\n\n\nakpw\n\n\nThis is a wrapper for akpw!. It's slower, but won't modify the original graph. See akpw! documentation for more details.\n\n\nakpw(origMat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/akpwWeighted.jl:818\n\n\nakpw!\n\n\nConstructs a low stretch tree using the Alon, Karp, Peleg, West algorithm. This version (akpw! instead of akpw) modifies the graph slightly changing the edges weights, then changing them back, which may lead to floating point imprecisions. akpw! is faster (about 10-20%), but akpw doesn't have float imprecisions.\n\n\nThe function has a few options:\n\n\nkind: default is :max, which regards each edge weight as the inverse of its length (just like kruskal). If this is   set to anything else (e.g. :min), it will regard edge weight as length\n\n\nrandomClusters: default is false. This means the partition function searches for the beginning of the next cluster   in node order, rather than randomly choosing nodes. If this is set to false, it will   randomly choose the next node. This slows down akpw, but may produce better stretch.\n\n\nmetisClustering: default is false. If this is set to false, the graph will be partitioned   each time by metis, rather than by the akpw partitioning method.\n\n\nshuffleClusters: default is true. This preserves the \"reshuffleClusters\" method after each each graph is   partitioned into clusters. If set to false, the function will skip this step. May be faster   but have worse stretch.\n\n\nexponentialX: default is true, where the funciton exp(sqrt(log(nVertices) * log(log(nVertices)))) is used for X.   If set fo false, the function log(nVertices+1)/log(2) will be used for X instead. \n\n\nEXAMPLE:\n\n\n[2, 1]  =  0.631273 [3, 1]  =  0.40103 [1, 2]  =  0.631273 [4, 2]  =  0.147018 [1, 3]  =  0.40103 [4, 3]  =  0.772661 [2, 4]  =  0.147018 [3, 4]  =  0.772661\n\n\n  |\n  |\n  V\n\n\n\n\n[2, 1]  =  0.631273 [3, 1]  =  0.40103 [1, 2]  =  0.631273 [1, 3]  =  0.40103 [4, 3]  =  0.772661 [3, 4]  =  0.772661\n\n\nakpw!(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/akpwWeighted.jl:733\n\n\napr\n\n\nComputes an approximate page rank vector from a starting set s, an alpha and an epsilon The algorithm follows the Anderson,Chung,Lang paper and Dan Spielman's lecture notes\n\n\napr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, alpha::Float64, eps::Float64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/localClustering.jl:440\n\n\naugTreePrecon\n\n\nThis is an augmented spanning tree preconditioner for diagonally dominant linear systems.  It takes as optional input a tree growing algorithm. The default is a randomized variant of Kruskal. It adds back 2sqrt(n) edges via augmentTree. With the right tree, it should never be too bad.\n\n\naugTreePrecon{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:188\n\n\naugTreeSolver\n\n\nThis is the solver that calls augTreePrecon\n\n\naugTreeSolver{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:210\n\n\naugmentTree\n\n\nTakes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It then adds back the k edges of highest stretch, and k edges sampled according to stretch\n\n\naugmentTree{Tv,Ti}(tree::SparseMatrixCSC{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti}, k::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:138\n\n\nbackIndices\n\n\nSame as the above, but now the graph is in adjacency list form \n\n\nComputes the back indices in a graph in O(M+N). works if for every edge (u,v), (v,u) is also in the graph \n\n\nbackIndices{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\nbackIndices{Tv1,Tv2}(G::Array{Array{Tuple{Tv1,Tv2},1},1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:35\n\n\nbiggestComp\n\n\nReturn the biggest component in a graph, as a graph\n\n\nbiggestComp(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:159\n\n\ncg\n\n\ncg(mat, b::Array{Float64,1})\ncg(mat, b::Array{Float32,1})\ncg(mat, b)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/pcg.jl:29\n\n\nchimera\n\n\nBuilds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nBuilds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace\n\n\nchimera(n::Integer)\nchimera(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:515\n\n\ncompConductance\n\n\nReturns the quality of the cut for a given graph and a given cut set s.   the result will be |outgoing edges| / min(|vertices in set|, |N - vertices in set|)\n\n\ncompConductance{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:136\n\n\ncompDepth\n\n\ncompDepth{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:311\n\n\ncompStretches\n\n\nCompute the stretched of every edge in \nmat\n with respect to the tree \ntree\n. Returns the answer as a sparse matrix with the same nonzero structure as \nmat\n. Assumes that \nmat\n is symmetric. \ntree\n should be the adjacency matrix of a spanning tree.\n\n\ncompStretches{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti})\ncompStretches{Tv,Ti}(tree::SparseMatrixCSC{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:393\n\n\ncompleteBinaryTree\n\n\nThe complete binary tree on n vertices\n\n\ncompleteBinaryTree(n::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:139\n\n\ncompleteGraph\n\n\nThe complete graph\n\n\ncompleteGraph(n::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:17\n\n\ncomponents\n\n\nComputes the connected components of a graph. Returns them as a vector of length equal to the number of vertices. The vector numbers the components from 1 through the maximum number. For example,\n\n\ngr = ErdosRenyi(10,11)\nc = components(gr)\n\n10-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 1\n 1\n 1\n 3\n 2\n\n\n\n\ncomponents{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:65\n\n\ndeg\n\n\ndeg{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:11\n\n\ndiagmat\n\n\nReturns the diagonal matrix(as a sparse matrix) of a graph\n\n\ndiagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:205\n\n\ndirEdgeVertexMat\n\n\nThe signed edge-vertex adjacency matrix\n\n\ndirEdgeVertexMat(A::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/toposort.jl:49\n\n\ndumb\n\n\nModify a cluster by passing through all the vertices exactly once and \nadding/removing them based on the value of (Deg_external - Deg_Internal).\n\n\n\n\ndumb{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/cutHeuristics.jl:106\n\n\nedgeVertexMat\n\n\nThe signed edge-vertex adjacency matrix\n\n\nedgeVertexMat(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:67\n\n\nfindEntries\n\n\nSimilar to findnz, but also returns 0 entries that have an edge in the sparse matrix \n\n\nfindEntries{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:114\n\n\nfloatGraph\n\n\nConvert the nonzero entries in a graph to Float64\n\n\nfloatGraph(a::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:6\n\n\ngeneralizedNecklace\n\n\nConstructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.\n\n\ngeneralizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti\n:Integer}, k::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:225\n\n\ngeneralizedRing\n\n\nA generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example, \n\n\ngeneralizedRing(17, [1 5])\n\n\n\n\ngeneralizedRing(n::Int64, gens)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:38\n\n\ngetObound\n\n\nComputes the number of edges leaving s \n\n\ngetObound{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:167\n\n\ngetVolume\n\n\nComputes the volume of subset s in an unweighted graph G \n\n\ngetVolume{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:149\n\n\ngrid2\n\n\nAn n-by-m grid graph.  iostropy is the weighting on edges in one direction.\n\n\ngrid2(n::Int64)\ngrid2(n::Int64, m::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:159\n\n\ngrid2coords\n\n\nCoordinates for plotting the vertices of the n-by-m grid graph\n\n\ngrid2coords(n::Int64, m::Int64)\ngrid2coords(n)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:163\n\n\ngrownGraph\n\n\nCreate a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.    \n\n\ngrownGraph(n::Int64, k::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:202\n\n\ngrownGraphD\n\n\nLike a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices\n\n\ngrownGraphD(n::Int64, k::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:234\n\n\nhyperCube\n\n\nThe d dimensional hypercube.  Has 2^d vertices\n\n\nhyperCube(d::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:125\n\n\nisConnected\n\n\nReturns true if graph is connected.  Calls components.\n\n\nisConnected(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:113\n\n\njoinGraphs\n\n\nCreate a disjoint union of graphs a and b,  and then put k random edges between them\n\n\njoinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:111\n\n\nkruskal\n\n\nUses Kruskal's algorithm to compute a minimum (or maximum) spanning tree. Set kind=:max if you want the max spanning tree. It returns it a a graph\n\n\nkruskal{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:407\n\n\nlap\n\n\nCreate a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry\n\n\nlap(a)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:12\n\n\nlapChol\n\n\nlapWrapSolver\n\n\nTakes a solver for solving nonsingular sdd systems, and returns a solver for solving Laplacian systems. The optional args tol and maxits are not necessarily taken by all solvers.  But, if they are, one can pass them here\n\n\nlapWrapSolver(solver)\nlapWrapSolver(solver, la::AbstractArray{T,N})\nlapWrapSolver(solver, la::AbstractArray{T,N}, b)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:108\n\n\nlocalImprove\n\n\nlocalImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1}; epsSigma=-1.0, err=1e-10, maxSize = max(G.n, G.m)\n\n\nThe LocalImprove function, from the Orrechia-Zhu paper. Given a graph and an initial set, finds a set of smaller conductance based on the starting set using a localized version of max-flow.\n\n\nSmall discussion: When adding in the neighbors of the initial component, if the resulting  conductance is worse than the initial one,  the algorithm will add more and more vertices until hitting a better conductance. However, if we fix a certain  maximum size for our component,  it might be the case that this new conductance will always be worse than what we had initially. Thus, if we run the algorithm with a small maxSize,  our initial conductance might be the best solution we can reach.\n\n\n\n\nG is the given graph, A is the initial set \n\n\nepsSigma is a measure of the quality of the returning set (the smaller the better). It's defaulted to volume(A) / volume(VA)\n\n\nerr is the numerical error considered throughout the algorithm. It's defaulted to 1e-10\n\n\nmaxSize is the maximum allowed size for the flow graph at any iteration of the algorithm. It's defaulted to |V|\n\n\n\n\nlocalImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/localClustering.jl:22\n\n\nmapweight\n\n\nCreate a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write\n\n\nb = mapweight(a, x-\nrand(1)[1])\n\n\n\n\nmapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:40\n\n\nmatToTree\n\n\nmatToTree{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\nmatToTree{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, root::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:32\n\n\nmatToTreeDepth\n\n\nmatToTreeDepth{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\nmatToTreeDepth{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, root::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:98\n\n\nmaxflow\n\n\nimplementation of Dinic's algorithm. computes the maximum flow and min-cut in G between s and t    we consider the adjacency matrix to be the capacity matrix \n\n\nmaxflow{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Int64, t::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/flow.jl:7\n\n\nnbri\n\n\nnbri{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:12\n\n\nnbrs\n\n\nnbrs{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:14\n\n\npathFromParents\n\n\npathFromParents(parents, y)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:215\n\n\npathGraph\n\n\nThe path graph on n vertices\n\n\npathGraph(n::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:8\n\n\npcg\n\n\npcg(mat, b::Array{Float64,1}, pre)\npcg(mat, b::Array{Float32,1}, pre)\npcg(mat, b, pre)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/pcg.jl:42\n\n\nplotGraph\n\n\nPlots graph gr with coordinates (x,y)\n\n\nplotGraph(gr, x, y)\nplotGraph(gr, x, y, color)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:129\n\n\nprefAttach\n\n\nA preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.\n\n\nprefAttach(n::Int64, k::Int64, p::Float64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:258\n\n\nprim\n\n\nprim(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:438\n\n\nprn\n\n\nprn{Tv, Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)\n\n\nThe PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper\n\n\ns is a set of starting vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]\n\n\nphi is a bound on the quality of the conductance of the cut - the smaller the phi, the higher the quality.  b is used to handle precision throughout the algorithm - the higher the b, the greater the precision.\n\n\nprn{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/localClustering.jl:374\n\n\nproductGraph\n\n\nThe Cartesian product of two graphs.  When applied to two paths, it gives a grid.\n\n\nproductGraph(a0::SparseMatrixCSC{Tv,Ti\n:Integer}, a1::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:58\n\n\npureRandomGraph\n\n\nGenerate a random graph with n vertices from one of our natural distributions\n\n\npureRandomGraph(n::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:379\n\n\nrandGenRing\n\n\nA random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution\n\n\nrandGenRing(n::Int64, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:62\n\n\nrandMatching\n\n\nA random matching on n vertices\n\n\nrandMatching(n::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:174\n\n\nrandRegular\n\n\nA sum of k random matchings on n vertices\n\n\nrandRegular(n::Int64, k::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:187\n\n\nrandWeight\n\n\nApplies one of a number of random weighting schemes to the edges of the graph\n\n\nrandWeight(a)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:535\n\n\nrandishKruskal\n\n\nrandishKruskal{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/randTrees.jl:10\n\n\nrandishPrim\n\n\nrandishPrim{Tval,Tind}(mat::SparseMatrixCSC{Tval,Tind})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/randTrees.jl:47\n\n\nrandperm\n\n\n..  randperm([rng,] n)\n\nConstruct a random permutation of length ``n``. The optional ``rng`` argument\nspecifies a random number generator, see :ref:`Random Numbers \nrandom-numbers\n`.\n\n\n\n\nRandomly permutes the vertex indices\n\n\nrandperm(r::AbstractRNG, n::Integer)\nrandperm(n::Integer)\nrandperm(mat::AbstractArray{T,2})\nrandperm(f::Expr)\n\n\n\n\nat random.jl:1341\n\n\nreadIJ\n\n\nTo read a simple edge list, each line being an (i, j) pair\n\n\nreadIJ(filename::AbstractString)\nreadIJ(filename::AbstractString, sep)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/IO.jl:4\n\n\nreadIJV\n\n\nTo read a simple edge list, each line being an (i, j, v) pair. The parens should not be there in the format, just commas separating. To generate this format in Matlab, you just need to be careful to write the vertex indices with sufficient precision.  For example, you can do this\n\n\n [ai,aj,av] = find(triu(a));\n\n dlmwrite('graph.txt',[ai,aj,av],'precision',9);\n\n\n\n\nreadIJV(filename::AbstractString)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/IO.jl:25\n\n\nrefineCut\n\n\nModifies a cluster by adding or removing vertices by picking at each step \nthe vertex that has the maximum value of (Deg_external - Deg_Internal).\nEach vertex can be added in/removed only once.\n\n\n\n\nrefineCut{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/cutHeuristics.jl:9\n\n\nringGraph\n\n\nThe simple ring on n vertices\n\n\nringGraph(n::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:23\n\n\nsemiWtedChimera\n\n\nA Chimera graph with some weights.  The weights just appear when graphs are combined. For more interesting weights, use \nwtedChimera\n\n\nsemiWtedChimera(n::Integer)\nsemiWtedChimera(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:442\n\n\nsetValue\n\n\nSets the value of a certain edge in a sparse graph; value can be 0 without the edges dissapearing \n\n\nsetValue{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti, a::Tv)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:29\n\n\nshortIntGraph\n\n\nConvert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much\n\n\nshortIntGraph(a::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:3\n\n\nshortestPathTree\n\n\nComputes the shortest path tree, and returns it as a sparse matrix. Treats edge weights as reciprocals of lengths. For example:\n\n\na = [0 2 1; 2 0 3; 1 3 0]\ntr = full(shortestPathTree(sparse(a),1))\n\n3x3 Array{Float64,2}:\n 0.0  2.0  0.0\n 2.0  0.0  3.0\n 0.0  3.0  0.0\n\n\n\n\nshortestPathTree(a, start)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:239\n\n\nshortestPaths\n\n\nComputes the lenghts of shortest paths from \nstart\n. Returns both a vector of the lenghts, and the parent array in the shortest path tree.\n\n\nThis algorithm treats edge weights as reciprocals of distances. DOC BETTER\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:175\n\n\nspectralCoords\n\n\nComputes the spectral coordinates of a graph\n\n\nspectralCoords(a)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:174\n\n\nspectralDrawing\n\n\nComputes spectral coordinates, and then uses plotGraph to draw\n\n\nspectralDrawing(a)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:166\n\n\nsubsampleEdges\n\n\nCreate a new graph from the old, but keeping edge edge with probability \np\n\n\nsubsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:76\n\n\ntarjanStretch\n\n\ntarjanStretch{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti}, depth::Array{Tv,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:334\n\n\ntoUnitVector\n\n\nCreates a unit vector of length n from a given set of integers, with weights based on the number of occurences\n\n\ntoUnitVector(a::Array{Int64,1}, n)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:183\n\n\ntoposort\n\n\ntoposort{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/toposort.jl:13\n\n\ntwoLift\n\n\nCreats a 2-lift of a.  \nflip\n is a boolean indicating which edges cross\n\n\ntwoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:99\n\n\nuniformWeight\n\n\nPut a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.\n\n\nuniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:49\n\n\nuniformWeight!\n\n\nSet the weight of every edge to 1\n\n\nuniformWeight!(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:53\n\n\nunweight\n\n\nCreate a new graph in that is the same as the original, but with all edge weights 1\n\n\nunweight{Tval,Tind}(ain::SparseMatrixCSC{Tval,Tind})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:16\n\n\nunweight!\n\n\nChange the weight of every edge in a to 1\n\n\nunweight!{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:26\n\n\nvecToComps\n\n\nThis turns a component vector, like that generated by components, into an array of arrays of indices of vertices in each component.  For example,\n\n\ncomps = vecToComps(c)\n\n3-element Array{Array{Int64,1},1}:\n [1,2,3,4,6,7,8]\n [5,10]\n [9]\n\n\n\n\nvecToComps{Ti}(compvec::Array{Ti,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:136\n\n\nwdeg\n\n\nFinds the weighted degree of a vertex in the graph \n\n\nwdeg{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:19\n\n\nweighti\n\n\nweighti{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:13\n\n\nwriteIJV\n\n\nWrites the upper portion of a matrix in ijv format, one row for each edge, separated by commas.  Only writes the upper triangular portion. The result can be read from Matlab like this:\n\n\n dl = dlmread('graph.txt');\n\n a = sparse(dl(:,1),dl(:,2),dl(:,3));\n\n n = max(size(a))\n\n a(n,n) = 0;\n\n a = a + a';\n\n\n\n\nwriteIJV(filename::AbstractString, mat)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/IO.jl:52\n\n\nwtedChimera\n\n\nBuilds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nGenerate a chimera, and then apply a random weighting scheme\n\n\nwtedChimera(n::Integer)\nwtedChimera(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:607", 
            "title": "Whole API"
        }, 
        {
            "location": "/wholeAPI/index.html#erdosrenyi", 
            "text": "Generate a random graph on n vertices with m edges. The actual number of edges will probably be smaller, as we sample with replacement  ErdosRenyi(n::Integer, m::Integer)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:337", 
            "title": "ErdosRenyi"
        }, 
        {
            "location": "/wholeAPI/index.html#erdosrenyicluster", 
            "text": "Generate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.  ErdosRenyiCluster(n::Integer, k::Integer)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:351", 
            "title": "ErdosRenyiCluster"
        }, 
        {
            "location": "/wholeAPI/index.html#erdosrenyiclusterfix", 
            "text": "Like an Erdos-Renyi cluster, but add back a tree so it has n vertices  ErdosRenyiClusterFix(n::Integer, k::Integer)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:364", 
            "title": "ErdosRenyiClusterFix"
        }, 
        {
            "location": "/wholeAPI/index.html#laplacians", 
            "text": "A package for graph computations related to graph Laplacians  Graphs are represented by sparse adjacency matrices, etc.", 
            "title": "Laplacians"
        }, 
        {
            "location": "/wholeAPI/index.html#rootedtree", 
            "text": "Summary:  type Laplacians.RootedTree{Tval,Tind}  : Any  Fields:  root     :: Tind\nparent   :: Array{Tind,1}\nchildren :: Array{Tind,1}\nweights  :: Array{Tval,1}\nnumKids  :: Array{Tind,1}\nkidsPtr  :: Array{Tind,1}", 
            "title": "RootedTree"
        }, 
        {
            "location": "/wholeAPI/index.html#akpw", 
            "text": "This is a wrapper for akpw!. It's slower, but won't modify the original graph. See akpw! documentation for more details.  akpw(origMat::SparseMatrixCSC{Tv,Ti :Integer})  at /Users/spielman/.julia/v0.4/Laplacians/src/akpwWeighted.jl:818", 
            "title": "akpw"
        }, 
        {
            "location": "/wholeAPI/index.html#akpw_1", 
            "text": "Constructs a low stretch tree using the Alon, Karp, Peleg, West algorithm. This version (akpw! instead of akpw) modifies the graph slightly changing the edges weights, then changing them back, which may lead to floating point imprecisions. akpw! is faster (about 10-20%), but akpw doesn't have float imprecisions.  The function has a few options:  kind: default is :max, which regards each edge weight as the inverse of its length (just like kruskal). If this is   set to anything else (e.g. :min), it will regard edge weight as length  randomClusters: default is false. This means the partition function searches for the beginning of the next cluster   in node order, rather than randomly choosing nodes. If this is set to false, it will   randomly choose the next node. This slows down akpw, but may produce better stretch.  metisClustering: default is false. If this is set to false, the graph will be partitioned   each time by metis, rather than by the akpw partitioning method.  shuffleClusters: default is true. This preserves the \"reshuffleClusters\" method after each each graph is   partitioned into clusters. If set to false, the function will skip this step. May be faster   but have worse stretch.  exponentialX: default is true, where the funciton exp(sqrt(log(nVertices) * log(log(nVertices)))) is used for X.   If set fo false, the function log(nVertices+1)/log(2) will be used for X instead.   EXAMPLE:  [2, 1]  =  0.631273 [3, 1]  =  0.40103 [1, 2]  =  0.631273 [4, 2]  =  0.147018 [1, 3]  =  0.40103 [4, 3]  =  0.772661 [2, 4]  =  0.147018 [3, 4]  =  0.772661    |\n  |\n  V  [2, 1]  =  0.631273 [3, 1]  =  0.40103 [1, 2]  =  0.631273 [1, 3]  =  0.40103 [4, 3]  =  0.772661 [3, 4]  =  0.772661  akpw!(mat::SparseMatrixCSC{Tv,Ti :Integer})  at /Users/spielman/.julia/v0.4/Laplacians/src/akpwWeighted.jl:733", 
            "title": "akpw!"
        }, 
        {
            "location": "/wholeAPI/index.html#apr", 
            "text": "Computes an approximate page rank vector from a starting set s, an alpha and an epsilon The algorithm follows the Anderson,Chung,Lang paper and Dan Spielman's lecture notes  apr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, alpha::Float64, eps::Float64)  at /Users/spielman/.julia/v0.4/Laplacians/src/localClustering.jl:440", 
            "title": "apr"
        }, 
        {
            "location": "/wholeAPI/index.html#augtreeprecon", 
            "text": "This is an augmented spanning tree preconditioner for diagonally dominant linear systems.  It takes as optional input a tree growing algorithm. The default is a randomized variant of Kruskal. It adds back 2sqrt(n) edges via augmentTree. With the right tree, it should never be too bad.  augTreePrecon{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})  at /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:188", 
            "title": "augTreePrecon"
        }, 
        {
            "location": "/wholeAPI/index.html#augtreesolver", 
            "text": "This is the solver that calls augTreePrecon  augTreeSolver{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})  at /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:210", 
            "title": "augTreeSolver"
        }, 
        {
            "location": "/wholeAPI/index.html#augmenttree", 
            "text": "Takes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It then adds back the k edges of highest stretch, and k edges sampled according to stretch  augmentTree{Tv,Ti}(tree::SparseMatrixCSC{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti}, k::Ti)  at /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:138", 
            "title": "augmentTree"
        }, 
        {
            "location": "/wholeAPI/index.html#backindices", 
            "text": "Same as the above, but now the graph is in adjacency list form   Computes the back indices in a graph in O(M+N). works if for every edge (u,v), (v,u) is also in the graph   backIndices{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\nbackIndices{Tv1,Tv2}(G::Array{Array{Tuple{Tv1,Tv2},1},1})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:35", 
            "title": "backIndices"
        }, 
        {
            "location": "/wholeAPI/index.html#biggestcomp", 
            "text": "Return the biggest component in a graph, as a graph  biggestComp(mat::SparseMatrixCSC{Tv,Ti :Integer})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:159", 
            "title": "biggestComp"
        }, 
        {
            "location": "/wholeAPI/index.html#cg", 
            "text": "cg(mat, b::Array{Float64,1})\ncg(mat, b::Array{Float32,1})\ncg(mat, b)  at /Users/spielman/.julia/v0.4/Laplacians/src/pcg.jl:29", 
            "title": "cg"
        }, 
        {
            "location": "/wholeAPI/index.html#chimera", 
            "text": "Builds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  Builds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace  chimera(n::Integer)\nchimera(n::Integer, k::Integer)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:515", 
            "title": "chimera"
        }, 
        {
            "location": "/wholeAPI/index.html#compconductance", 
            "text": "Returns the quality of the cut for a given graph and a given cut set s.   the result will be |outgoing edges| / min(|vertices in set|, |N - vertices in set|)  compConductance{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:136", 
            "title": "compConductance"
        }, 
        {
            "location": "/wholeAPI/index.html#compdepth", 
            "text": "compDepth{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti})  at /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:311", 
            "title": "compDepth"
        }, 
        {
            "location": "/wholeAPI/index.html#compstretches", 
            "text": "Compute the stretched of every edge in  mat  with respect to the tree  tree . Returns the answer as a sparse matrix with the same nonzero structure as  mat . Assumes that  mat  is symmetric.  tree  should be the adjacency matrix of a spanning tree.  compStretches{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti})\ncompStretches{Tv,Ti}(tree::SparseMatrixCSC{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti})  at /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:393", 
            "title": "compStretches"
        }, 
        {
            "location": "/wholeAPI/index.html#completebinarytree", 
            "text": "The complete binary tree on n vertices  completeBinaryTree(n::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:139", 
            "title": "completeBinaryTree"
        }, 
        {
            "location": "/wholeAPI/index.html#completegraph", 
            "text": "The complete graph  completeGraph(n::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:17", 
            "title": "completeGraph"
        }, 
        {
            "location": "/wholeAPI/index.html#components", 
            "text": "Computes the connected components of a graph. Returns them as a vector of length equal to the number of vertices. The vector numbers the components from 1 through the maximum number. For example,  gr = ErdosRenyi(10,11)\nc = components(gr)\n\n10-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 1\n 1\n 1\n 3\n 2  components{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:65", 
            "title": "components"
        }, 
        {
            "location": "/wholeAPI/index.html#deg", 
            "text": "deg{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:11", 
            "title": "deg"
        }, 
        {
            "location": "/wholeAPI/index.html#diagmat", 
            "text": "Returns the diagonal matrix(as a sparse matrix) of a graph  diagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:205", 
            "title": "diagmat"
        }, 
        {
            "location": "/wholeAPI/index.html#diredgevertexmat", 
            "text": "The signed edge-vertex adjacency matrix  dirEdgeVertexMat(A::SparseMatrixCSC{Tv,Ti :Integer})  at /Users/spielman/.julia/v0.4/Laplacians/src/toposort.jl:49", 
            "title": "dirEdgeVertexMat"
        }, 
        {
            "location": "/wholeAPI/index.html#dumb", 
            "text": "Modify a cluster by passing through all the vertices exactly once and \nadding/removing them based on the value of (Deg_external - Deg_Internal).  dumb{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})  at /Users/spielman/.julia/v0.4/Laplacians/src/cutHeuristics.jl:106", 
            "title": "dumb"
        }, 
        {
            "location": "/wholeAPI/index.html#edgevertexmat", 
            "text": "The signed edge-vertex adjacency matrix  edgeVertexMat(mat::SparseMatrixCSC{Tv,Ti :Integer})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:67", 
            "title": "edgeVertexMat"
        }, 
        {
            "location": "/wholeAPI/index.html#findentries", 
            "text": "Similar to findnz, but also returns 0 entries that have an edge in the sparse matrix   findEntries{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:114", 
            "title": "findEntries"
        }, 
        {
            "location": "/wholeAPI/index.html#floatgraph", 
            "text": "Convert the nonzero entries in a graph to Float64  floatGraph(a::SparseMatrixCSC{Tv,Ti :Integer})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:6", 
            "title": "floatGraph"
        }, 
        {
            "location": "/wholeAPI/index.html#generalizednecklace", 
            "text": "Constructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.  generalizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti :Integer}, k::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:225", 
            "title": "generalizedNecklace"
        }, 
        {
            "location": "/wholeAPI/index.html#generalizedring", 
            "text": "A generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example,   generalizedRing(17, [1 5])  generalizedRing(n::Int64, gens)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:38", 
            "title": "generalizedRing"
        }, 
        {
            "location": "/wholeAPI/index.html#getobound", 
            "text": "Computes the number of edges leaving s   getObound{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:167", 
            "title": "getObound"
        }, 
        {
            "location": "/wholeAPI/index.html#getvolume", 
            "text": "Computes the volume of subset s in an unweighted graph G   getVolume{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:149", 
            "title": "getVolume"
        }, 
        {
            "location": "/wholeAPI/index.html#grid2", 
            "text": "An n-by-m grid graph.  iostropy is the weighting on edges in one direction.  grid2(n::Int64)\ngrid2(n::Int64, m::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:159", 
            "title": "grid2"
        }, 
        {
            "location": "/wholeAPI/index.html#grid2coords", 
            "text": "Coordinates for plotting the vertices of the n-by-m grid graph  grid2coords(n::Int64, m::Int64)\ngrid2coords(n)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:163", 
            "title": "grid2coords"
        }, 
        {
            "location": "/wholeAPI/index.html#growngraph", 
            "text": "Create a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.      grownGraph(n::Int64, k::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:202", 
            "title": "grownGraph"
        }, 
        {
            "location": "/wholeAPI/index.html#growngraphd", 
            "text": "Like a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices  grownGraphD(n::Int64, k::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:234", 
            "title": "grownGraphD"
        }, 
        {
            "location": "/wholeAPI/index.html#hypercube", 
            "text": "The d dimensional hypercube.  Has 2^d vertices  hyperCube(d::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:125", 
            "title": "hyperCube"
        }, 
        {
            "location": "/wholeAPI/index.html#isconnected", 
            "text": "Returns true if graph is connected.  Calls components.  isConnected(mat::SparseMatrixCSC{Tv,Ti :Integer})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:113", 
            "title": "isConnected"
        }, 
        {
            "location": "/wholeAPI/index.html#joingraphs", 
            "text": "Create a disjoint union of graphs a and b,  and then put k random edges between them  joinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:111", 
            "title": "joinGraphs"
        }, 
        {
            "location": "/wholeAPI/index.html#kruskal", 
            "text": "Uses Kruskal's algorithm to compute a minimum (or maximum) spanning tree. Set kind=:max if you want the max spanning tree. It returns it a a graph  kruskal{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:407", 
            "title": "kruskal"
        }, 
        {
            "location": "/wholeAPI/index.html#lap", 
            "text": "Create a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry  lap(a)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:12", 
            "title": "lap"
        }, 
        {
            "location": "/wholeAPI/index.html#lapchol", 
            "text": "", 
            "title": "lapChol"
        }, 
        {
            "location": "/wholeAPI/index.html#lapwrapsolver", 
            "text": "Takes a solver for solving nonsingular sdd systems, and returns a solver for solving Laplacian systems. The optional args tol and maxits are not necessarily taken by all solvers.  But, if they are, one can pass them here  lapWrapSolver(solver)\nlapWrapSolver(solver, la::AbstractArray{T,N})\nlapWrapSolver(solver, la::AbstractArray{T,N}, b)  at /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:108", 
            "title": "lapWrapSolver"
        }, 
        {
            "location": "/wholeAPI/index.html#localimprove", 
            "text": "localImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1}; epsSigma=-1.0, err=1e-10, maxSize = max(G.n, G.m)  The LocalImprove function, from the Orrechia-Zhu paper. Given a graph and an initial set, finds a set of smaller conductance based on the starting set using a localized version of max-flow.  Small discussion: When adding in the neighbors of the initial component, if the resulting  conductance is worse than the initial one,  the algorithm will add more and more vertices until hitting a better conductance. However, if we fix a certain  maximum size for our component,  it might be the case that this new conductance will always be worse than what we had initially. Thus, if we run the algorithm with a small maxSize,  our initial conductance might be the best solution we can reach.   G is the given graph, A is the initial set   epsSigma is a measure of the quality of the returning set (the smaller the better). It's defaulted to volume(A) / volume(VA)  err is the numerical error considered throughout the algorithm. It's defaulted to 1e-10  maxSize is the maximum allowed size for the flow graph at any iteration of the algorithm. It's defaulted to |V|   localImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1})  at /Users/spielman/.julia/v0.4/Laplacians/src/localClustering.jl:22", 
            "title": "localImprove"
        }, 
        {
            "location": "/wholeAPI/index.html#mapweight", 
            "text": "Create a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write  b = mapweight(a, x- rand(1)[1])  mapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:40", 
            "title": "mapweight"
        }, 
        {
            "location": "/wholeAPI/index.html#mattotree", 
            "text": "matToTree{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\nmatToTree{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, root::Ti)  at /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:32", 
            "title": "matToTree"
        }, 
        {
            "location": "/wholeAPI/index.html#mattotreedepth", 
            "text": "matToTreeDepth{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\nmatToTreeDepth{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, root::Ti)  at /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:98", 
            "title": "matToTreeDepth"
        }, 
        {
            "location": "/wholeAPI/index.html#maxflow", 
            "text": "implementation of Dinic's algorithm. computes the maximum flow and min-cut in G between s and t    we consider the adjacency matrix to be the capacity matrix   maxflow{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Int64, t::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/flow.jl:7", 
            "title": "maxflow"
        }, 
        {
            "location": "/wholeAPI/index.html#nbri", 
            "text": "nbri{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:12", 
            "title": "nbri"
        }, 
        {
            "location": "/wholeAPI/index.html#nbrs", 
            "text": "nbrs{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:14", 
            "title": "nbrs"
        }, 
        {
            "location": "/wholeAPI/index.html#pathfromparents", 
            "text": "pathFromParents(parents, y)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:215", 
            "title": "pathFromParents"
        }, 
        {
            "location": "/wholeAPI/index.html#pathgraph", 
            "text": "The path graph on n vertices  pathGraph(n::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:8", 
            "title": "pathGraph"
        }, 
        {
            "location": "/wholeAPI/index.html#pcg", 
            "text": "pcg(mat, b::Array{Float64,1}, pre)\npcg(mat, b::Array{Float32,1}, pre)\npcg(mat, b, pre)  at /Users/spielman/.julia/v0.4/Laplacians/src/pcg.jl:42", 
            "title": "pcg"
        }, 
        {
            "location": "/wholeAPI/index.html#plotgraph", 
            "text": "Plots graph gr with coordinates (x,y)  plotGraph(gr, x, y)\nplotGraph(gr, x, y, color)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:129", 
            "title": "plotGraph"
        }, 
        {
            "location": "/wholeAPI/index.html#prefattach", 
            "text": "A preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.  prefAttach(n::Int64, k::Int64, p::Float64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:258", 
            "title": "prefAttach"
        }, 
        {
            "location": "/wholeAPI/index.html#prim", 
            "text": "prim(mat::SparseMatrixCSC{Tv,Ti :Integer})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:438", 
            "title": "prim"
        }, 
        {
            "location": "/wholeAPI/index.html#prn", 
            "text": "prn{Tv, Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)  The PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper  s is a set of starting vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]  phi is a bound on the quality of the conductance of the cut - the smaller the phi, the higher the quality.  b is used to handle precision throughout the algorithm - the higher the b, the greater the precision.  prn{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/localClustering.jl:374", 
            "title": "prn"
        }, 
        {
            "location": "/wholeAPI/index.html#productgraph", 
            "text": "The Cartesian product of two graphs.  When applied to two paths, it gives a grid.  productGraph(a0::SparseMatrixCSC{Tv,Ti :Integer}, a1::SparseMatrixCSC{Tv,Ti :Integer})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:58", 
            "title": "productGraph"
        }, 
        {
            "location": "/wholeAPI/index.html#purerandomgraph", 
            "text": "Generate a random graph with n vertices from one of our natural distributions  pureRandomGraph(n::Integer)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:379", 
            "title": "pureRandomGraph"
        }, 
        {
            "location": "/wholeAPI/index.html#randgenring", 
            "text": "A random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution  randGenRing(n::Int64, k::Integer)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:62", 
            "title": "randGenRing"
        }, 
        {
            "location": "/wholeAPI/index.html#randmatching", 
            "text": "A random matching on n vertices  randMatching(n::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:174", 
            "title": "randMatching"
        }, 
        {
            "location": "/wholeAPI/index.html#randregular", 
            "text": "A sum of k random matchings on n vertices  randRegular(n::Int64, k::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:187", 
            "title": "randRegular"
        }, 
        {
            "location": "/wholeAPI/index.html#randweight", 
            "text": "Applies one of a number of random weighting schemes to the edges of the graph  randWeight(a)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:535", 
            "title": "randWeight"
        }, 
        {
            "location": "/wholeAPI/index.html#randishkruskal", 
            "text": "randishKruskal{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})  at /Users/spielman/.julia/v0.4/Laplacians/src/randTrees.jl:10", 
            "title": "randishKruskal"
        }, 
        {
            "location": "/wholeAPI/index.html#randishprim", 
            "text": "randishPrim{Tval,Tind}(mat::SparseMatrixCSC{Tval,Tind})  at /Users/spielman/.julia/v0.4/Laplacians/src/randTrees.jl:47", 
            "title": "randishPrim"
        }, 
        {
            "location": "/wholeAPI/index.html#randperm", 
            "text": "..  randperm([rng,] n)\n\nConstruct a random permutation of length ``n``. The optional ``rng`` argument\nspecifies a random number generator, see :ref:`Random Numbers  random-numbers `.  Randomly permutes the vertex indices  randperm(r::AbstractRNG, n::Integer)\nrandperm(n::Integer)\nrandperm(mat::AbstractArray{T,2})\nrandperm(f::Expr)  at random.jl:1341", 
            "title": "randperm"
        }, 
        {
            "location": "/wholeAPI/index.html#readij", 
            "text": "To read a simple edge list, each line being an (i, j) pair  readIJ(filename::AbstractString)\nreadIJ(filename::AbstractString, sep)  at /Users/spielman/.julia/v0.4/Laplacians/src/IO.jl:4", 
            "title": "readIJ"
        }, 
        {
            "location": "/wholeAPI/index.html#readijv", 
            "text": "To read a simple edge list, each line being an (i, j, v) pair. The parens should not be there in the format, just commas separating. To generate this format in Matlab, you just need to be careful to write the vertex indices with sufficient precision.  For example, you can do this   [ai,aj,av] = find(triu(a));  dlmwrite('graph.txt',[ai,aj,av],'precision',9);  readIJV(filename::AbstractString)  at /Users/spielman/.julia/v0.4/Laplacians/src/IO.jl:25", 
            "title": "readIJV"
        }, 
        {
            "location": "/wholeAPI/index.html#refinecut", 
            "text": "Modifies a cluster by adding or removing vertices by picking at each step \nthe vertex that has the maximum value of (Deg_external - Deg_Internal).\nEach vertex can be added in/removed only once.  refineCut{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})  at /Users/spielman/.julia/v0.4/Laplacians/src/cutHeuristics.jl:9", 
            "title": "refineCut"
        }, 
        {
            "location": "/wholeAPI/index.html#ringgraph", 
            "text": "The simple ring on n vertices  ringGraph(n::Int64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:23", 
            "title": "ringGraph"
        }, 
        {
            "location": "/wholeAPI/index.html#semiwtedchimera", 
            "text": "A Chimera graph with some weights.  The weights just appear when graphs are combined. For more interesting weights, use  wtedChimera  semiWtedChimera(n::Integer)\nsemiWtedChimera(n::Integer, k::Integer)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:442", 
            "title": "semiWtedChimera"
        }, 
        {
            "location": "/wholeAPI/index.html#setvalue", 
            "text": "Sets the value of a certain edge in a sparse graph; value can be 0 without the edges dissapearing   setValue{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti, a::Tv)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:29", 
            "title": "setValue"
        }, 
        {
            "location": "/wholeAPI/index.html#shortintgraph", 
            "text": "Convert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much  shortIntGraph(a::SparseMatrixCSC{Tv,Ti :Integer})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:3", 
            "title": "shortIntGraph"
        }, 
        {
            "location": "/wholeAPI/index.html#shortestpathtree", 
            "text": "Computes the shortest path tree, and returns it as a sparse matrix. Treats edge weights as reciprocals of lengths. For example:  a = [0 2 1; 2 0 3; 1 3 0]\ntr = full(shortestPathTree(sparse(a),1))\n\n3x3 Array{Float64,2}:\n 0.0  2.0  0.0\n 2.0  0.0  3.0\n 0.0  3.0  0.0  shortestPathTree(a, start)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:239", 
            "title": "shortestPathTree"
        }, 
        {
            "location": "/wholeAPI/index.html#shortestpaths", 
            "text": "Computes the lenghts of shortest paths from  start . Returns both a vector of the lenghts, and the parent array in the shortest path tree.  This algorithm treats edge weights as reciprocals of distances. DOC BETTER  shortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:175", 
            "title": "shortestPaths"
        }, 
        {
            "location": "/wholeAPI/index.html#spectralcoords", 
            "text": "Computes the spectral coordinates of a graph  spectralCoords(a)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:174", 
            "title": "spectralCoords"
        }, 
        {
            "location": "/wholeAPI/index.html#spectraldrawing", 
            "text": "Computes spectral coordinates, and then uses plotGraph to draw  spectralDrawing(a)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:166", 
            "title": "spectralDrawing"
        }, 
        {
            "location": "/wholeAPI/index.html#subsampleedges", 
            "text": "Create a new graph from the old, but keeping edge edge with probability  p  subsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:76", 
            "title": "subsampleEdges"
        }, 
        {
            "location": "/wholeAPI/index.html#tarjanstretch", 
            "text": "tarjanStretch{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti}, depth::Array{Tv,1})  at /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:334", 
            "title": "tarjanStretch"
        }, 
        {
            "location": "/wholeAPI/index.html#tounitvector", 
            "text": "Creates a unit vector of length n from a given set of integers, with weights based on the number of occurences  toUnitVector(a::Array{Int64,1}, n)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:183", 
            "title": "toUnitVector"
        }, 
        {
            "location": "/wholeAPI/index.html#toposort", 
            "text": "toposort{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})  at /Users/spielman/.julia/v0.4/Laplacians/src/toposort.jl:13", 
            "title": "toposort"
        }, 
        {
            "location": "/wholeAPI/index.html#twolift", 
            "text": "Creats a 2-lift of a.   flip  is a boolean indicating which edges cross  twoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:99", 
            "title": "twoLift"
        }, 
        {
            "location": "/wholeAPI/index.html#uniformweight", 
            "text": "Put a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.  uniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:49", 
            "title": "uniformWeight"
        }, 
        {
            "location": "/wholeAPI/index.html#uniformweight_1", 
            "text": "Set the weight of every edge to 1  uniformWeight!(mat::SparseMatrixCSC{Tv,Ti :Integer})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:53", 
            "title": "uniformWeight!"
        }, 
        {
            "location": "/wholeAPI/index.html#unweight", 
            "text": "Create a new graph in that is the same as the original, but with all edge weights 1  unweight{Tval,Tind}(ain::SparseMatrixCSC{Tval,Tind})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:16", 
            "title": "unweight"
        }, 
        {
            "location": "/wholeAPI/index.html#unweight_1", 
            "text": "Change the weight of every edge in a to 1  unweight!{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:26", 
            "title": "unweight!"
        }, 
        {
            "location": "/wholeAPI/index.html#vectocomps", 
            "text": "This turns a component vector, like that generated by components, into an array of arrays of indices of vertices in each component.  For example,  comps = vecToComps(c)\n\n3-element Array{Array{Int64,1},1}:\n [1,2,3,4,6,7,8]\n [5,10]\n [9]  vecToComps{Ti}(compvec::Array{Ti,1})  at /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:136", 
            "title": "vecToComps"
        }, 
        {
            "location": "/wholeAPI/index.html#wdeg", 
            "text": "Finds the weighted degree of a vertex in the graph   wdeg{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:19", 
            "title": "wdeg"
        }, 
        {
            "location": "/wholeAPI/index.html#weighti", 
            "text": "weighti{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:13", 
            "title": "weighti"
        }, 
        {
            "location": "/wholeAPI/index.html#writeijv", 
            "text": "Writes the upper portion of a matrix in ijv format, one row for each edge, separated by commas.  Only writes the upper triangular portion. The result can be read from Matlab like this:   dl = dlmread('graph.txt');  a = sparse(dl(:,1),dl(:,2),dl(:,3));  n = max(size(a))  a(n,n) = 0;  a = a + a';  writeIJV(filename::AbstractString, mat)  at /Users/spielman/.julia/v0.4/Laplacians/src/IO.jl:52", 
            "title": "writeIJV"
        }, 
        {
            "location": "/wholeAPI/index.html#wtedchimera", 
            "text": "Builds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  Generate a chimera, and then apply a random weighting scheme  wtedChimera(n::Integer)\nwtedChimera(n::Integer, k::Integer)  at /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:607", 
            "title": "wtedChimera"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html", 
            "text": "graphGenerators\n\n\npathGraph\n\n\nThe path graph on n vertices\n\n\npathGraph(n::Int64)\n\n\n\n\ngraphGenerators.jl:8\n\n\ncompleteGraph\n\n\nThe complete graph\n\n\ncompleteGraph(n::Int64)\n\n\n\n\ngraphGenerators.jl:17\n\n\nringGraph\n\n\nThe simple ring on n vertices\n\n\nringGraph(n::Int64)\n\n\n\n\ngraphGenerators.jl:23\n\n\ngeneralizedRing\n\n\nA generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example, \n\n\ngeneralizedRing(17, [1 5])\n\n\n\n\ngeneralizedRing(n::Int64, gens)\n\n\n\n\ngraphGenerators.jl:38\n\n\nrandGenRing\n\n\nA random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution\n\n\nrandGenRing(n::Int64, k::Integer)\n\n\n\n\ngraphGenerators.jl:62\n\n\nhyperCube\n\n\nThe d dimensional hypercube.  Has 2^d vertices\n\n\nhyperCube(d::Int64)\n\n\n\n\ngraphGenerators.jl:125\n\n\ncompleteBinaryTree\n\n\nThe complete binary tree on n vertices\n\n\ncompleteBinaryTree(n::Int64)\n\n\n\n\ngraphGenerators.jl:139\n\n\ngrid2\n\n\nAn n-by-m grid graph.  iostropy is the weighting on edges in one direction.\n\n\ngrid2(n::Int64)\ngrid2(n::Int64, m::Int64)\n\n\n\n\ngraphGenerators.jl:159\n\n\ngrid2coords\n\n\nCoordinates for plotting the vertices of the n-by-m grid graph\n\n\ngrid2coords(n::Int64, m::Int64)\ngrid2coords(n)\n\n\n\n\ngraphGenerators.jl:163\n\n\nrandMatching\n\n\nA random matching on n vertices\n\n\nrandMatching(n::Int64)\n\n\n\n\ngraphGenerators.jl:174\n\n\nrandRegular\n\n\nA sum of k random matchings on n vertices\n\n\nrandRegular(n::Int64, k::Int64)\n\n\n\n\ngraphGenerators.jl:187\n\n\ngrownGraph\n\n\nCreate a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.    \n\n\ngrownGraph(n::Int64, k::Int64)\n\n\n\n\ngraphGenerators.jl:202\n\n\ngrownGraphD\n\n\nLike a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices\n\n\ngrownGraphD(n::Int64, k::Int64)\n\n\n\n\ngraphGenerators.jl:234\n\n\nprefAttach\n\n\nA preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.\n\n\nprefAttach(n::Int64, k::Int64, p::Float64)\n\n\n\n\ngraphGenerators.jl:258\n\n\nErdosRenyi\n\n\nGenerate a random graph on n vertices with m edges. The actual number of edges will probably be smaller, as we sample with replacement\n\n\nErdosRenyi(n::Integer, m::Integer)\n\n\n\n\ngraphGenerators.jl:337\n\n\nErdosRenyiCluster\n\n\nGenerate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.\n\n\nErdosRenyiCluster(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:351\n\n\nErdosRenyiClusterFix\n\n\nLike an Erdos-Renyi cluster, but add back a tree so it has n vertices\n\n\nErdosRenyiClusterFix(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:364\n\n\npureRandomGraph\n\n\nGenerate a random graph with n vertices from one of our natural distributions\n\n\npureRandomGraph(n::Integer)\n\n\n\n\ngraphGenerators.jl:379\n\n\nsemiWtedChimera\n\n\nA Chimera graph with some weights.  The weights just appear when graphs are combined. For more interesting weights, use \nwtedChimera\n\n\nsemiWtedChimera(n::Integer)\nsemiWtedChimera(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:442\n\n\nchimera\n\n\nBuilds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nBuilds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace\n\n\nchimera(n::Integer)\nchimera(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:515\n\n\nrandWeight\n\n\nApplies one of a number of random weighting schemes to the edges of the graph\n\n\nrandWeight(a)\n\n\n\n\ngraphGenerators.jl:535\n\n\nwtedChimera\n\n\nBuilds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nGenerate a chimera, and then apply a random weighting scheme\n\n\nwtedChimera(n::Integer)\nwtedChimera(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:607", 
            "title": "graphGenerators"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#graphgenerators", 
            "text": "", 
            "title": "graphGenerators"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#pathgraph", 
            "text": "The path graph on n vertices  pathGraph(n::Int64)  graphGenerators.jl:8", 
            "title": "pathGraph"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#completegraph", 
            "text": "The complete graph  completeGraph(n::Int64)  graphGenerators.jl:17", 
            "title": "completeGraph"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#ringgraph", 
            "text": "The simple ring on n vertices  ringGraph(n::Int64)  graphGenerators.jl:23", 
            "title": "ringGraph"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#generalizedring", 
            "text": "A generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example,   generalizedRing(17, [1 5])  generalizedRing(n::Int64, gens)  graphGenerators.jl:38", 
            "title": "generalizedRing"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#randgenring", 
            "text": "A random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution  randGenRing(n::Int64, k::Integer)  graphGenerators.jl:62", 
            "title": "randGenRing"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#hypercube", 
            "text": "The d dimensional hypercube.  Has 2^d vertices  hyperCube(d::Int64)  graphGenerators.jl:125", 
            "title": "hyperCube"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#completebinarytree", 
            "text": "The complete binary tree on n vertices  completeBinaryTree(n::Int64)  graphGenerators.jl:139", 
            "title": "completeBinaryTree"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#grid2", 
            "text": "An n-by-m grid graph.  iostropy is the weighting on edges in one direction.  grid2(n::Int64)\ngrid2(n::Int64, m::Int64)  graphGenerators.jl:159", 
            "title": "grid2"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#grid2coords", 
            "text": "Coordinates for plotting the vertices of the n-by-m grid graph  grid2coords(n::Int64, m::Int64)\ngrid2coords(n)  graphGenerators.jl:163", 
            "title": "grid2coords"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#randmatching", 
            "text": "A random matching on n vertices  randMatching(n::Int64)  graphGenerators.jl:174", 
            "title": "randMatching"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#randregular", 
            "text": "A sum of k random matchings on n vertices  randRegular(n::Int64, k::Int64)  graphGenerators.jl:187", 
            "title": "randRegular"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#growngraph", 
            "text": "Create a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.      grownGraph(n::Int64, k::Int64)  graphGenerators.jl:202", 
            "title": "grownGraph"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#growngraphd", 
            "text": "Like a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices  grownGraphD(n::Int64, k::Int64)  graphGenerators.jl:234", 
            "title": "grownGraphD"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#prefattach", 
            "text": "A preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.  prefAttach(n::Int64, k::Int64, p::Float64)  graphGenerators.jl:258", 
            "title": "prefAttach"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#erdosrenyi", 
            "text": "Generate a random graph on n vertices with m edges. The actual number of edges will probably be smaller, as we sample with replacement  ErdosRenyi(n::Integer, m::Integer)  graphGenerators.jl:337", 
            "title": "ErdosRenyi"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#erdosrenyicluster", 
            "text": "Generate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.  ErdosRenyiCluster(n::Integer, k::Integer)  graphGenerators.jl:351", 
            "title": "ErdosRenyiCluster"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#erdosrenyiclusterfix", 
            "text": "Like an Erdos-Renyi cluster, but add back a tree so it has n vertices  ErdosRenyiClusterFix(n::Integer, k::Integer)  graphGenerators.jl:364", 
            "title": "ErdosRenyiClusterFix"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#purerandomgraph", 
            "text": "Generate a random graph with n vertices from one of our natural distributions  pureRandomGraph(n::Integer)  graphGenerators.jl:379", 
            "title": "pureRandomGraph"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#semiwtedchimera", 
            "text": "A Chimera graph with some weights.  The weights just appear when graphs are combined. For more interesting weights, use  wtedChimera  semiWtedChimera(n::Integer)\nsemiWtedChimera(n::Integer, k::Integer)  graphGenerators.jl:442", 
            "title": "semiWtedChimera"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#chimera", 
            "text": "Builds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  Builds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace  chimera(n::Integer)\nchimera(n::Integer, k::Integer)  graphGenerators.jl:515", 
            "title": "chimera"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#randweight", 
            "text": "Applies one of a number of random weighting schemes to the edges of the graph  randWeight(a)  graphGenerators.jl:535", 
            "title": "randWeight"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#wtedchimera", 
            "text": "Builds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  Generate a chimera, and then apply a random weighting scheme  wtedChimera(n::Integer)\nwtedChimera(n::Integer, k::Integer)  graphGenerators.jl:607", 
            "title": "wtedChimera"
        }, 
        {
            "location": "/IOAPI/index.html", 
            "text": "IO\n\n\nreadIJ\n\n\nTo read a simple edge list, each line being an (i, j) pair\n\n\nreadIJ(filename::AbstractString)\nreadIJ(filename::AbstractString, sep)\n\n\n\n\nIO.jl:4\n\n\nreadIJV\n\n\nTo read a simple edge list, each line being an (i, j, v) pair. The parens should not be there in the format, just commas separating. To generate this format in Matlab, you just need to be careful to write the vertex indices with sufficient precision.  For example, you can do this\n\n\n [ai,aj,av] = find(triu(a));\n\n dlmwrite('graph.txt',[ai,aj,av],'precision',9);\n\n\n\n\nreadIJV(filename::AbstractString)\n\n\n\n\nIO.jl:25\n\n\nwriteIJV\n\n\nWrites the upper portion of a matrix in ijv format, one row for each edge, separated by commas.  Only writes the upper triangular portion. The result can be read from Matlab like this:\n\n\n dl = dlmread('graph.txt');\n\n a = sparse(dl(:,1),dl(:,2),dl(:,3));\n\n n = max(size(a))\n\n a(n,n) = 0;\n\n a = a + a';\n\n\n\n\nwriteIJV(filename::AbstractString, mat)\n\n\n\n\nIO.jl:52", 
            "title": "IO"
        }, 
        {
            "location": "/IOAPI/index.html#io", 
            "text": "", 
            "title": "IO"
        }, 
        {
            "location": "/IOAPI/index.html#readij", 
            "text": "To read a simple edge list, each line being an (i, j) pair  readIJ(filename::AbstractString)\nreadIJ(filename::AbstractString, sep)  IO.jl:4", 
            "title": "readIJ"
        }, 
        {
            "location": "/IOAPI/index.html#readijv", 
            "text": "To read a simple edge list, each line being an (i, j, v) pair. The parens should not be there in the format, just commas separating. To generate this format in Matlab, you just need to be careful to write the vertex indices with sufficient precision.  For example, you can do this   [ai,aj,av] = find(triu(a));  dlmwrite('graph.txt',[ai,aj,av],'precision',9);  readIJV(filename::AbstractString)  IO.jl:25", 
            "title": "readIJV"
        }, 
        {
            "location": "/IOAPI/index.html#writeijv", 
            "text": "Writes the upper portion of a matrix in ijv format, one row for each edge, separated by commas.  Only writes the upper triangular portion. The result can be read from Matlab like this:   dl = dlmread('graph.txt');  a = sparse(dl(:,1),dl(:,2),dl(:,3));  n = max(size(a))  a(n,n) = 0;  a = a + a';  writeIJV(filename::AbstractString, mat)  IO.jl:52", 
            "title": "writeIJV"
        }, 
        {
            "location": "/graphOpsAPI/index.html", 
            "text": "graphOps\n\n\nshortIntGraph\n\n\nConvert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much\n\n\nshortIntGraph(a::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:3\n\n\nfloatGraph\n\n\nConvert the nonzero entries in a graph to Float64\n\n\nfloatGraph(a::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:6\n\n\nlap\n\n\nCreate a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry\n\n\nlap(a)\n\n\n\n\ngraphOps.jl:12\n\n\nunweight\n\n\nCreate a new graph in that is the same as the original, but with all edge weights 1\n\n\nunweight{Tval,Tind}(ain::SparseMatrixCSC{Tval,Tind})\n\n\n\n\ngraphOps.jl:16\n\n\nunweight!\n\n\nChange the weight of every edge in a to 1\n\n\nunweight!{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\ngraphOps.jl:26\n\n\nmapweight\n\n\nCreate a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write\n\n\nb = mapweight(a, x-\nrand(1)[1])\n\n\n\n\nmapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)\n\n\n\n\ngraphOps.jl:40\n\n\nuniformWeight\n\n\nPut a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.\n\n\nuniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\ngraphOps.jl:49\n\n\nuniformWeight!\n\n\nSet the weight of every edge to 1\n\n\nuniformWeight!(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:53\n\n\nproductGraph\n\n\nThe Cartesian product of two graphs.  When applied to two paths, it gives a grid.\n\n\nproductGraph(a0::SparseMatrixCSC{Tv,Ti\n:Integer}, a1::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:58\n\n\nedgeVertexMat\n\n\nThe signed edge-vertex adjacency matrix\n\n\nedgeVertexMat(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:67\n\n\nsubsampleEdges\n\n\nCreate a new graph from the old, but keeping edge edge with probability \np\n\n\nsubsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)\n\n\n\n\ngraphOps.jl:76\n\n\ntwoLift\n\n\nCreats a 2-lift of a.  \nflip\n is a boolean indicating which edges cross\n\n\ntwoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)\n\n\n\n\ngraphOps.jl:99\n\n\njoinGraphs\n\n\nCreate a disjoint union of graphs a and b,  and then put k random edges between them\n\n\njoinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)\n\n\n\n\ngraphOps.jl:111\n\n\nplotGraph\n\n\nPlots graph gr with coordinates (x,y)\n\n\nplotGraph(gr, x, y)\nplotGraph(gr, x, y, color)\n\n\n\n\ngraphOps.jl:129\n\n\nspectralDrawing\n\n\nComputes spectral coordinates, and then uses plotGraph to draw\n\n\nspectralDrawing(a)\n\n\n\n\ngraphOps.jl:166\n\n\nspectralCoords\n\n\nComputes the spectral coordinates of a graph\n\n\nspectralCoords(a)\n\n\n\n\ngraphOps.jl:174\n\n\ntoUnitVector\n\n\nCreates a unit vector of length n from a given set of integers, with weights based on the number of occurences\n\n\ntoUnitVector(a::Array{Int64,1}, n)\n\n\n\n\ngraphOps.jl:183\n\n\ndiagmat\n\n\nReturns the diagonal matrix(as a sparse matrix) of a graph\n\n\ndiagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\n\n\n\n\ngraphOps.jl:205\n\n\ngeneralizedNecklace\n\n\nConstructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.\n\n\ngeneralizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti\n:Integer}, k::Int64)\n\n\n\n\ngraphOps.jl:225", 
            "title": "graphOps"
        }, 
        {
            "location": "/graphOpsAPI/index.html#graphops", 
            "text": "", 
            "title": "graphOps"
        }, 
        {
            "location": "/graphOpsAPI/index.html#shortintgraph", 
            "text": "Convert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much  shortIntGraph(a::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:3", 
            "title": "shortIntGraph"
        }, 
        {
            "location": "/graphOpsAPI/index.html#floatgraph", 
            "text": "Convert the nonzero entries in a graph to Float64  floatGraph(a::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:6", 
            "title": "floatGraph"
        }, 
        {
            "location": "/graphOpsAPI/index.html#lap", 
            "text": "Create a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry  lap(a)  graphOps.jl:12", 
            "title": "lap"
        }, 
        {
            "location": "/graphOpsAPI/index.html#unweight", 
            "text": "Create a new graph in that is the same as the original, but with all edge weights 1  unweight{Tval,Tind}(ain::SparseMatrixCSC{Tval,Tind})  graphOps.jl:16", 
            "title": "unweight"
        }, 
        {
            "location": "/graphOpsAPI/index.html#unweight_1", 
            "text": "Change the weight of every edge in a to 1  unweight!{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})  graphOps.jl:26", 
            "title": "unweight!"
        }, 
        {
            "location": "/graphOpsAPI/index.html#mapweight", 
            "text": "Create a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write  b = mapweight(a, x- rand(1)[1])  mapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)  graphOps.jl:40", 
            "title": "mapweight"
        }, 
        {
            "location": "/graphOpsAPI/index.html#uniformweight", 
            "text": "Put a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.  uniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})  graphOps.jl:49", 
            "title": "uniformWeight"
        }, 
        {
            "location": "/graphOpsAPI/index.html#uniformweight_1", 
            "text": "Set the weight of every edge to 1  uniformWeight!(mat::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:53", 
            "title": "uniformWeight!"
        }, 
        {
            "location": "/graphOpsAPI/index.html#productgraph", 
            "text": "The Cartesian product of two graphs.  When applied to two paths, it gives a grid.  productGraph(a0::SparseMatrixCSC{Tv,Ti :Integer}, a1::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:58", 
            "title": "productGraph"
        }, 
        {
            "location": "/graphOpsAPI/index.html#edgevertexmat", 
            "text": "The signed edge-vertex adjacency matrix  edgeVertexMat(mat::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:67", 
            "title": "edgeVertexMat"
        }, 
        {
            "location": "/graphOpsAPI/index.html#subsampleedges", 
            "text": "Create a new graph from the old, but keeping edge edge with probability  p  subsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)  graphOps.jl:76", 
            "title": "subsampleEdges"
        }, 
        {
            "location": "/graphOpsAPI/index.html#twolift", 
            "text": "Creats a 2-lift of a.   flip  is a boolean indicating which edges cross  twoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)  graphOps.jl:99", 
            "title": "twoLift"
        }, 
        {
            "location": "/graphOpsAPI/index.html#joingraphs", 
            "text": "Create a disjoint union of graphs a and b,  and then put k random edges between them  joinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)  graphOps.jl:111", 
            "title": "joinGraphs"
        }, 
        {
            "location": "/graphOpsAPI/index.html#plotgraph", 
            "text": "Plots graph gr with coordinates (x,y)  plotGraph(gr, x, y)\nplotGraph(gr, x, y, color)  graphOps.jl:129", 
            "title": "plotGraph"
        }, 
        {
            "location": "/graphOpsAPI/index.html#spectraldrawing", 
            "text": "Computes spectral coordinates, and then uses plotGraph to draw  spectralDrawing(a)  graphOps.jl:166", 
            "title": "spectralDrawing"
        }, 
        {
            "location": "/graphOpsAPI/index.html#spectralcoords", 
            "text": "Computes the spectral coordinates of a graph  spectralCoords(a)  graphOps.jl:174", 
            "title": "spectralCoords"
        }, 
        {
            "location": "/graphOpsAPI/index.html#tounitvector", 
            "text": "Creates a unit vector of length n from a given set of integers, with weights based on the number of occurences  toUnitVector(a::Array{Int64,1}, n)  graphOps.jl:183", 
            "title": "toUnitVector"
        }, 
        {
            "location": "/graphOpsAPI/index.html#diagmat", 
            "text": "Returns the diagonal matrix(as a sparse matrix) of a graph  diagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})  graphOps.jl:205", 
            "title": "diagmat"
        }, 
        {
            "location": "/graphOpsAPI/index.html#generalizednecklace", 
            "text": "Constructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.  generalizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti :Integer}, k::Int64)  graphOps.jl:225", 
            "title": "generalizedNecklace"
        }, 
        {
            "location": "/graphAlgsAPI/index.html", 
            "text": "graphAlgs\n\n\ncomponents\n\n\nComputes the connected components of a graph. Returns them as a vector of length equal to the number of vertices. The vector numbers the components from 1 through the maximum number. For example,\n\n\ngr = ErdosRenyi(10,11)\nc = components(gr)\n\n10-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 1\n 1\n 1\n 3\n 2\n\n\n\n\ncomponents{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\ngraphAlgs.jl:65\n\n\nisConnected\n\n\nReturns true if graph is connected.  Calls components.\n\n\nisConnected(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphAlgs.jl:113\n\n\nvecToComps\n\n\nThis turns a component vector, like that generated by components, into an array of arrays of indices of vertices in each component.  For example,\n\n\ncomps = vecToComps(c)\n\n3-element Array{Array{Int64,1},1}:\n [1,2,3,4,6,7,8]\n [5,10]\n [9]\n\n\n\n\nvecToComps{Ti}(compvec::Array{Ti,1})\n\n\n\n\ngraphAlgs.jl:136\n\n\nbiggestComp\n\n\nReturn the biggest component in a graph, as a graph\n\n\nbiggestComp(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphAlgs.jl:159\n\n\nshortestPaths\n\n\nComputes the lenghts of shortest paths from \nstart\n. Returns both a vector of the lenghts, and the parent array in the shortest path tree.\n\n\nThis algorithm treats edge weights as reciprocals of distances. DOC BETTER\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\ngraphAlgs.jl:175\n\n\nshortestPathTree\n\n\nComputes the shortest path tree, and returns it as a sparse matrix. Treats edge weights as reciprocals of lengths. For example:\n\n\na = [0 2 1; 2 0 3; 1 3 0]\ntr = full(shortestPathTree(sparse(a),1))\n\n3x3 Array{Float64,2}:\n 0.0  2.0  0.0\n 2.0  0.0  3.0\n 0.0  3.0  0.0\n\n\n\n\nshortestPathTree(a, start)\n\n\n\n\ngraphAlgs.jl:239\n\n\nkruskal\n\n\nUses Kruskal's algorithm to compute a minimum (or maximum) spanning tree. Set kind=:max if you want the max spanning tree. It returns it a a graph\n\n\nkruskal{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\ngraphAlgs.jl:407", 
            "title": "graphAlgs"
        }, 
        {
            "location": "/graphAlgsAPI/index.html#graphalgs", 
            "text": "", 
            "title": "graphAlgs"
        }, 
        {
            "location": "/graphAlgsAPI/index.html#components", 
            "text": "Computes the connected components of a graph. Returns them as a vector of length equal to the number of vertices. The vector numbers the components from 1 through the maximum number. For example,  gr = ErdosRenyi(10,11)\nc = components(gr)\n\n10-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 1\n 1\n 1\n 3\n 2  components{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})  graphAlgs.jl:65", 
            "title": "components"
        }, 
        {
            "location": "/graphAlgsAPI/index.html#isconnected", 
            "text": "Returns true if graph is connected.  Calls components.  isConnected(mat::SparseMatrixCSC{Tv,Ti :Integer})  graphAlgs.jl:113", 
            "title": "isConnected"
        }, 
        {
            "location": "/graphAlgsAPI/index.html#vectocomps", 
            "text": "This turns a component vector, like that generated by components, into an array of arrays of indices of vertices in each component.  For example,  comps = vecToComps(c)\n\n3-element Array{Array{Int64,1},1}:\n [1,2,3,4,6,7,8]\n [5,10]\n [9]  vecToComps{Ti}(compvec::Array{Ti,1})  graphAlgs.jl:136", 
            "title": "vecToComps"
        }, 
        {
            "location": "/graphAlgsAPI/index.html#biggestcomp", 
            "text": "Return the biggest component in a graph, as a graph  biggestComp(mat::SparseMatrixCSC{Tv,Ti :Integer})  graphAlgs.jl:159", 
            "title": "biggestComp"
        }, 
        {
            "location": "/graphAlgsAPI/index.html#shortestpaths", 
            "text": "Computes the lenghts of shortest paths from  start . Returns both a vector of the lenghts, and the parent array in the shortest path tree.  This algorithm treats edge weights as reciprocals of distances. DOC BETTER  shortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})  graphAlgs.jl:175", 
            "title": "shortestPaths"
        }, 
        {
            "location": "/graphAlgsAPI/index.html#shortestpathtree", 
            "text": "Computes the shortest path tree, and returns it as a sparse matrix. Treats edge weights as reciprocals of lengths. For example:  a = [0 2 1; 2 0 3; 1 3 0]\ntr = full(shortestPathTree(sparse(a),1))\n\n3x3 Array{Float64,2}:\n 0.0  2.0  0.0\n 2.0  0.0  3.0\n 0.0  3.0  0.0  shortestPathTree(a, start)  graphAlgs.jl:239", 
            "title": "shortestPathTree"
        }, 
        {
            "location": "/graphAlgsAPI/index.html#kruskal", 
            "text": "Uses Kruskal's algorithm to compute a minimum (or maximum) spanning tree. Set kind=:max if you want the max spanning tree. It returns it a a graph  kruskal{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})  graphAlgs.jl:407", 
            "title": "kruskal"
        }, 
        {
            "location": "/graphUtilsAPI/index.html", 
            "text": "graphUtils\n\n\nwdeg\n\n\nFinds the weighted degree of a vertex in the graph \n\n\nwdeg{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)\n\n\n\n\ngraphUtils.jl:19\n\n\nsetValue\n\n\nSets the value of a certain edge in a sparse graph; value can be 0 without the edges dissapearing \n\n\nsetValue{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti, a::Tv)\n\n\n\n\ngraphUtils.jl:29\n\n\nbackIndices\n\n\nSame as the above, but now the graph is in adjacency list form \n\n\nComputes the back indices in a graph in O(M+N). works if for every edge (u,v), (v,u) is also in the graph \n\n\nbackIndices{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\nbackIndices{Tv1,Tv2}(G::Array{Array{Tuple{Tv1,Tv2},1},1})\n\n\n\n\ngraphUtils.jl:35\n\n\nfindEntries\n\n\nSimilar to findnz, but also returns 0 entries that have an edge in the sparse matrix \n\n\nfindEntries{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\n\n\n\n\ngraphUtils.jl:114\n\n\ncompConductance\n\n\nReturns the quality of the cut for a given graph and a given cut set s.   the result will be |outgoing edges| / min(|vertices in set|, |N - vertices in set|)\n\n\ncompConductance{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\ngraphUtils.jl:136\n\n\ngetVolume\n\n\nComputes the volume of subset s in an unweighted graph G \n\n\ngetVolume{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\ngraphUtils.jl:149\n\n\ngetObound\n\n\nComputes the number of edges leaving s \n\n\ngetObound{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\ngraphUtils.jl:167", 
            "title": "graphUtils"
        }, 
        {
            "location": "/graphUtilsAPI/index.html#graphutils", 
            "text": "", 
            "title": "graphUtils"
        }, 
        {
            "location": "/graphUtilsAPI/index.html#wdeg", 
            "text": "Finds the weighted degree of a vertex in the graph   wdeg{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)  graphUtils.jl:19", 
            "title": "wdeg"
        }, 
        {
            "location": "/graphUtilsAPI/index.html#setvalue", 
            "text": "Sets the value of a certain edge in a sparse graph; value can be 0 without the edges dissapearing   setValue{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti, a::Tv)  graphUtils.jl:29", 
            "title": "setValue"
        }, 
        {
            "location": "/graphUtilsAPI/index.html#backindices", 
            "text": "Same as the above, but now the graph is in adjacency list form   Computes the back indices in a graph in O(M+N). works if for every edge (u,v), (v,u) is also in the graph   backIndices{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\nbackIndices{Tv1,Tv2}(G::Array{Array{Tuple{Tv1,Tv2},1},1})  graphUtils.jl:35", 
            "title": "backIndices"
        }, 
        {
            "location": "/graphUtilsAPI/index.html#findentries", 
            "text": "Similar to findnz, but also returns 0 entries that have an edge in the sparse matrix   findEntries{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})  graphUtils.jl:114", 
            "title": "findEntries"
        }, 
        {
            "location": "/graphUtilsAPI/index.html#compconductance", 
            "text": "Returns the quality of the cut for a given graph and a given cut set s.   the result will be |outgoing edges| / min(|vertices in set|, |N - vertices in set|)  compConductance{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})  graphUtils.jl:136", 
            "title": "compConductance"
        }, 
        {
            "location": "/graphUtilsAPI/index.html#getvolume", 
            "text": "Computes the volume of subset s in an unweighted graph G   getVolume{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})  graphUtils.jl:149", 
            "title": "getVolume"
        }, 
        {
            "location": "/graphUtilsAPI/index.html#getobound", 
            "text": "Computes the number of edges leaving s   getObound{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})  graphUtils.jl:167", 
            "title": "getObound"
        }, 
        {
            "location": "/solversAPI/index.html", 
            "text": "solvers\n\n\nlapWrapSolver\n\n\nTakes a solver for solving nonsingular sdd systems, and returns a solver for solving Laplacian systems. The optional args tol and maxits are not necessarily taken by all solvers.  But, if they are, one can pass them here\n\n\nlapWrapSolver(solver)\nlapWrapSolver(solver, la::AbstractArray{T,N})\nlapWrapSolver(solver, la::AbstractArray{T,N}, b)\n\n\n\n\nsolvers.jl:108\n\n\naugmentTree\n\n\nTakes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It then adds back the k edges of highest stretch, and k edges sampled according to stretch\n\n\naugmentTree{Tv,Ti}(tree::SparseMatrixCSC{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti}, k::Ti)\n\n\n\n\nsolvers.jl:138\n\n\naugTreePrecon\n\n\nThis is an augmented spanning tree preconditioner for diagonally dominant linear systems.  It takes as optional input a tree growing algorithm. The default is a randomized variant of Kruskal. It adds back 2sqrt(n) edges via augmentTree. With the right tree, it should never be too bad.\n\n\naugTreePrecon{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nsolvers.jl:188\n\n\naugTreeSolver\n\n\nThis is the solver that calls augTreePrecon\n\n\naugTreeSolver{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nsolvers.jl:210", 
            "title": "solvers"
        }, 
        {
            "location": "/solversAPI/index.html#solvers", 
            "text": "", 
            "title": "solvers"
        }, 
        {
            "location": "/solversAPI/index.html#lapwrapsolver", 
            "text": "Takes a solver for solving nonsingular sdd systems, and returns a solver for solving Laplacian systems. The optional args tol and maxits are not necessarily taken by all solvers.  But, if they are, one can pass them here  lapWrapSolver(solver)\nlapWrapSolver(solver, la::AbstractArray{T,N})\nlapWrapSolver(solver, la::AbstractArray{T,N}, b)  solvers.jl:108", 
            "title": "lapWrapSolver"
        }, 
        {
            "location": "/solversAPI/index.html#augmenttree", 
            "text": "Takes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It then adds back the k edges of highest stretch, and k edges sampled according to stretch  augmentTree{Tv,Ti}(tree::SparseMatrixCSC{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti}, k::Ti)  solvers.jl:138", 
            "title": "augmentTree"
        }, 
        {
            "location": "/solversAPI/index.html#augtreeprecon", 
            "text": "This is an augmented spanning tree preconditioner for diagonally dominant linear systems.  It takes as optional input a tree growing algorithm. The default is a randomized variant of Kruskal. It adds back 2sqrt(n) edges via augmentTree. With the right tree, it should never be too bad.  augTreePrecon{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})  solvers.jl:188", 
            "title": "augTreePrecon"
        }, 
        {
            "location": "/solversAPI/index.html#augtreesolver", 
            "text": "This is the solver that calls augTreePrecon  augTreeSolver{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})  solvers.jl:210", 
            "title": "augTreeSolver"
        }, 
        {
            "location": "/cutHeuristicsAPI/index.html", 
            "text": "cutHeuristics\n\n\nrefineCut\n\n\nModifies a cluster by adding or removing vertices by picking at each step \nthe vertex that has the maximum value of (Deg_external - Deg_Internal).\nEach vertex can be added in/removed only once.\n\n\n\n\nrefineCut{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\ncutHeuristics.jl:9\n\n\ndumb\n\n\nModify a cluster by passing through all the vertices exactly once and \nadding/removing them based on the value of (Deg_external - Deg_Internal).\n\n\n\n\ndumb{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\ncutHeuristics.jl:106", 
            "title": "cutHeuristics"
        }, 
        {
            "location": "/cutHeuristicsAPI/index.html#cutheuristics", 
            "text": "", 
            "title": "cutHeuristics"
        }, 
        {
            "location": "/cutHeuristicsAPI/index.html#refinecut", 
            "text": "Modifies a cluster by adding or removing vertices by picking at each step \nthe vertex that has the maximum value of (Deg_external - Deg_Internal).\nEach vertex can be added in/removed only once.  refineCut{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})  cutHeuristics.jl:9", 
            "title": "refineCut"
        }, 
        {
            "location": "/cutHeuristicsAPI/index.html#dumb", 
            "text": "Modify a cluster by passing through all the vertices exactly once and \nadding/removing them based on the value of (Deg_external - Deg_Internal).  dumb{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})  cutHeuristics.jl:106", 
            "title": "dumb"
        }, 
        {
            "location": "/localClusteringAPI/index.html", 
            "text": "localClustering\n\n\nlocalImprove\n\n\nlocalImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1}; epsSigma=-1.0, err=1e-10, maxSize = max(G.n, G.m)\n\n\nThe LocalImprove function, from the Orrechia-Zhu paper. Given a graph and an initial set, finds a set of smaller conductance based on the starting set using a localized version of max-flow.\n\n\nSmall discussion: When adding in the neighbors of the initial component, if the resulting  conductance is worse than the initial one,  the algorithm will add more and more vertices until hitting a better conductance. However, if we fix a certain  maximum size for our component,  it might be the case that this new conductance will always be worse than what we had initially. Thus, if we run the algorithm with a small maxSize,  our initial conductance might be the best solution we can reach.\n\n\n\n\nG is the given graph, A is the initial set \n\n\nepsSigma is a measure of the quality of the returning set (the smaller the better). It's defaulted to volume(A) / volume(VA)\n\n\nerr is the numerical error considered throughout the algorithm. It's defaulted to 1e-10\n\n\nmaxSize is the maximum allowed size for the flow graph at any iteration of the algorithm. It's defaulted to |V|\n\n\n\n\nlocalImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1})\n\n\n\n\nlocalClustering.jl:22\n\n\nprn\n\n\nprn{Tv, Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)\n\n\nThe PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper\n\n\ns is a set of starting vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]\n\n\nphi is a bound on the quality of the conductance of the cut - the smaller the phi, the higher the quality.  b is used to handle precision throughout the algorithm - the higher the b, the greater the precision.\n\n\nprn{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)\n\n\n\n\nlocalClustering.jl:374\n\n\napr\n\n\nComputes an approximate page rank vector from a starting set s, an alpha and an epsilon The algorithm follows the Anderson,Chung,Lang paper and Dan Spielman's lecture notes\n\n\napr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, alpha::Float64, eps::Float64)\n\n\n\n\nlocalClustering.jl:440", 
            "title": "localClustering"
        }, 
        {
            "location": "/localClusteringAPI/index.html#localclustering", 
            "text": "", 
            "title": "localClustering"
        }, 
        {
            "location": "/localClusteringAPI/index.html#localimprove", 
            "text": "localImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1}; epsSigma=-1.0, err=1e-10, maxSize = max(G.n, G.m)  The LocalImprove function, from the Orrechia-Zhu paper. Given a graph and an initial set, finds a set of smaller conductance based on the starting set using a localized version of max-flow.  Small discussion: When adding in the neighbors of the initial component, if the resulting  conductance is worse than the initial one,  the algorithm will add more and more vertices until hitting a better conductance. However, if we fix a certain  maximum size for our component,  it might be the case that this new conductance will always be worse than what we had initially. Thus, if we run the algorithm with a small maxSize,  our initial conductance might be the best solution we can reach.   G is the given graph, A is the initial set   epsSigma is a measure of the quality of the returning set (the smaller the better). It's defaulted to volume(A) / volume(VA)  err is the numerical error considered throughout the algorithm. It's defaulted to 1e-10  maxSize is the maximum allowed size for the flow graph at any iteration of the algorithm. It's defaulted to |V|   localImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1})  localClustering.jl:22", 
            "title": "localImprove"
        }, 
        {
            "location": "/localClusteringAPI/index.html#prn", 
            "text": "prn{Tv, Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)  The PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper  s is a set of starting vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]  phi is a bound on the quality of the conductance of the cut - the smaller the phi, the higher the quality.  b is used to handle precision throughout the algorithm - the higher the b, the greater the precision.  prn{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)  localClustering.jl:374", 
            "title": "prn"
        }, 
        {
            "location": "/localClusteringAPI/index.html#apr", 
            "text": "Computes an approximate page rank vector from a starting set s, an alpha and an epsilon The algorithm follows the Anderson,Chung,Lang paper and Dan Spielman's lecture notes  apr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, alpha::Float64, eps::Float64)  localClustering.jl:440", 
            "title": "apr"
        }
    ]
}